{"cells":[{"cell_type":"markdown","metadata":{"id":"oMiQjtU0Xa_n"},"source":["# Topic modelling to classify documents using LDA"]},{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":659,"status":"ok","timestamp":1659535215155,"user":{"displayName":"Kabhilan S","userId":"04978609858624650983"},"user_tz":-330},"id":"2nA5yvdmZLVy"},"outputs":[],"source":["import pandas as pd\n","import json\n","import numpy as np"]},{"cell_type":"markdown","metadata":{"id":"mZ5mmyg6ZLV3"},"source":["### Load Google drive directory"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":25937,"status":"ok","timestamp":1659535241089,"user":{"displayName":"Kabhilan S","userId":"04978609858624650983"},"user_tz":-330},"id":"NW03HODWZLV4","outputId":"600398cf-f0e4-42d3-f074-f22d89733b0b"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/gdrive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/gdrive')"]},{"cell_type":"markdown","metadata":{"id":"8dL13W4dZLV5"},"source":["### Load the csv file"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":733,"status":"ok","timestamp":1659535241819,"user":{"displayName":"Kabhilan S","userId":"04978609858624650983"},"user_tz":-330},"id":"m0wX35r4ZLV5","outputId":"489f00f3-b796-4f40-c1b8-4d170421da43"},"outputs":[{"output_type":"stream","name":"stdout","text":["   Index  _id                                               text\n","0      1    1    Predictive models allow subject-specific inf...\n","1      2    2    Rotation invariance and translation invarian...\n","2      3    3    We introduce and develop the notion of spher...\n","3      4    4    The stochastic Landau--Lifshitz--Gilbert (LL...\n","4      5    5    Fourier-transform infra-red (FTIR) spectra o...\n","   Index  _id                                               text  Topic1  \\\n","0      1    1    Predictive models allow subject-specific inf...     NaN   \n","1      2    2    Rotation invariance and translation invarian...     NaN   \n","2      3    3    We introduce and develop the notion of spher...     NaN   \n","3      4    4    The stochastic Landau--Lifshitz--Gilbert (LL...     NaN   \n","4      5    5    Fourier-transform infra-red (FTIR) spectra o...     NaN   \n","\n","   Prob1  Topic2  Prob2  Topic3  Prob3  Topic4  Prob4  Topic5  Prob5  \n","0    NaN     NaN    NaN     NaN    NaN     NaN    NaN     NaN    NaN  \n","1    NaN     NaN    NaN     NaN    NaN     NaN    NaN     NaN    NaN  \n","2    NaN     NaN    NaN     NaN    NaN     NaN    NaN     NaN    NaN  \n","3    NaN     NaN    NaN     NaN    NaN     NaN    NaN     NaN    NaN  \n","4    NaN     NaN    NaN     NaN    NaN     NaN    NaN     NaN    NaN  \n"]}],"source":["#documents = pd.DataFrame(data)\n","documents = pd.read_csv(\"/content/gdrive/My Drive/trainldaf.csv\")\n","print(documents[:5])\n","topic=['Topic1','Topic2','Topic3','Topic4','Topic5']\n","prob =['Prob1','Prob2', 'Prob3', 'Prob4','Prob5']\n","for i in range(len(topic)):\n","    documents[topic[i]]=np.nan\n","    documents[prob[i]]=np.nan\n","print(documents[:5])"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":112},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1659535241819,"user":{"displayName":"Kabhilan S","userId":"04978609858624650983"},"user_tz":-330},"id":"RQS0DXsEZLV7","outputId":"60caa5e3-ea11-4529-f638-68053495ae78"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["   Index  _id                                               text  Topic1  \\\n","0      1    1    Predictive models allow subject-specific inf...     NaN   \n","1      2    2    Rotation invariance and translation invarian...     NaN   \n","\n","   Prob1  Topic2  Prob2  Topic3  Prob3  Topic4  Prob4  Topic5  Prob5  \n","0    NaN     NaN    NaN     NaN    NaN     NaN    NaN     NaN    NaN  \n","1    NaN     NaN    NaN     NaN    NaN     NaN    NaN     NaN    NaN  "],"text/html":["\n","  <div id=\"df-66a03e8f-e6a8-46ed-afad-8cc7fe83df69\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Index</th>\n","      <th>_id</th>\n","      <th>text</th>\n","      <th>Topic1</th>\n","      <th>Prob1</th>\n","      <th>Topic2</th>\n","      <th>Prob2</th>\n","      <th>Topic3</th>\n","      <th>Prob3</th>\n","      <th>Topic4</th>\n","      <th>Prob4</th>\n","      <th>Topic5</th>\n","      <th>Prob5</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>Predictive models allow subject-specific inf...</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>2</td>\n","      <td>2</td>\n","      <td>Rotation invariance and translation invarian...</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-66a03e8f-e6a8-46ed-afad-8cc7fe83df69')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-66a03e8f-e6a8-46ed-afad-8cc7fe83df69 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-66a03e8f-e6a8-46ed-afad-8cc7fe83df69');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":4}],"source":["documents.head(2)"]},{"cell_type":"markdown","metadata":{"id":"yrS4_02IZLV8"},"source":["### Data Preprocessing"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":3310,"status":"ok","timestamp":1659535245126,"user":{"displayName":"Kabhilan S","userId":"04978609858624650983"},"user_tz":-330},"id":"atNzOWXrZLV8"},"outputs":[],"source":["import gensim\n","from gensim.utils import simple_preprocess\n","from gensim.parsing.preprocessing import STOPWORDS\n","from nltk.stem import WordNetLemmatizer, SnowballStemmer\n","from nltk.stem.porter import *\n","np.random.seed(2018)"]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":444,"status":"ok","timestamp":1659535473311,"user":{"displayName":"Kabhilan S","userId":"04978609858624650983"},"user_tz":-330},"id":"2m1CpalVZLV9","outputId":"1098bb65-d44f-40d9-cabf-688c63a7dec2"},"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n","[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":10}],"source":["import nltk\n","nltk.download('wordnet')\n","nltk.download('omw-1.4')"]},{"cell_type":"code","execution_count":11,"metadata":{"executionInfo":{"elapsed":683,"status":"ok","timestamp":1659535475912,"user":{"displayName":"Kabhilan S","userId":"04978609858624650983"},"user_tz":-330},"id":"tKceDDuiZLV-"},"outputs":[],"source":["stemmer = SnowballStemmer('english')\n","def lemmatize_stemming(text):\n","    return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))\n","\n","def preprocess(text):\n","    result = []\n","    for token in gensim.utils.simple_preprocess(text):\n","        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3:\n","            result.append(lemmatize_stemming(token))\n","    return result"]},{"cell_type":"code","execution_count":12,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":46873,"status":"ok","timestamp":1659535524692,"user":{"displayName":"Kabhilan S","userId":"04978609858624650983"},"user_tz":-330},"id":"CBae0rOuZLV_","outputId":"d1e9a9b6-491b-4d47-a3ec-b3e2edc82501"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["0    [predict, model, allow, subject, specif, infer...\n","1    [rotat, invari, translat, invari, great, valu,...\n","2    [introduc, develop, notion, spheric, polyharmo...\n","3    [stochast, landau, lifshitz, gilbert, equat, c...\n","4    [fourier, transform, infra, ftir, spectra, sam...\n","Name: text, dtype: object"]},"metadata":{},"execution_count":12}],"source":["# Preprocess the data\n","processed_docs = documents['text'].map(preprocess)\n","processed_docs.head(5)"]},{"cell_type":"code","execution_count":13,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":420,"status":"ok","timestamp":1659535531065,"user":{"displayName":"Kabhilan S","userId":"04978609858624650983"},"user_tz":-330},"id":"_txwUedWO09g","outputId":"c9fa29b7-56de-4cf7-9d05-13950b0783d7"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["0    [predict, model, allow, subject, specif, infer...\n","1    [rotat, invari, translat, invari, great, valu,...\n","2    [introduc, develop, notion, spheric, polyharmo...\n","3    [stochast, landau, lifshitz, gilbert, equat, c...\n","4    [fourier, transform, infra, ftir, spectra, sam...\n","Name: text, dtype: object"]},"metadata":{},"execution_count":13}],"source":["processed_docs.head(5)"]},{"cell_type":"markdown","metadata":{"id":"Z0sCSmCHZLV_"},"source":["### Bag of words on the dataset"]},{"cell_type":"code","execution_count":14,"metadata":{"executionInfo":{"elapsed":2238,"status":"ok","timestamp":1659535533959,"user":{"displayName":"Kabhilan S","userId":"04978609858624650983"},"user_tz":-330},"id":"nRH6M2Z7ZLWA"},"outputs":[],"source":["dictionary = gensim.corpora.Dictionary(processed_docs)\n","# Saving the dictionary\n","dictionary.save('dictionary.gensim')"]},{"cell_type":"code","execution_count":15,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1659535533959,"user":{"displayName":"Kabhilan S","userId":"04978609858624650983"},"user_tz":-330},"id":"GI0pemIBZLWA","outputId":"50269bf3-8caf-4d3d-976d-564bb5b5d29b"},"outputs":[{"output_type":"stream","name":"stdout","text":["0 accuraci\n","1 adni\n","2 aim\n","3 algorithm\n","4 allow\n","5 alter\n","6 alzheim\n","7 amyloid\n","8 analys\n","9 analyz\n","10 approach\n"]}],"source":["count = 0\n","for k, v in dictionary.iteritems():\n","    print(k, v)\n","    count += 1\n","    if count > 10:\n","        break"]},{"cell_type":"code","execution_count":16,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1659535533960,"user":{"displayName":"Kabhilan S","userId":"04978609858624650983"},"user_tz":-330},"id":"HDEAplHaZLWB"},"outputs":[],"source":["dictionary.filter_extremes(no_below=15, no_above=0.5, keep_n=50000)"]},{"cell_type":"code","execution_count":17,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2024,"status":"ok","timestamp":1659535535981,"user":{"displayName":"Kabhilan S","userId":"04978609858624650983"},"user_tz":-330},"id":"tHvBkKTLZLWB","outputId":"9ec950f0-5a37-49a8-d847-95fd5750a3fc"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["[(21, 3),\n"," (28, 1),\n"," (44, 1),\n"," (49, 1),\n"," (50, 1),\n"," (56, 1),\n"," (73, 1),\n"," (81, 1),\n"," (101, 2),\n"," (109, 2),\n"," (115, 1),\n"," (118, 1),\n"," (124, 1),\n"," (132, 1),\n"," (182, 1),\n"," (202, 2),\n"," (203, 1),\n"," (208, 2),\n"," (254, 1),\n"," (270, 1),\n"," (299, 1),\n"," (310, 1),\n"," (318, 3),\n"," (345, 1),\n"," (350, 1),\n"," (352, 1),\n"," (360, 1),\n"," (404, 1),\n"," (407, 1),\n"," (439, 1),\n"," (444, 2),\n"," (477, 1),\n"," (480, 1),\n"," (488, 1),\n"," (490, 1),\n"," (654, 5),\n"," (679, 1),\n"," (713, 1),\n"," (830, 1),\n"," (917, 1),\n"," (938, 1),\n"," (949, 1),\n"," (955, 1),\n"," (968, 3),\n"," (972, 1),\n"," (991, 1),\n"," (993, 1),\n"," (996, 1),\n"," (1040, 1),\n"," (1047, 1),\n"," (1051, 2),\n"," (1180, 1),\n"," (1239, 1),\n"," (1240, 2),\n"," (1247, 3),\n"," (1253, 1),\n"," (1301, 2),\n"," (1362, 1),\n"," (1439, 1),\n"," (1514, 2),\n"," (1551, 1),\n"," (1556, 1),\n"," (1587, 1),\n"," (1734, 1),\n"," (1822, 1),\n"," (1831, 1),\n"," (1954, 1),\n"," (2058, 1),\n"," (2059, 1),\n"," (2066, 1),\n"," (2121, 1),\n"," (2122, 1),\n"," (2123, 1),\n"," (2124, 1),\n"," (2125, 1),\n"," (2126, 1),\n"," (2127, 1),\n"," (2128, 1),\n"," (2129, 1)]"]},"metadata":{},"execution_count":17}],"source":["bow_corpus = [dictionary.doc2bow(doc) for doc in processed_docs]\n","# Saving the corpus\n","import pickle\n","pickle.dump(bow_corpus, open('bow_corpus.pkl', 'wb'))\n","bow_corpus[150]"]},{"cell_type":"code","execution_count":18,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1659535535981,"user":{"displayName":"Kabhilan S","userId":"04978609858624650983"},"user_tz":-330},"id":"3SEFbKFBZLWB","outputId":"0792f164-7ca1-49d1-8b97-f4e2b7ef3c45"},"outputs":[{"output_type":"stream","name":"stdout","text":["Word 21 (\"data\") appears 3 time.\n","Word 28 (\"direct\") appears 1 time.\n","Word 44 (\"generat\") appears 1 time.\n","Word 49 (\"improv\") appears 1 time.\n","Word 50 (\"individu\") appears 1 time.\n","Word 56 (\"level\") appears 1 time.\n","Word 73 (\"particular\") appears 1 time.\n","Word 81 (\"propos\") appears 1 time.\n","Word 101 (\"architectur\") appears 2 time.\n","Word 109 (\"learn\") appears 2 time.\n","Word 115 (\"paper\") appears 1 time.\n","Word 118 (\"recognit\") appears 1 time.\n","Word 124 (\"translat\") appears 1 time.\n","Word 132 (\"develop\") appears 1 time.\n","Word 182 (\"time\") appears 1 time.\n","Word 202 (\"machin\") appears 2 time.\n","Word 203 (\"make\") appears 1 time.\n","Word 208 (\"process\") appears 2 time.\n","Word 254 (\"possibl\") appears 1 time.\n","Word 270 (\"chang\") appears 1 time.\n","Word 299 (\"address\") appears 1 time.\n","Word 310 (\"environ\") appears 1 time.\n","Word 318 (\"increas\") appears 3 time.\n","Word 345 (\"context\") appears 1 time.\n","Word 350 (\"high\") appears 1 time.\n","Word 352 (\"impact\") appears 1 time.\n","Word 360 (\"potenti\") appears 1 time.\n","Word 404 (\"base\") appears 1 time.\n","Word 407 (\"comput\") appears 1 time.\n","Word 439 (\"product\") appears 1 time.\n","Word 444 (\"research\") appears 2 time.\n","Word 477 (\"help\") appears 1 time.\n","Word 480 (\"interact\") appears 1 time.\n","Word 488 (\"need\") appears 1 time.\n","Word 490 (\"organ\") appears 1 time.\n","Word 654 (\"system\") appears 5 time.\n","Word 679 (\"robust\") appears 1 time.\n","Word 713 (\"methodolog\") appears 1 time.\n","Word 830 (\"open\") appears 1 time.\n","Word 917 (\"adversari\") appears 1 time.\n","Word 938 (\"advanc\") appears 1 time.\n","Word 949 (\"deploy\") appears 1 time.\n","Word 955 (\"infrastructur\") appears 1 time.\n","Word 968 (\"technolog\") appears 3 time.\n","Word 972 (\"amount\") appears 1 time.\n","Word 991 (\"access\") appears 1 time.\n","Word 993 (\"artifici\") appears 1 time.\n","Word 996 (\"critic\") appears 1 time.\n","Word 1040 (\"move\") appears 1 time.\n","Word 1047 (\"broad\") appears 1 time.\n","Word 1051 (\"decis\") appears 2 time.\n","Word 1180 (\"softwar\") appears 1 time.\n","Word 1239 (\"person\") appears 1 time.\n","Word 1240 (\"promis\") appears 2 time.\n","Word 1247 (\"challeng\") appears 3 time.\n","Word 1253 (\"secur\") appears 1 time.\n","Word 1301 (\"live\") appears 2 time.\n","Word 1362 (\"widespread\") appears 1 time.\n","Word 1439 (\"rais\") appears 1 time.\n","Word 1514 (\"intellig\") appears 2 time.\n","Word 1551 (\"acceler\") appears 1 time.\n","Word 1556 (\"speech\") appears 1 time.\n","Word 1587 (\"constrain\") appears 1 time.\n","Word 1734 (\"frequent\") appears 1 time.\n","Word 1822 (\"vision\") appears 1 time.\n","Word 1831 (\"innov\") appears 1 time.\n","Word 1954 (\"digit\") appears 1 time.\n","Word 2058 (\"store\") appears 1 time.\n","Word 2059 (\"unpreced\") appears 1 time.\n","Word 2066 (\"mission\") appears 1 time.\n","Word 2121 (\"advertis\") appears 1 time.\n","Word 2122 (\"compromis\") appears 1 time.\n","Word 2123 (\"confidenti\") appears 1 time.\n","Word 2124 (\"moor\") appears 1 time.\n","Word 2125 (\"realiz\") appears 1 time.\n","Word 2126 (\"safe\") appears 1 time.\n","Word 2127 (\"societi\") appears 1 time.\n","Word 2128 (\"sophist\") appears 1 time.\n","Word 2129 (\"unpredict\") appears 1 time.\n"]}],"source":["bow_doc_1500 = bow_corpus[150]\n","\n","for i in range(len(bow_doc_1500)):\n","    print(\"Word {} (\\\"{}\\\") appears {} time.\".format(bow_doc_1500[i][0], \n","                                                     dictionary[bow_doc_1500[i][0]], \n","                                                     bow_doc_1500[i][1]))"]},{"cell_type":"markdown","metadata":{"id":"nfGSGBVTZLWC"},"source":["### TF-IDF"]},{"cell_type":"code","execution_count":19,"metadata":{"executionInfo":{"elapsed":2188,"status":"ok","timestamp":1659535538167,"user":{"displayName":"Kabhilan S","userId":"04978609858624650983"},"user_tz":-330},"id":"QOb2IIwKZLWC"},"outputs":[],"source":["from gensim import corpora, models\n","tfidf = models.TfidfModel(bow_corpus)"]},{"cell_type":"code","execution_count":20,"metadata":{"executionInfo":{"elapsed":6,"status":"ok","timestamp":1659535538167,"user":{"displayName":"Kabhilan S","userId":"04978609858624650983"},"user_tz":-330},"id":"J2YFOjvLZLWC"},"outputs":[],"source":["corpus_tfidf = tfidf[bow_corpus]"]},{"cell_type":"code","execution_count":21,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7,"status":"ok","timestamp":1659535538168,"user":{"displayName":"Kabhilan S","userId":"04978609858624650983"},"user_tz":-330},"id":"cekAQo3pZLWD","outputId":"77e3c3f6-7472-468c-f7f8-16e3f4b1b03f"},"outputs":[{"output_type":"stream","name":"stdout","text":["[(0, 0.04480517911086714),\n"," (1, 0.061456962892085985),\n"," (2, 0.02770820186099269),\n"," (3, 0.035970680135387595),\n"," (4, 0.0858365866514374),\n"," (5, 0.21970214053006895),\n"," (6, 0.06010135434695343),\n"," (7, 0.0440571817357302),\n"," (8, 0.0251798060922578),\n"," (9, 0.0570310496164714),\n"," (10, 0.04481695419383753),\n"," (11, 0.05303024447101651),\n"," (12, 0.07755484087063615),\n"," (13, 0.12278576362688469),\n"," (14, 0.0847567275752202),\n"," (15, 0.21709437829369346),\n"," (16, 0.0333363804249345),\n"," (17, 0.07482790661970028),\n"," (18, 0.10762255740322424),\n"," (19, 0.04913845911321858),\n"," (20, 0.11045466328254697),\n"," (21, 0.18313267237752462),\n"," (22, 0.07151698594390492),\n"," (23, 0.0815044946925878),\n"," (24, 0.03009455982810629),\n"," (25, 0.2551900633518117),\n"," (26, 0.0859766463750317),\n"," (27, 0.02695254337117091),\n"," (28, 0.03780327662330683),\n"," (29, 0.22601586955413702),\n"," (30, 0.0706568463938093),\n"," (31, 0.05771255323275502),\n"," (32, 0.04565029876966566),\n"," (33, 0.033207128963737974),\n"," (34, 0.03958628737327542),\n"," (35, 0.06058158976703328),\n"," (36, 0.039128527584585925),\n"," (37, 0.03244089053399748),\n"," (38, 0.04204617339342873),\n"," (39, 0.05605236766886192),\n"," (40, 0.08290384134645055),\n"," (41, 0.042358099119793644),\n"," (42, 0.07111003437869054),\n"," (43, 0.03813858052735978),\n"," (44, 0.03480304133716097),\n"," (45, 0.03323055448456489),\n"," (46, 0.10458663917815288),\n"," (47, 0.0501790663491052),\n"," (48, 0.042146155495492076),\n"," (49, 0.10454460963590223),\n"," (50, 0.052109519496365116),\n"," (51, 0.19686019562599288),\n"," (52, 0.03551388499445065),\n"," (53, 0.04992074239880939),\n"," (54, 0.10816178505039616),\n"," (55, 0.07440231974369764),\n"," (56, 0.08128393602396974),\n"," (57, 0.07700987892634538),\n"," (58, 0.08362036896551357),\n"," (59, 0.0683639681616585),\n"," (60, 0.05306893208468936),\n"," (61, 0.10435154292074737),\n"," (62, 0.05619203451341277),\n"," (63, 0.033573993341676454),\n"," (64, 0.10312157448166387),\n"," (65, 0.04263087843116678),\n"," (66, 0.09957688967734392),\n"," (67, 0.08589836221625134),\n"," (68, 0.07609660781557072),\n"," (69, 0.21424827202819724),\n"," (70, 0.052460498180821866),\n"," (71, 0.0697859854631025),\n"," (72, 0.03440202099931),\n"," (73, 0.03609557437755946),\n"," (74, 0.025493034650005165),\n"," (75, 0.06696451779578018),\n"," (76, 0.10312157448166387),\n"," (77, 0.0740385180461696),\n"," (78, 0.10943353946483632),\n"," (79, 0.05649873936601723),\n"," (80, 0.02367355785356918),\n"," (81, 0.04246549285279599),\n"," (82, 0.07798372723548383),\n"," (83, 0.12524078105969236),\n"," (84, 0.04328764753747948),\n"," (85, 0.03199430810965341),\n"," (86, 0.12679152390413925),\n"," (87, 0.016045148825589082),\n"," (88, 0.07691338479588992),\n"," (89, 0.06652308276507553),\n"," (90, 0.2398130602167959),\n"," (91, 0.028324560708075302),\n"," (92, 0.021870612758991704),\n"," (93, 0.4301987402446019),\n"," (94, 0.12336643075175847),\n"," (95, 0.08714182496443144),\n"," (96, 0.07839110464968842),\n"," (97, 0.03780327662330683),\n"," (98, 0.04510209183689279),\n"," (99, 0.10720047636678634)]\n"]}],"source":["from pprint import pprint\n","\n","for doc in corpus_tfidf:\n","    pprint(doc)\n","    break"]},{"cell_type":"markdown","metadata":{"id":"97EUHKRMXBkb"},"source":["### LDA"]},{"cell_type":"markdown","metadata":{"id":"uPs8Zn51WMxa"},"source":["P(w|t),\n","P(t|d),\n","P(w|t,d)=P(w|t)*P(t|d)![ai1.png](data:image/png;base64,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)\n","![image.png](data:image/png;base64,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)\n","![image.png](data:image/png;base64,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)"]},{"cell_type":"markdown","metadata":{"id":"myJZSrzmZLWD"},"source":["### Running LDA using Bag of Words"]},{"cell_type":"code","execution_count":22,"metadata":{"executionInfo":{"elapsed":47781,"status":"ok","timestamp":1659535585945,"user":{"displayName":"Kabhilan S","userId":"04978609858624650983"},"user_tz":-330},"id":"pjcr5xH3ZLWD"},"outputs":[],"source":["lda_model = gensim.models.LdaMulticore(bow_corpus, num_topics=5, id2word=dictionary, passes=2, workers=2, random_state= 2)\n","# Saving the model\n","lda_model.save('lda_model.gensim')"]},{"cell_type":"code","execution_count":23,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":12,"status":"ok","timestamp":1659535585946,"user":{"displayName":"Kabhilan S","userId":"04978609858624650983"},"user_tz":-330},"id":"BXfOxHJLQTVT","outputId":"423c3041-8034-4185-d4ad-3c9cfd4ca96d"},"outputs":[{"output_type":"stream","name":"stdout","text":["LdaModel(num_terms=4407, num_topics=5, decay=0.5, chunksize=2000)\n"]}],"source":["print(lda_model)"]},{"cell_type":"code","execution_count":24,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8,"status":"ok","timestamp":1659535585946,"user":{"displayName":"Kabhilan S","userId":"04978609858624650983"},"user_tz":-330},"id":"LTz_9x88ZLWE","outputId":"9c8cfc57-8be7-4852-acef-3b049f75bcb8"},"outputs":[{"output_type":"stream","name":"stdout","text":["Topic: 0 \n","Words: 0.021*\"network\" + 0.017*\"model\" + 0.014*\"learn\" + 0.010*\"data\" + 0.009*\"method\" + 0.008*\"propos\" + 0.008*\"train\" + 0.007*\"neural\" + 0.007*\"base\" + 0.007*\"imag\"\n","Topic: 1 \n","Words: 0.007*\"model\" + 0.007*\"field\" + 0.006*\"observ\" + 0.006*\"result\" + 0.005*\"time\" + 0.005*\"energi\" + 0.005*\"studi\" + 0.005*\"phase\" + 0.005*\"state\" + 0.005*\"effect\"\n","Topic: 2 \n","Words: 0.016*\"algorithm\" + 0.014*\"problem\" + 0.011*\"estim\" + 0.011*\"data\" + 0.010*\"method\" + 0.009*\"optim\" + 0.009*\"model\" + 0.008*\"propos\" + 0.008*\"function\" + 0.008*\"result\"\n","Topic: 3 \n","Words: 0.017*\"model\" + 0.011*\"data\" + 0.011*\"method\" + 0.009*\"base\" + 0.009*\"propos\" + 0.008*\"approach\" + 0.008*\"time\" + 0.007*\"test\" + 0.006*\"perform\" + 0.006*\"paper\"\n","Topic: 4 \n","Words: 0.010*\"result\" + 0.010*\"space\" + 0.008*\"problem\" + 0.007*\"group\" + 0.007*\"model\" + 0.007*\"general\" + 0.007*\"paper\" + 0.007*\"function\" + 0.007*\"graph\" + 0.007*\"prove\"\n"]}],"source":["for idx, topic in lda_model.print_topics(-1):\n","    print('Topic: {} \\nWords: {}'.format(idx, topic))"]},{"cell_type":"markdown","metadata":{"id":"R1-ge3AvZLWE"},"source":["### Running LDA using TF-IDF"]},{"cell_type":"code","execution_count":25,"metadata":{"executionInfo":{"elapsed":40704,"status":"ok","timestamp":1659535626644,"user":{"displayName":"Kabhilan S","userId":"04978609858624650983"},"user_tz":-330},"id":"uV9Q5iuZZLWE"},"outputs":[],"source":["lda_model_tfidf = gensim.models.LdaMulticore(corpus_tfidf, num_topics=5, id2word=dictionary, passes=2, workers=4, random_state=2)\n","# Saving the model\n","lda_model.save('lda_model_tf_idf.gensim')"]},{"cell_type":"code","execution_count":26,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":19,"status":"ok","timestamp":1659535626644,"user":{"displayName":"Kabhilan S","userId":"04978609858624650983"},"user_tz":-330},"id":"MFVWiDCAZLWE","outputId":"dd3291e0-0eb0-408b-ff23-b88795e4b425"},"outputs":[{"output_type":"stream","name":"stdout","text":["Topic: 0 Word: 0.005*\"network\" + 0.004*\"model\" + 0.004*\"learn\" + 0.003*\"imag\" + 0.003*\"data\" + 0.003*\"train\" + 0.003*\"method\" + 0.003*\"state\" + 0.003*\"neural\" + 0.003*\"magnet\"\n","Topic: 1 Word: 0.004*\"mathbb\" + 0.004*\"group\" + 0.004*\"algebra\" + 0.003*\"star\" + 0.003*\"space\" + 0.003*\"prove\" + 0.003*\"mathcal\" + 0.003*\"field\" + 0.003*\"equat\" + 0.002*\"function\"\n","Topic: 2 Word: 0.005*\"algorithm\" + 0.004*\"problem\" + 0.004*\"estim\" + 0.004*\"function\" + 0.004*\"method\" + 0.004*\"model\" + 0.003*\"data\" + 0.003*\"distribut\" + 0.003*\"optim\" + 0.003*\"graph\"\n","Topic: 3 Word: 0.005*\"model\" + 0.005*\"estim\" + 0.004*\"data\" + 0.004*\"method\" + 0.003*\"test\" + 0.003*\"propos\" + 0.003*\"approach\" + 0.003*\"algorithm\" + 0.003*\"distribut\" + 0.003*\"time\"\n","Topic: 4 Word: 0.004*\"robot\" + 0.004*\"learn\" + 0.004*\"algorithm\" + 0.004*\"network\" + 0.003*\"agent\" + 0.003*\"control\" + 0.003*\"optim\" + 0.003*\"model\" + 0.003*\"game\" + 0.003*\"problem\"\n"]}],"source":["for idx, topic in lda_model_tfidf.print_topics(-1):\n","    print('Topic: {} Word: {}'.format(idx, topic))"]},{"cell_type":"markdown","metadata":{"id":"56skrzD-ZLWG"},"source":["### Classification of the topics\n","### Performance evaluation by classifying sample document using LDA Bag of Words model\n"]},{"cell_type":"code","execution_count":27,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":17,"status":"ok","timestamp":1659535626645,"user":{"displayName":"Kabhilan S","userId":"04978609858624650983"},"user_tz":-330},"id":"kdsgeiMjZLWG","outputId":"ce8433a2-bec4-451c-c4af-b17dc42aa5c9"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["['increas',\n"," 'commodit',\n"," 'vision',\n"," 'speech',\n"," 'recognit',\n"," 'machin',\n"," 'translat',\n"," 'system',\n"," 'widespread',\n"," 'deploy',\n"," 'learn',\n"," 'base',\n"," 'technolog',\n"," 'digit',\n"," 'advertis',\n"," 'intellig',\n"," 'infrastructur',\n"," 'artifici',\n"," 'intellig',\n"," 'move',\n"," 'research',\n"," 'lab',\n"," 'product',\n"," 'chang',\n"," 'possibl',\n"," 'unpreced',\n"," 'level',\n"," 'data',\n"," 'comput',\n"," 'methodolog',\n"," 'advanc',\n"," 'machin',\n"," 'learn',\n"," 'innov',\n"," 'system',\n"," 'softwar',\n"," 'architectur',\n"," 'broad',\n"," 'access',\n"," 'technolog',\n"," 'generat',\n"," 'system',\n"," 'promis',\n"," 'acceler',\n"," 'develop',\n"," 'increas',\n"," 'impact',\n"," 'live',\n"," 'frequent',\n"," 'interact',\n"," 'make',\n"," 'mission',\n"," 'critic',\n"," 'decis',\n"," 'behalf',\n"," 'high',\n"," 'person',\n"," 'context',\n"," 'realiz',\n"," 'promis',\n"," 'rais',\n"," 'daunt',\n"," 'challeng',\n"," 'particular',\n"," 'need',\n"," 'system',\n"," 'time',\n"," 'safe',\n"," 'decis',\n"," 'unpredict',\n"," 'environ',\n"," 'robust',\n"," 'sophist',\n"," 'adversari',\n"," 'process',\n"," 'increas',\n"," 'amount',\n"," 'data',\n"," 'organ',\n"," 'individu',\n"," 'compromis',\n"," 'confidenti',\n"," 'challeng',\n"," 'exacerb',\n"," 'moor',\n"," 'constrain',\n"," 'data',\n"," 'technolog',\n"," 'store',\n"," 'process',\n"," 'paper',\n"," 'propos',\n"," 'open',\n"," 'research',\n"," 'direct',\n"," 'system',\n"," 'architectur',\n"," 'secur',\n"," 'address',\n"," 'challeng',\n"," 'help',\n"," 'unlock',\n"," 'potenti',\n"," 'improv',\n"," 'live',\n"," 'societi']"]},"metadata":{},"execution_count":27}],"source":["processed_docs[150]"]},{"cell_type":"code","execution_count":28,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":14,"status":"ok","timestamp":1659535626645,"user":{"displayName":"Kabhilan S","userId":"04978609858624650983"},"user_tz":-330},"id":"gIcfWHF1ZLWG","outputId":"70698561-1a1b-42a2-ce3f-811222da961f"},"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Score: 0.8299204111099243\t \n","Topic: 0.021*\"network\" + 0.017*\"model\" + 0.014*\"learn\" + 0.010*\"data\" + 0.009*\"method\" + 0.008*\"propos\" + 0.008*\"train\" + 0.007*\"neural\" + 0.007*\"base\" + 0.007*\"imag\"\n","\n","Score: 0.16394580900669098\t \n","Topic: 0.017*\"model\" + 0.011*\"data\" + 0.011*\"method\" + 0.009*\"base\" + 0.009*\"propos\" + 0.008*\"approach\" + 0.008*\"time\" + 0.007*\"test\" + 0.006*\"perform\" + 0.006*\"paper\"\n"]}],"source":["for index, score in sorted(lda_model[bow_corpus[150]], key=lambda tup: -1*tup[1]):\n","    print(\"\\nScore: {}\\t \\nTopic: {}\".format(score, lda_model.print_topic(index, 10)))"]},{"cell_type":"markdown","metadata":{"id":"FotC9aJdZLWH"},"source":["### Performance evaluation by classifying sample document using LDA TF-IDF model"]},{"cell_type":"code","execution_count":29,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":10,"status":"ok","timestamp":1659535626645,"user":{"displayName":"Kabhilan S","userId":"04978609858624650983"},"user_tz":-330},"id":"yjUlUSJYZLWH","outputId":"30fc9b1d-c191-45c5-a3c9-c44ddfb88cff"},"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Score: 0.5927186012268066\t \n","Topic: 0.004*\"mathbb\" + 0.004*\"group\" + 0.004*\"algebra\" + 0.003*\"star\" + 0.003*\"space\" + 0.003*\"prove\" + 0.003*\"mathcal\" + 0.003*\"field\" + 0.003*\"equat\" + 0.002*\"function\"\n","\n","Score: 0.3973928689956665\t \n","Topic: 0.005*\"network\" + 0.004*\"model\" + 0.004*\"learn\" + 0.003*\"imag\" + 0.003*\"data\" + 0.003*\"train\" + 0.003*\"method\" + 0.003*\"state\" + 0.003*\"neural\" + 0.003*\"magnet\"\n"]}],"source":["for index, score in sorted(lda_model_tfidf[bow_corpus[1500]], key=lambda tup: -1*tup[1]):\n","    print(\"\\nScore: {}\\t \\nTopic: {}\".format(score, lda_model_tfidf.print_topic(index, 10)))"]},{"cell_type":"markdown","metadata":{"id":"NKiEFVdIZLWH"},"source":["### Testing model on seen document"]},{"cell_type":"code","execution_count":30,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8,"status":"ok","timestamp":1659535626645,"user":{"displayName":"Kabhilan S","userId":"04978609858624650983"},"user_tz":-330},"id":"lCWpz4AUZLWH","outputId":"89efa44e-f9c8-4375-80f1-a3df1f5ddef6"},"outputs":[{"output_type":"stream","name":"stdout","text":["Score: 0.41279223561286926\t Topic: 0.016*\"algorithm\" + 0.014*\"problem\" + 0.011*\"estim\" + 0.011*\"data\" + 0.010*\"method\"\n","Score: 0.31288954615592957\t Topic: 0.007*\"model\" + 0.007*\"field\" + 0.006*\"observ\" + 0.006*\"result\" + 0.005*\"time\"\n","Score: 0.2675166726112366\t Topic: 0.010*\"result\" + 0.010*\"space\" + 0.008*\"problem\" + 0.007*\"group\" + 0.007*\"model\"\n"]}],"source":["bow_vector = dictionary.doc2bow(preprocess(documents['text'][3]))\n","for index, score in sorted(lda_model[bow_vector], key=lambda tup: -1*tup[1]):\n","    print(\"Score: {}\\t Topic: {}\".format(score, lda_model.print_topic(index, 5)))"]},{"cell_type":"markdown","metadata":{"id":"TSz1n78wZLWI"},"source":["### Updating the topic and probability columns with respective values in decreasing order"]},{"cell_type":"code","execution_count":31,"metadata":{"executionInfo":{"elapsed":65869,"status":"ok","timestamp":1659535692507,"user":{"displayName":"Kabhilan S","userId":"04978609858624650983"},"user_tz":-330},"id":"DAVENIfeZLWI"},"outputs":[],"source":["for index in range(len(documents)):\n","    topicsOrder=[]\n","    bow_vector = dictionary.doc2bow(preprocess(documents['text'][index]))\n","    topicsOrder = list(map(list, list(lda_model.get_document_topics(bow_vector))))\n","    topicsOrder.sort(key=lambda x: x[1],reverse=True)\n","    # Printing the sorted probabilities\n","    # print(topicsOrder)\n","    for i in range(len(topicsOrder)):\n","        topic = \"Topic\"+str(i+1)\n","        documents.at[index,topic] = (topicsOrder[i][0])\n","        prob = \"Prob\"+str(i+1)\n","        documents.at[index,prob] = (topicsOrder[i][1])"]},{"cell_type":"code","execution_count":32,"metadata":{"executionInfo":{"elapsed":30,"status":"ok","timestamp":1659535692515,"user":{"displayName":"Kabhilan S","userId":"04978609858624650983"},"user_tz":-330},"id":"l_EFNR_WTyYv"},"outputs":[],"source":["db=documents.copy()"]},{"cell_type":"code","execution_count":33,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":30,"status":"ok","timestamp":1659535692516,"user":{"displayName":"Kabhilan S","userId":"04978609858624650983"},"user_tz":-330},"id":"-3JOAvRzbehB","outputId":"78f31ea5-e533-46fd-df59-f17f824c1e99"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["0        0.0\n","1        0.0\n","2        4.0\n","3        2.0\n","4        0.0\n","        ... \n","20967    0.0\n","20968    1.0\n","20969    0.0\n","20970    2.0\n","20971    3.0\n","Name: Topic1, Length: 20972, dtype: float64"]},"metadata":{},"execution_count":33}],"source":["\n","db.Topic1\n"]},{"cell_type":"code","execution_count":34,"metadata":{"executionInfo":{"elapsed":27,"status":"ok","timestamp":1659535692516,"user":{"displayName":"Kabhilan S","userId":"04978609858624650983"},"user_tz":-330},"id":"fSlyZnsNZLWI"},"outputs":[],"source":["# Labeling the topic numbers in documents dataframe\n","documents.Topic1=documents.Topic1.replace(0, \"First Topic\").replace(1, \"Second Topic\").replace(2, \"Third Topic\").replace(3, \"Fourth Topic\").replace(4, \"Fifth Topic\")\n","documents.Topic2=documents.Topic2.replace(0, \"First Topic\").replace(1, \"Second Topic\").replace(2, \"Third Topic\").replace(3, \"Fourth Topic\").replace(4, \"Fifth Topic\")\n","documents.Topic3=documents.Topic3.replace(0, \"First Topic\").replace(1, \"Second Topic\").replace(2, \"Third Topic\").replace(3, \"Fourth Topic\").replace(4, \"Fifth Topic\")\n","documents.Topic4=documents.Topic4.replace(0, \"First Topic\").replace(1, \"Second Topic\").replace(2, \"Third Topic\").replace(3, \"Fourth Topic\").replace(4, \"Fifth Topic\")\n","documents.Topic5=documents.Topic5.replace(0, \"First Topic\").replace(1, \"Second Topic\").replace(2, \"Third Topic\").replace(3, \"Fourth Topic\").replace(4, \"Fifth Topic\")"]},{"cell_type":"code","execution_count":35,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":677},"executionInfo":{"elapsed":28,"status":"ok","timestamp":1659535692517,"user":{"displayName":"Kabhilan S","userId":"04978609858624650983"},"user_tz":-330},"id":"2e9pYrgEZLWJ","outputId":"6586bb8f-79a6-47d9-d826-b7470b9a3d9d"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["    Index  _id                                               text  \\\n","0       1    1    Predictive models allow subject-specific inf...   \n","1       2    2    Rotation invariance and translation invarian...   \n","2       3    3    We introduce and develop the notion of spher...   \n","3       4    4    The stochastic Landau--Lifshitz--Gilbert (LL...   \n","4       5    5    Fourier-transform infra-red (FTIR) spectra o...   \n","5       6    6    Let $\\Omega \\subset \\mathbb{R}^n$ be a bound...   \n","6       7    7    We observed the newly discovered hyperbolic ...   \n","7       8    8    The ability of metallic nanoparticles to sup...   \n","8       9    9    We model large-scale ($\\approx$2000km) impac...   \n","9      10   10    Time varying susceptibility of host at indiv...   \n","10     11   11    We present a systematic global sensitivity a...   \n","11     12   12    \"Three is a crowd\" is an old proverb that ap...   \n","12     13   13    We study the exciton magnetic polaron (EMP) ...   \n","13     14   14    The classical Eilenberg correspondence, base...   \n","14     15   15    Using low-temperature Magnetic Force Microsc...   \n","15     16   16    The recent discovery that the exponent of ma...   \n","16     17   17    The process that leads to the formation of t...   \n","17     18   18    We describe a variant construction of the un...   \n","18     19   19    When investigators seek to estimate causal e...   \n","19     20   20    Assigning homogeneous boundary conditions, s...   \n","\n","          Topic1     Prob1        Topic2     Prob2        Topic3     Prob3  \\\n","0    First Topic  0.627490  Fourth Topic  0.260451   Third Topic  0.109456   \n","1    First Topic  0.780916   Fifth Topic  0.204142           NaN       NaN   \n","2    Fifth Topic  0.981835           NaN       NaN           NaN       NaN   \n","3    Third Topic  0.413008  Second Topic  0.312975   Fifth Topic  0.267216   \n","4    First Topic  0.730031   Third Topic  0.261180           NaN       NaN   \n","5   Second Topic  0.536035   Fifth Topic  0.457264           NaN       NaN   \n","6   Second Topic  0.979785           NaN       NaN           NaN       NaN   \n","7   Second Topic  0.980012  Fourth Topic  0.010954           NaN       NaN   \n","8   Second Topic  0.987652           NaN       NaN           NaN       NaN   \n","9   Fourth Topic  0.761117  Second Topic  0.231277           NaN       NaN   \n","10  Second Topic  0.742426  Fourth Topic  0.252982           NaN       NaN   \n","11  Second Topic  0.442434   Fifth Topic  0.234011   First Topic  0.197382   \n","12  Second Topic  0.965450   First Topic  0.025800           NaN       NaN   \n","13   Fifth Topic  0.988045           NaN       NaN           NaN       NaN   \n","14  Second Topic  0.988246           NaN       NaN           NaN       NaN   \n","15   Fifth Topic  0.544423   Third Topic  0.430998           NaN       NaN   \n","16  Second Topic  0.993721           NaN       NaN           NaN       NaN   \n","17   Fifth Topic  0.967457           NaN       NaN           NaN       NaN   \n","18   Third Topic  0.853205  Fourth Topic  0.107663   First Topic  0.035334   \n","19  Second Topic  0.581341   Third Topic  0.308433  Fourth Topic  0.107414   \n","\n","         Topic4     Prob4 Topic5  Prob5  \n","0           NaN       NaN    NaN    NaN  \n","1           NaN       NaN    NaN    NaN  \n","2           NaN       NaN    NaN    NaN  \n","3           NaN       NaN    NaN    NaN  \n","4           NaN       NaN    NaN    NaN  \n","5           NaN       NaN    NaN    NaN  \n","6           NaN       NaN    NaN    NaN  \n","7           NaN       NaN    NaN    NaN  \n","8           NaN       NaN    NaN    NaN  \n","9           NaN       NaN    NaN    NaN  \n","10          NaN       NaN    NaN    NaN  \n","11  Third Topic  0.118004    NaN    NaN  \n","12          NaN       NaN    NaN    NaN  \n","13          NaN       NaN    NaN    NaN  \n","14          NaN       NaN    NaN    NaN  \n","15          NaN       NaN    NaN    NaN  \n","16          NaN       NaN    NaN    NaN  \n","17          NaN       NaN    NaN    NaN  \n","18          NaN       NaN    NaN    NaN  \n","19          NaN       NaN    NaN    NaN  "],"text/html":["\n","  <div id=\"df-1b567acd-20e1-4d0d-91c7-c2d944f27904\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Index</th>\n","      <th>_id</th>\n","      <th>text</th>\n","      <th>Topic1</th>\n","      <th>Prob1</th>\n","      <th>Topic2</th>\n","      <th>Prob2</th>\n","      <th>Topic3</th>\n","      <th>Prob3</th>\n","      <th>Topic4</th>\n","      <th>Prob4</th>\n","      <th>Topic5</th>\n","      <th>Prob5</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>Predictive models allow subject-specific inf...</td>\n","      <td>First Topic</td>\n","      <td>0.627490</td>\n","      <td>Fourth Topic</td>\n","      <td>0.260451</td>\n","      <td>Third Topic</td>\n","      <td>0.109456</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>2</td>\n","      <td>2</td>\n","      <td>Rotation invariance and translation invarian...</td>\n","      <td>First Topic</td>\n","      <td>0.780916</td>\n","      <td>Fifth Topic</td>\n","      <td>0.204142</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>3</td>\n","      <td>3</td>\n","      <td>We introduce and develop the notion of spher...</td>\n","      <td>Fifth Topic</td>\n","      <td>0.981835</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>4</td>\n","      <td>4</td>\n","      <td>The stochastic Landau--Lifshitz--Gilbert (LL...</td>\n","      <td>Third Topic</td>\n","      <td>0.413008</td>\n","      <td>Second Topic</td>\n","      <td>0.312975</td>\n","      <td>Fifth Topic</td>\n","      <td>0.267216</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>5</td>\n","      <td>5</td>\n","      <td>Fourier-transform infra-red (FTIR) spectra o...</td>\n","      <td>First Topic</td>\n","      <td>0.730031</td>\n","      <td>Third Topic</td>\n","      <td>0.261180</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>6</td>\n","      <td>6</td>\n","      <td>Let $\\Omega \\subset \\mathbb{R}^n$ be a bound...</td>\n","      <td>Second Topic</td>\n","      <td>0.536035</td>\n","      <td>Fifth Topic</td>\n","      <td>0.457264</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>7</td>\n","      <td>7</td>\n","      <td>We observed the newly discovered hyperbolic ...</td>\n","      <td>Second Topic</td>\n","      <td>0.979785</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>8</td>\n","      <td>8</td>\n","      <td>The ability of metallic nanoparticles to sup...</td>\n","      <td>Second Topic</td>\n","      <td>0.980012</td>\n","      <td>Fourth Topic</td>\n","      <td>0.010954</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>9</td>\n","      <td>9</td>\n","      <td>We model large-scale ($\\approx$2000km) impac...</td>\n","      <td>Second Topic</td>\n","      <td>0.987652</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>10</td>\n","      <td>10</td>\n","      <td>Time varying susceptibility of host at indiv...</td>\n","      <td>Fourth Topic</td>\n","      <td>0.761117</td>\n","      <td>Second Topic</td>\n","      <td>0.231277</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>10</th>\n","      <td>11</td>\n","      <td>11</td>\n","      <td>We present a systematic global sensitivity a...</td>\n","      <td>Second Topic</td>\n","      <td>0.742426</td>\n","      <td>Fourth Topic</td>\n","      <td>0.252982</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>11</th>\n","      <td>12</td>\n","      <td>12</td>\n","      <td>\"Three is a crowd\" is an old proverb that ap...</td>\n","      <td>Second Topic</td>\n","      <td>0.442434</td>\n","      <td>Fifth Topic</td>\n","      <td>0.234011</td>\n","      <td>First Topic</td>\n","      <td>0.197382</td>\n","      <td>Third Topic</td>\n","      <td>0.118004</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>12</th>\n","      <td>13</td>\n","      <td>13</td>\n","      <td>We study the exciton magnetic polaron (EMP) ...</td>\n","      <td>Second Topic</td>\n","      <td>0.965450</td>\n","      <td>First Topic</td>\n","      <td>0.025800</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>13</th>\n","      <td>14</td>\n","      <td>14</td>\n","      <td>The classical Eilenberg correspondence, base...</td>\n","      <td>Fifth Topic</td>\n","      <td>0.988045</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>14</th>\n","      <td>15</td>\n","      <td>15</td>\n","      <td>Using low-temperature Magnetic Force Microsc...</td>\n","      <td>Second Topic</td>\n","      <td>0.988246</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>15</th>\n","      <td>16</td>\n","      <td>16</td>\n","      <td>The recent discovery that the exponent of ma...</td>\n","      <td>Fifth Topic</td>\n","      <td>0.544423</td>\n","      <td>Third Topic</td>\n","      <td>0.430998</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>16</th>\n","      <td>17</td>\n","      <td>17</td>\n","      <td>The process that leads to the formation of t...</td>\n","      <td>Second Topic</td>\n","      <td>0.993721</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>17</th>\n","      <td>18</td>\n","      <td>18</td>\n","      <td>We describe a variant construction of the un...</td>\n","      <td>Fifth Topic</td>\n","      <td>0.967457</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>18</th>\n","      <td>19</td>\n","      <td>19</td>\n","      <td>When investigators seek to estimate causal e...</td>\n","      <td>Third Topic</td>\n","      <td>0.853205</td>\n","      <td>Fourth Topic</td>\n","      <td>0.107663</td>\n","      <td>First Topic</td>\n","      <td>0.035334</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>19</th>\n","      <td>20</td>\n","      <td>20</td>\n","      <td>Assigning homogeneous boundary conditions, s...</td>\n","      <td>Second Topic</td>\n","      <td>0.581341</td>\n","      <td>Third Topic</td>\n","      <td>0.308433</td>\n","      <td>Fourth Topic</td>\n","      <td>0.107414</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-1b567acd-20e1-4d0d-91c7-c2d944f27904')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-1b567acd-20e1-4d0d-91c7-c2d944f27904 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-1b567acd-20e1-4d0d-91c7-c2d944f27904');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":35}],"source":["documents.head(20)"]},{"cell_type":"markdown","metadata":{"id":"Pyz0RlGhZLWJ"},"source":["### Dropping the unnecessary columns from the dataframe documents"]},{"cell_type":"code","execution_count":36,"metadata":{"executionInfo":{"elapsed":27,"status":"ok","timestamp":1659535692517,"user":{"displayName":"Kabhilan S","userId":"04978609858624650983"},"user_tz":-330},"id":"oh29E3rsZLWJ"},"outputs":[],"source":["documents_dict=documents\n","documents_dict=documents_dict.drop('text',axis=1)"]},{"cell_type":"markdown","metadata":{"id":"7z7CRPpPZLWJ"},"source":["### Creating a list of dictionary in a proper format"]},{"cell_type":"code","execution_count":37,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":26,"status":"ok","timestamp":1659535692517,"user":{"displayName":"Kabhilan S","userId":"04978609858624650983"},"user_tz":-330},"id":"HsChZIliZLWK","outputId":"d6349359-33d4-47f1-a684-cf6ada311178"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:3: FutureWarning: Using short name for 'orient' is deprecated. Only the options: ('dict', list, 'series', 'split', 'records', 'index') will be used in a future version. Use one of the above to silence this warning.\n","  This is separate from the ipykernel package so we can avoid doing imports until\n"]}],"source":["documents_dict = documents_dict.replace(np.nan, '', regex=True)\n","# Creates a list of dictionaries\n","documents_dict = documents_dict.to_dict('record')\n","# Delete the keys with null values\n","for dict1 in documents_dict:\n","    empty_key=[]\n","    for key, value in dict1.items():\n","        if(value == ''):\n","            empty_key.append(key)\n","    for name in empty_key:\n","        dict1.pop(name)"]},{"cell_type":"code","execution_count":38,"metadata":{"executionInfo":{"elapsed":25,"status":"ok","timestamp":1659535692518,"user":{"displayName":"Kabhilan S","userId":"04978609858624650983"},"user_tz":-330},"id":"FxziEGCtZLWK"},"outputs":[],"source":["# Saving the json file\n","with open('topic_document_lda_model.json', 'w') as f:\n","    json.dump(documents_dict, f, ensure_ascii=False)"]},{"cell_type":"code","execution_count":39,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":25,"status":"ok","timestamp":1659535692518,"user":{"displayName":"Kabhilan S","userId":"04978609858624650983"},"user_tz":-330},"id":"l3aeuvmm44xK","outputId":"919ca049-99eb-49b8-df28-e85e3d787f6c"},"outputs":[{"output_type":"stream","name":"stdout","text":["Score: 0.9513707756996155\t Topic: 0.016*\"algorithm\" + 0.014*\"problem\" + 0.011*\"estim\" + 0.011*\"data\" + 0.010*\"method\" + 0.009*\"optim\" + 0.009*\"model\" + 0.008*\"propos\" + 0.008*\"function\" + 0.008*\"result\"\n","Score: 0.01230707298964262\t Topic: 0.007*\"model\" + 0.007*\"field\" + 0.006*\"observ\" + 0.006*\"result\" + 0.005*\"time\" + 0.005*\"energi\" + 0.005*\"studi\" + 0.005*\"phase\" + 0.005*\"state\" + 0.005*\"effect\"\n","Score: 0.01225071307271719\t Topic: 0.010*\"result\" + 0.010*\"space\" + 0.008*\"problem\" + 0.007*\"group\" + 0.007*\"model\" + 0.007*\"general\" + 0.007*\"paper\" + 0.007*\"function\" + 0.007*\"graph\" + 0.007*\"prove\"\n","Score: 0.01206338219344616\t Topic: 0.021*\"network\" + 0.017*\"model\" + 0.014*\"learn\" + 0.010*\"data\" + 0.009*\"method\" + 0.008*\"propos\" + 0.008*\"train\" + 0.007*\"neural\" + 0.007*\"base\" + 0.007*\"imag\"\n","Score: 0.01200805138796568\t Topic: 0.017*\"model\" + 0.011*\"data\" + 0.011*\"method\" + 0.009*\"base\" + 0.009*\"propos\" + 0.008*\"approach\" + 0.008*\"time\" + 0.007*\"test\" + 0.006*\"perform\" + 0.006*\"paper\"\n"]}],"source":["unseen_document = 'The recent discovery that the exponent of matrix multiplication is determined by the rank of the symmetrized matrix multiplication tensor has invigorated interest in better understanding symmetrized matrix multiplication'\n","bow_vector = dictionary.doc2bow(preprocess(unseen_document))\n","for index, score in sorted(lda_model[bow_vector], key=lambda tup: -1*tup[1]):\n","    print(\"Score: {}\\t Topic: {}\".format(score, lda_model.print_topic(index, 10)))"]},{"cell_type":"code","execution_count":40,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":23,"status":"ok","timestamp":1659535692518,"user":{"displayName":"Kabhilan S","userId":"04978609858624650983"},"user_tz":-330},"id":"gK4nF8dCEHen","outputId":"5e5f2f34-5415-4f23-dd67-ca3d0ab27828"},"outputs":[{"output_type":"stream","name":"stdout","text":["Score: 0.012063339352607727\t Topic: 0.021*\"network\" + 0.017*\"model\" + 0.014*\"learn\" + 0.010*\"data\" + 0.009*\"method\" + 0.008*\"propos\" + 0.008*\"train\" + 0.007*\"neural\" + 0.007*\"base\" + 0.007*\"imag\"\n","Score: 0.012307061813771725\t Topic: 0.007*\"model\" + 0.007*\"field\" + 0.006*\"observ\" + 0.006*\"result\" + 0.005*\"time\" + 0.005*\"energi\" + 0.005*\"studi\" + 0.005*\"phase\" + 0.005*\"state\" + 0.005*\"effect\"\n","Score: 0.9513882994651794\t Topic: 0.016*\"algorithm\" + 0.014*\"problem\" + 0.011*\"estim\" + 0.011*\"data\" + 0.010*\"method\" + 0.009*\"optim\" + 0.009*\"model\" + 0.008*\"propos\" + 0.008*\"function\" + 0.008*\"result\"\n","Score: 0.012008031830191612\t Topic: 0.017*\"model\" + 0.011*\"data\" + 0.011*\"method\" + 0.009*\"base\" + 0.009*\"propos\" + 0.008*\"approach\" + 0.008*\"time\" + 0.007*\"test\" + 0.006*\"perform\" + 0.006*\"paper\"\n","Score: 0.012233232147991657\t Topic: 0.010*\"result\" + 0.010*\"space\" + 0.008*\"problem\" + 0.007*\"group\" + 0.007*\"model\" + 0.007*\"general\" + 0.007*\"paper\" + 0.007*\"function\" + 0.007*\"graph\" + 0.007*\"prove\"\n"]}],"source":["unseen_document = 'The recent discovery that the exponent of matrix multiplication is determined by the rank of the symmetrized matrix multiplication tensor has invigorated interest in better understanding symmetrized matrix multiplication'\n","bow_vector = dictionary.doc2bow(preprocess(unseen_document))\n","for index, score in lda_model[bow_vector]:\n","    print(\"Score: {}\\t Topic: {}\".format(score, lda_model.print_topic(index, 10)))"]},{"cell_type":"code","execution_count":41,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":22,"status":"ok","timestamp":1659535692519,"user":{"displayName":"Kabhilan S","userId":"04978609858624650983"},"user_tz":-330},"id":"2bni0i5MZLWK","outputId":"f417ac31-ca5c-4114-a370-81cd20f950c5"},"outputs":[{"output_type":"stream","name":"stdout","text":["0        0.0\n","1        0.0\n","2        4.0\n","3        2.0\n","4        0.0\n","        ... \n","20967    0.0\n","20968    1.0\n","20969    0.0\n","20970    2.0\n","20971    3.0\n","Name: Topic1, Length: 20972, dtype: float64\n"]}],"source":["print(db.Topic1)"]},{"cell_type":"code","execution_count":42,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":21,"status":"ok","timestamp":1659535692519,"user":{"displayName":"Kabhilan S","userId":"04978609858624650983"},"user_tz":-330},"id":"n_Xrey-pe-p7","outputId":"dd369f1e-aa79-435a-c5dc-32bd7d7cf423"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["272636"]},"metadata":{},"execution_count":42}],"source":["db.size"]},{"cell_type":"code","execution_count":43,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4476,"status":"ok","timestamp":1659535696976,"user":{"displayName":"Kabhilan S","userId":"04978609858624650983"},"user_tz":-330},"id":"66NGsv0EwXP8","outputId":"3c36fb1c-a728-49f6-fa98-134429312e6d"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting flask_ngrok\n","  Downloading flask_ngrok-0.0.25-py3-none-any.whl (3.1 kB)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from flask_ngrok) (2.23.0)\n","Requirement already satisfied: Flask>=0.8 in /usr/local/lib/python3.7/dist-packages (from flask_ngrok) (1.1.4)\n","Requirement already satisfied: click<8.0,>=5.1 in /usr/local/lib/python3.7/dist-packages (from Flask>=0.8->flask_ngrok) (7.1.2)\n","Requirement already satisfied: Jinja2<3.0,>=2.10.1 in /usr/local/lib/python3.7/dist-packages (from Flask>=0.8->flask_ngrok) (2.11.3)\n","Requirement already satisfied: Werkzeug<2.0,>=0.15 in /usr/local/lib/python3.7/dist-packages (from Flask>=0.8->flask_ngrok) (1.0.1)\n","Requirement already satisfied: itsdangerous<2.0,>=0.24 in /usr/local/lib/python3.7/dist-packages (from Flask>=0.8->flask_ngrok) (1.1.0)\n","Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from Jinja2<3.0,>=2.10.1->Flask>=0.8->flask_ngrok) (2.0.1)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->flask_ngrok) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->flask_ngrok) (2022.6.15)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->flask_ngrok) (1.24.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->flask_ngrok) (2.10)\n","Installing collected packages: flask-ngrok\n","Successfully installed flask-ngrok-0.0.25\n"]}],"source":["pip install flask_ngrok"]},{"cell_type":"code","execution_count":44,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2534,"status":"ok","timestamp":1659535699498,"user":{"displayName":"Kabhilan S","userId":"04978609858624650983"},"user_tz":-330},"id":"BfSIR3ubmsmA","outputId":"671455c0-599c-415e-d871-48b33bf1482f"},"outputs":[{"output_type":"stream","name":"stdout","text":["['  Following the recent progress in image classification and captioning using\\ndeep learning, we develop a novel natural language person retrieval system\\nbased on an attention mechanism. More specifically, given the description of a\\nperson, the goal is to localize the person in an image. To this end, we first\\nconstruct a benchmark dataset for natural language person retrieval. To do so,\\nwe generate bounding boxes for persons in a public image dataset from the\\nsegmentation masks, which are then annotated with descriptions and attributes\\nusing the Amazon Mechanical Turk. We then adopt a region proposal network in\\nFaster R-CNN as a candidate region generator. The cropped images based on the\\nregion proposals as well as the whole images with attention weights are fed\\ninto Convolutional Neural Networks for visual feature extraction, while the\\nnatural language expression and attributes are input to Bidirectional Long\\nShort- Term Memory (BLSTM) models for text feature extraction. The visual and\\ntext features are integrated to score region proposals, and the one with the\\nhighest score is retrieved as the output of our system. The experimental\\nresults show significant improvement over the state-of-the-art method for\\ngeneric object retrieval and this line of research promises to benefit search\\nin surveillance video footage.\\n', '  Machine learning algorithms such as linear regression, SVM and neural network\\nhave played an increasingly important role in the process of scientific\\ndiscovery. However, none of them is both interpretable and accurate on\\nnonlinear datasets. Here we present contextual regression, a method that joins\\nthese two desirable properties together using a hybrid architecture of neural\\nnetwork embedding and dot product layer. We demonstrate its high prediction\\naccuracy and sensitivity through the task of predictive feature selection on a\\nsimulated dataset and the application of predicting open chromatin sites in the\\nhuman genome. On the simulated data, our method achieved high fidelity recovery\\nof feature contributions under random noise levels up to 200%. On the open\\nchromatin dataset, the application of our method not only outperformed the\\nstate of the art method in terms of accuracy, but also unveiled two previously\\nunfound open chromatin related histone marks. Our method can fill the blank of\\naccurate and interpretable nonlinear modeling in scientific data mining tasks.\\n', '  Many people are suffering from voice disorders, which can adversely affect\\nthe quality of their lives. In response, some researchers have proposed\\nalgorithms for automatic assessment of these disorders, based on voice signals.\\nHowever, these signals can be sensitive to the recording devices. Indeed, the\\nchannel effect is a pervasive problem in machine learning for healthcare. In\\nthis study, we propose a detection system for pathological voice, which is\\nrobust against the channel effect. This system is based on a bidirectional LSTM\\nnetwork. To increase the performance robustness against channel mismatch, we\\nintegrate domain adversarial training (DAT) to eliminate the differences\\nbetween the devices. When we train on data recorded on a high-quality\\nmicrophone and evaluate on smartphone data without labels, our robust detection\\nsystem increases the PR-AUC from 0.8448 to 0.9455 (and 0.9522 with target\\nsample labels). To the best of our knowledge, this is the first study applying\\nunsupervised domain adaptation to pathological voice detection. Notably, our\\nsystem does not need target device sample labels, which allows for\\ngeneralization to many new devices.\\n', '  End-to-end approaches have drawn much attention recently for significantly\\nsimplifying the construction of an automatic speech recognition (ASR) system.\\nRNN transducer (RNN-T) is one of the popular end-to-end methods. Previous\\nstudies have shown that RNN-T is difficult to train and a very complex training\\nprocess is needed for a reasonable performance. In this paper, we explore RNN-T\\nfor a Chinese large vocabulary continuous speech recognition (LVCSR) task and\\naim to simplify the training process while maintaining performance. First, a\\nnew strategy of learning rate decay is proposed to accelerate the model\\nconvergence. Second, we find that adding convolutional layers at the beginning\\nof the network and using ordered data can discard the pre-training process of\\nthe encoder without loss of performance. Besides, we design experiments to find\\na balance among the usage of GPU memory, training circle and model performance.\\nFinally, we achieve 16.9% character error rate (CER) on our test set which is\\n2% absolute improvement from a strong BLSTM CE system with language model\\ntrained on the same text corpus.\\n', \"  Neuroscientists classify neurons into different types that perform similar\\ncomputations at different locations in the visual field. Traditional methods\\nfor neural system identification do not capitalize on this separation of 'what'\\nand 'where'. Learning deep convolutional feature spaces that are shared among\\nmany neurons provides an exciting path forward, but the architectural design\\nneeds to account for data limitations: While new experimental techniques enable\\nrecordings from thousands of neurons, experimental time is limited so that one\\ncan sample only a small fraction of each neuron's response space. Here, we show\\nthat a major bottleneck for fitting convolutional neural networks (CNNs) to\\nneural data is the estimation of the individual receptive field locations, a\\nproblem that has been scratched only at the surface thus far. We propose a CNN\\narchitecture with a sparse readout layer factorizing the spatial (where) and\\nfeature (what) dimensions. Our network scales well to thousands of neurons and\\nshort recordings and can be trained end-to-end. We evaluate this architecture\\non ground-truth data to explore the challenges and limitations of CNN-based\\nsystem identification. Moreover, we show that our network model outperforms\\ncurrent state-of-the art system identification models of mouse primary visual\\ncortex.\\n\", '  Social media has changed the ways of communication, where everyone is\\nequipped with the power to express their opinions to others in online\\ndiscussion platforms. Previously, a number of stud- ies have been presented to\\nidentify opinion leaders in online discussion networks. Feng (\"Are you\\nconnected? Evaluating information cascade in online discussion about the\\n#RaceTogether campaign\", Computers in Human Behavior, 2016) identified five\\ntypes of central users and their communication patterns in an online\\ncommunication network of a limited time span. However, to trace the change in\\ncommunication pattern, a long-term analysis is required. In this study, we\\ncritically analyzed framework presented by Feng based on five types of central\\nusers in online communication network and their communication pattern in a\\nlong-term manner. We take another case study presented by Udnor et al.\\n(\"Determining social media impact on the politics of developing countries using\\nsocial network analytics\", Program, 2016) to further understand the dynamics as\\nwell as to perform validation . Results indicate that there may not exist all\\nof these central users in an online communication network in a long-term\\nmanner. Furthermore, we discuss the changing positions of opinion leaders and\\ntheir power to keep isolates interested in an online discussion network.\\n', '  Developing neural network image classification models often requires\\nsignificant architecture engineering. In this paper, we study a method to learn\\nthe model architectures directly on the dataset of interest. As this approach\\nis expensive when the dataset is large, we propose to search for an\\narchitectural building block on a small dataset and then transfer the block to\\na larger dataset. The key contribution of this work is the design of a new\\nsearch space (the \"NASNet search space\") which enables transferability. In our\\nexperiments, we search for the best convolutional layer (or \"cell\") on the\\nCIFAR-10 dataset and then apply this cell to the ImageNet dataset by stacking\\ntogether more copies of this cell, each with their own parameters to design a\\nconvolutional architecture, named \"NASNet architecture\". We also introduce a\\nnew regularization technique called ScheduledDropPath that significantly\\nimproves generalization in the NASNet models. On CIFAR-10 itself, NASNet\\nachieves 2.4% error rate, which is state-of-the-art. On ImageNet, NASNet\\nachieves, among the published works, state-of-the-art accuracy of 82.7% top-1\\nand 96.2% top-5 on ImageNet. Our model is 1.2% better in top-1 accuracy than\\nthe best human-invented architectures while having 9 billion fewer FLOPS - a\\nreduction of 28% in computational demand from the previous state-of-the-art\\nmodel. When evaluated at different levels of computational cost, accuracies of\\nNASNets exceed those of the state-of-the-art human-designed models. For\\ninstance, a small version of NASNet also achieves 74% top-1 accuracy, which is\\n3.1% better than equivalently-sized, state-of-the-art models for mobile\\nplatforms. Finally, the learned features by NASNet used with the Faster-RCNN\\nframework surpass state-of-the-art by 4.0% achieving 43.1% mAP on the COCO\\ndataset.\\n', \"  Large-scale datasets have played a significant role in progress of neural\\nnetwork and deep learning areas. YouTube-8M is such a benchmark dataset for\\ngeneral multi-label video classification. It was created from over 7 million\\nYouTube videos (450,000 hours of video) and includes video labels from a\\nvocabulary of 4716 classes (3.4 labels/video on average). It also comes with\\npre-extracted audio & visual features from every second of video (3.2 billion\\nfeature vectors in total). Google cloud recently released the datasets and\\norganized 'Google Cloud & YouTube-8M Video Understanding Challenge' on Kaggle.\\nCompetitors are challenged to develop classification algorithms that assign\\nvideo-level labels using the new and improved Youtube-8M V2 dataset. Inspired\\nby the competition, we started exploration of audio understanding and\\nclassification using deep learning algorithms and ensemble methods. We built\\nseveral baseline predictions according to the benchmark paper and public github\\ntensorflow code. Furthermore, we improved global prediction accuracy (GAP) from\\nbase level 77% to 80.7% through approaches of ensemble.\\n\", '  In this paper we present a novel methodology for identifying scholars with a\\nTwitter account. By combining bibliometric data from Web of Science and Twitter\\nusers identified by Altmetric.com we have obtained the largest set of\\nindividual scholars matched with Twitter users made so far. Our methodology\\nconsists of a combination of matching algorithms, considering different\\nlinguistic elements of both author names and Twitter names; followed by a\\nrule-based scoring system that weights the common occurrence of several\\nelements related with the names, individual elements and activities of both\\nTwitter users and scholars matched. Our results indicate that about 2% of the\\noverall population of scholars in the Web of Science is active on Twitter. By\\ndomain we find a strong presence of researchers from the Social Sciences and\\nthe Humanities. Natural Sciences is the domain with the lowest level of\\nscholars on Twitter. Researchers on Twitter also tend to be younger than those\\nthat are not on Twitter. As this is a bibliometric-based approach, it is\\nimportant to highlight the reliance of the method on the number of publications\\nproduced and tweeted by the scholars, thus the share of scholars on Twitter\\nranges between 1% and 5% depending on their level of productivity. Further\\nresearch is suggested in order to improve and expand the methodology.\\n', '  In processing human produced text using natural language processing (NLP)\\ntechniques, two fundamental subtasks that arise are (i) segmentation of the\\nplain text into meaningful subunits (e.g., entities), and (ii) dependency\\nparsing, to establish relations between subunits. In this paper, we develop a\\nrelatively simple and effective neural joint model that performs both\\nsegmentation and dependency parsing together, instead of one after the other as\\nin most state-of-the-art works. We will focus in particular on the real estate\\nad setting, aiming to convert an ad to a structured description, which we name\\nproperty tree, comprising the tasks of (1) identifying important entities of a\\nproperty (e.g., rooms) from classifieds and (2) structuring them into a tree\\nformat. In this work, we propose a new joint model that is able to tackle the\\ntwo tasks simultaneously and construct the property tree by (i) avoiding the\\nerror propagation that would arise from the subtasks one after the other in a\\npipelined fashion, and (ii) exploiting the interactions between the subtasks.\\nFor this purpose, we perform an extensive comparative study of the pipeline\\nmethods and the new proposed joint model, reporting an improvement of over\\nthree percentage points in the overall edge F1 score of the property tree.\\nAlso, we propose attention methods, to encourage our model to focus on salient\\ntokens during the construction of the property tree. Thus we experimentally\\ndemonstrate the usefulness of attentive neural architectures for the proposed\\njoint model, showcasing a further improvement of two percentage points in edge\\nF1 score for our application.\\n', '  Deep convolutional neural networks (CNNs) have recently achieved great\\nsuccess in many visual recognition tasks. However, existing deep neural network\\nmodels are computationally expensive and memory intensive, hindering their\\ndeployment in devices with low memory resources or in applications with strict\\nlatency requirements. Therefore, a natural thought is to perform model\\ncompression and acceleration in deep networks without significantly decreasing\\nthe model performance. During the past few years, tremendous progress has been\\nmade in this area. In this paper, we survey the recent advanced techniques for\\ncompacting and accelerating CNNs model developed. These techniques are roughly\\ncategorized into four schemes: parameter pruning and sharing, low-rank\\nfactorization, transferred/compact convolutional filters, and knowledge\\ndistillation. Methods of parameter pruning and sharing will be described at the\\nbeginning, after that the other techniques will be introduced. For each scheme,\\nwe provide insightful analysis regarding the performance, related applications,\\nadvantages, and drawbacks etc. Then we will go through a few very recent\\nadditional successful methods, for example, dynamic capacity networks and\\nstochastic depths networks. After that, we survey the evaluation matrix, the\\nmain datasets used for evaluating the model performance and recent benchmarking\\nefforts. Finally, we conclude this paper, discuss remaining challenges and\\npossible directions on this topic.\\n', '  This paper proposes a non-parallel many-to-many voice conversion (VC) method\\nusing a variant of the conditional variational autoencoder (VAE) called an\\nauxiliary classifier VAE (ACVAE). The proposed method has three key features.\\nFirst, it adopts fully convolutional architectures to construct the encoder and\\ndecoder networks so that the networks can learn conversion rules that capture\\ntime dependencies in the acoustic feature sequences of source and target\\nspeech. Second, it uses an information-theoretic regularization for the model\\ntraining to ensure that the information in the attribute class label will not\\nbe lost in the conversion process. With regular CVAEs, the encoder and decoder\\nare free to ignore the attribute class label input. This can be problematic\\nsince in such a situation, the attribute class label will have little effect on\\ncontrolling the voice characteristics of input speech at test time. Such\\nsituations can be avoided by introducing an auxiliary classifier and training\\nthe encoder and decoder so that the attribute classes of the decoder outputs\\nare correctly predicted by the classifier. Third, it avoids producing\\nbuzzy-sounding speech at test time by simply transplanting the spectral details\\nof the input speech into its converted version. Subjective evaluation\\nexperiments revealed that this simple method worked reasonably well in a\\nnon-parallel many-to-many speaker identity conversion task.\\n', '  One of the most basic skills a robot should possess is predicting the effect\\nof physical interactions with objects in the environment. This enables optimal\\naction selection to reach a certain goal state. Traditionally, dynamics are\\napproximated by physics-based analytical models. These models rely on specific\\nstate representations that may be hard to obtain from raw sensory data,\\nespecially if no knowledge of the object shape is assumed. More recently, we\\nhave seen learning approaches that can predict the effect of complex physical\\ninteractions directly from sensory input. It is however an open question how\\nfar these models generalize beyond their training data. In this work, we\\ninvestigate the advantages and limitations of neural network based learning\\napproaches for predicting the effects of actions based on sensory input and\\nshow how analytical and learned models can be combined to leverage the best of\\nboth worlds. As physical interaction task, we use planar pushing, for which\\nthere exists a well-known analytical model and a large real-world dataset. We\\npropose to use a convolutional neural network to convert raw depth images or\\norganized point clouds into a suitable representation for the analytical model\\nand compare this approach to using neural networks for both, perception and\\nprediction. A systematic evaluation of the proposed approach on a very large\\nreal-world dataset shows two main advantages of the hybrid architecture.\\nCompared to a pure neural network, it significantly (i) reduces required\\ntraining data and (ii) improves generalization to novel physical interaction.\\n', \"  Theory of Mind is the ability to attribute mental states (beliefs, intents,\\nknowledge, perspectives, etc.) to others and recognize that these mental states\\nmay differ from one's own. Theory of Mind is critical to effective\\ncommunication and to teams demonstrating higher collective performance. To\\neffectively leverage the progress in Artificial Intelligence (AI) to make our\\nlives more productive, it is important for humans and AI to work well together\\nin a team. Traditionally, there has been much emphasis on research to make AI\\nmore accurate, and (to a lesser extent) on having it better understand human\\nintentions, tendencies, beliefs, and contexts. The latter involves making AI\\nmore human-like and having it develop a theory of our minds. In this work, we\\nargue that for human-AI teams to be effective, humans must also develop a\\ntheory of AI's mind (ToAIM) - get to know its strengths, weaknesses, beliefs,\\nand quirks. We instantiate these ideas within the domain of Visual Question\\nAnswering (VQA). We find that using just a few examples (50), lay people can be\\ntrained to better predict responses and oncoming failures of a complex VQA\\nmodel. We further evaluate the role existing explanation (or interpretability)\\nmodalities play in helping humans build ToAIM. Explainable AI has received\\nconsiderable scientific and popular attention in recent times. Surprisingly, we\\nfind that having access to the model's internal states - its confidence in its\\ntop-k predictions, explicit or implicit attention maps which highlight regions\\nin the image (and words in the question) the model is looking at (and listening\\nto) while answering a question about an image - do not help people better\\npredict its behavior.\\n\", '  We present a clustering-based language model using word embeddings for text\\nreadability prediction. Presumably, an Euclidean semantic space hypothesis\\nholds true for word embeddings whose training is done by observing word\\nco-occurrences. We argue that clustering with word embeddings in the metric\\nspace should yield feature representations in a higher semantic space\\nappropriate for text regression. Also, by representing features in terms of\\nhistograms, our approach can naturally address documents of varying lengths. An\\nempirical evaluation using the Common Core Standards corpus reveals that the\\nfeatures formed on our clustering-based language model significantly improve\\nthe previously known results for the same corpus in readability prediction. We\\nalso evaluate the task of sentence matching based on semantic relatedness using\\nthe Wiki-SimpleWiki corpus and find that our features lead to superior matching\\nperformance.\\n', '  In automatic speech processing systems, speaker diarization is a crucial\\nfront-end component to separate segments from different speakers. Inspired by\\nthe recent success of deep neural networks (DNNs) in semantic inferencing,\\ntriplet loss-based architectures have been successfully used for this problem.\\nHowever, existing work utilizes conventional i-vectors as the input\\nrepresentation and builds simple fully connected networks for metric learning,\\nthus not fully leveraging the modeling power of DNN architectures. This paper\\ninvestigates the importance of learning effective representations from the\\nsequences directly in metric learning pipelines for speaker diarization. More\\nspecifically, we propose to employ attention models to learn embeddings and the\\nmetric jointly in an end-to-end fashion. Experiments are conducted on the\\nCALLHOME conversational speech corpus. The diarization results demonstrate\\nthat, besides providing a unified model, the proposed approach achieves\\nimproved performance when compared against existing approaches.\\n', '  The best summary of a long video differs among different people due to its\\nhighly subjective nature. Even for the same person, the best summary may change\\nwith time or mood. In this paper, we introduce the task of generating\\ncustomized video summaries through simple text. First, we train a deep\\narchitecture to effectively learn semantic embeddings of video frames by\\nleveraging the abundance of image-caption data via a progressive and residual\\nmanner. Given a user-specific text description, our algorithm is able to select\\nsemantically relevant video segments and produce a temporally aligned video\\nsummary. In order to evaluate our textually customized video summaries, we\\nconduct experimental comparison with baseline methods that utilize ground-truth\\ninformation. Despite the challenging baselines, our method still manages to\\nshow comparable or even exceeding performance. We also show that our method is\\nable to generate semantically diverse video summaries by only utilizing the\\nlearned visual embeddings.\\n', \"  In recent years, research has been done on applying Recurrent Neural Networks\\n(RNNs) as recommender systems. Results have been promising, especially in the\\nsession-based setting where RNNs have been shown to outperform state-of-the-art\\nmodels. In many of these experiments, the RNN could potentially improve the\\nrecommendations by utilizing information about the user's past sessions, in\\naddition to its own interactions in the current session. A problem for\\nsession-based recommendation, is how to produce accurate recommendations at the\\nstart of a session, before the system has learned much about the user's current\\ninterests. We propose a novel approach that extends a RNN recommender to be\\nable to process the user's recent sessions, in order to improve\\nrecommendations. This is done by using a second RNN to learn from recent\\nsessions, and predict the user's interest in the current session. By feeding\\nthis information to the original RNN, it is able to improve its\\nrecommendations. Our experiments on two different datasets show that the\\nproposed approach can significantly improve recommendations throughout the\\nsessions, compared to a single RNN working only on the current session. The\\nproposed model especially improves recommendations at the start of sessions,\\nand is therefore able to deal with the cold start problem within sessions.\\n\", '  A complex system can be represented and analyzed as a network, where nodes\\nrepresent the units of the network and edges represent connections between\\nthose units. For example, a brain network represents neurons as nodes and axons\\nbetween neurons as edges. In many networks, some nodes have a\\ndisproportionately high number of edges. These nodes also have many edges\\nbetween each other, and are referred to as the rich club. In many different\\nnetworks, the nodes of this club are assumed to support global network\\nintegration. However, another set of nodes potentially exhibits a connectivity\\nstructure that is more advantageous to global network integration. Here, in a\\nmyriad of different biological and man-made networks, we discover the diverse\\nclub--a set of nodes that have edges diversely distributed across the network.\\nThe diverse club exhibits, to a greater extent than the rich club, properties\\nconsistent with an integrative network function--these nodes are more highly\\ninterconnected and their edges are more critical for efficient global\\nintegration. Moreover, we present a generative evolutionary network model that\\nproduces networks with a diverse club but not a rich club, thus demonstrating\\nthat these two clubs potentially evolved via distinct selection pressures.\\nGiven the variety of different networks that we analyzed--the c. elegans, the\\nmacaque brain, the human brain, the United States power grid, and global air\\ntraffic--the diverse club appears to be ubiquitous in complex networks. These\\nresults warrant the distinction and analysis of two critical clubs of nodes in\\nall complex systems.\\n', '  Deep learning has been demonstrated to achieve excellent results for image\\nclassification and object detection. However, the impact of deep learning on\\nvideo analysis (e.g. action detection and recognition) has been limited due to\\ncomplexity of video data and lack of annotations. Previous convolutional neural\\nnetworks (CNN) based video action detection approaches usually consist of two\\nmajor steps: frame-level action proposal detection and association of proposals\\nacross frames. Also, these methods employ two-stream CNN framework to handle\\nspatial and temporal feature separately. In this paper, we propose an\\nend-to-end deep network called Tube Convolutional Neural Network (T-CNN) for\\naction detection in videos. The proposed architecture is a unified network that\\nis able to recognize and localize action based on 3D convolution features. A\\nvideo is first divided into equal length clips and for each clip a set of tube\\nproposals are generated next based on 3D Convolutional Network (ConvNet)\\nfeatures. Finally, the tube proposals of different clips are linked together\\nemploying network flow and spatio-temporal action detection is performed using\\nthese linked video proposals. Extensive experiments on several video datasets\\ndemonstrate the superior performance of T-CNN for classifying and localizing\\nactions in both trimmed and untrimmed videos compared to state-of-the-arts.\\n', '  Very often features come with their own vectorial descriptions which provide\\ndetailed information about their properties. We refer to these vectorial\\ndescriptions as feature side-information. In the standard learning scenario,\\ninput is represented as a vector of features and the feature side-information\\nis most often ignored or used only for feature selection prior to model\\nfitting. We believe that feature side-information which carries information\\nabout features intrinsic property will help improve model prediction if used in\\na proper way during learning process. In this paper, we propose a framework\\nthat allows for the incorporation of the feature side-information during the\\nlearning of very general model families to improve the prediction performance.\\nWe control the structures of the learned models so that they reflect features\\nsimilarities as these are defined on the basis of the side-information. We\\nperform experiments on a number of benchmark datasets which show significant\\npredictive performance gains, over a number of baselines, as a result of the\\nexploitation of the side-information.\\n', '  Convolutional Neural Networks (CNNs) are commonly thought to recognise\\nobjects by learning increasingly complex representations of object shapes. Some\\nrecent studies suggest a more important role of image textures. We here put\\nthese conflicting hypotheses to a quantitative test by evaluating CNNs and\\nhuman observers on images with a texture-shape cue conflict. We show that\\nImageNet-trained CNNs are strongly biased towards recognising textures rather\\nthan shapes, which is in stark contrast to human behavioural evidence and\\nreveals fundamentally different classification strategies. We then demonstrate\\nthat the same standard architecture (ResNet-50) that learns a texture-based\\nrepresentation on ImageNet is able to learn a shape-based representation\\ninstead when trained on \"Stylized-ImageNet\", a stylized version of ImageNet.\\nThis provides a much better fit for human behavioural performance in our\\nwell-controlled psychophysical lab setting (nine experiments totalling 48,560\\npsychophysical trials across 97 observers) and comes with a number of\\nunexpected emergent benefits such as improved object detection performance and\\npreviously unseen robustness towards a wide range of image distortions,\\nhighlighting advantages of a shape-based representation.\\n', \"  An important, yet largely unstudied, problem in student data analysis is to\\ndetect misconceptions from students' responses to open-response questions.\\nMisconception detection enables instructors to deliver more targeted feedback\\non the misconceptions exhibited by many students in their class, thus improving\\nthe quality of instruction. In this paper, we propose a new natural language\\nprocessing-based framework to detect the common misconceptions among students'\\ntextual responses to short-answer questions. We propose a probabilistic model\\nfor students' textual responses involving misconceptions and experimentally\\nvalidate it on a real-world student-response dataset. Experimental results show\\nthat our proposed framework excels at classifying whether a response exhibits\\none or more misconceptions. More importantly, it can also automatically detect\\nthe common misconceptions exhibited across responses from multiple students to\\nmultiple questions; this property is especially important at large scale, since\\ninstructors will no longer need to manually specify all possible misconceptions\\nthat students might exhibit.\\n\", '  In this paper, we introduce a simple, yet powerful pipeline for medical image\\nsegmentation that combines Fully Convolutional Networks (FCNs) with Fully\\nConvolutional Residual Networks (FC-ResNets). We propose and examine a design\\nthat takes particular advantage of recent advances in the understanding of both\\nConvolutional Neural Networks as well as ResNets. Our approach focuses upon the\\nimportance of a trainable pre-processing when using FC-ResNets and we show that\\na low-capacity FCN model can serve as a pre-processor to normalize medical\\ninput data. In our image segmentation pipeline, we use FCNs to obtain\\nnormalized images, which are then iteratively refined by means of a FC-ResNet\\nto generate a segmentation prediction. As in other fully convolutional\\napproaches, our pipeline can be used off-the-shelf on different image\\nmodalities. We show that using this pipeline, we exhibit state-of-the-art\\nperformance on the challenging Electron Microscopy benchmark, when compared to\\nother 2D methods. We improve segmentation results on CT images of liver\\nlesions, when contrasting with standard FCN methods. Moreover, when applying\\nour 2D pipeline on a challenging 3D MRI prostate segmentation challenge we\\nreach results that are competitive even when compared to 3D methods. The\\nobtained results illustrate the strong potential and versatility of the\\npipeline by achieving highly accurate results on multi-modality images from\\ndifferent anatomical regions and organs.\\n', '  Deep learning has demonstrated tremendous potential for Automatic Text\\nScoring (ATS) tasks. In this paper, we describe a new neural architecture that\\nenhances vanilla neural network models with auxiliary neural coherence\\nfeatures. Our new method proposes a new \\\\textsc{SkipFlow} mechanism that models\\nrelationships between snapshots of the hidden representations of a long\\nshort-term memory (LSTM) network as it reads. Subsequently, the semantic\\nrelationships between multiple snapshots are used as auxiliary features for\\nprediction. This has two main benefits. Firstly, essays are typically long\\nsequences and therefore the memorization capability of the LSTM network may be\\ninsufficient. Implicit access to multiple snapshots can alleviate this problem\\nby acting as a protection against vanishing gradients. The parameters of the\\n\\\\textsc{SkipFlow} mechanism also acts as an auxiliary memory. Secondly,\\nmodeling relationships between multiple positions allows our model to learn\\nfeatures that represent and approximate textual coherence. In our model, we\\ncall this \\\\textit{neural coherence} features. Overall, we present a unified\\ndeep learning architecture that generates neural coherence features as it reads\\nin an end-to-end fashion. Our approach demonstrates state-of-the-art\\nperformance on the benchmark ASAP dataset, outperforming not only feature\\nengineering baselines but also other deep learning models.\\n', '  Developers increasingly rely on text matching tools to analyze the relation\\nbetween natural language words and APIs. However, semantic gaps, namely textual\\nmismatches between words and APIs, negatively affect these tools. Previous\\nstudies have transformed words or APIs into low-dimensional vectors for\\nmatching; however, inaccurate results were obtained due to the failure of\\nmodeling words and APIs simultaneously. To resolve this problem, two main\\nchallenges are to be addressed: the acquisition of massive words and APIs for\\nmining and the alignment of words and APIs for modeling. Therefore, this study\\nproposes Word2API to effectively estimate relatedness of words and APIs.\\nWord2API collects millions of commonly used words and APIs from code\\nrepositories to address the acquisition challenge. Then, a shuffling strategy\\nis used to transform related words and APIs into tuples to address the\\nalignment challenge. Using these tuples, Word2API models words and APIs\\nsimultaneously. Word2API outperforms baselines by 10%-49.6% of relatedness\\nestimation in terms of precision and NDCG. Word2API is also effective on\\nsolving typical software tasks, e.g., query expansion and API documents\\nlinking. A simple system with Word2API-expanded queries recommends up to 21.4%\\nmore related APIs for developers. Meanwhile, Word2API improves comparison\\nalgorithms by 7.9%-17.4% in linking questions in Question&Answer communities to\\nAPI documents.\\n', '  Extensive efforts have been devoted to recognizing facial action units (AUs).\\nHowever, it is still challenging to recognize AUs from spontaneous facial\\ndisplays especially when they are accompanied with speech. Different from all\\nprior work that utilized visual observations for facial AU recognition, this\\npaper presents a novel approach that recognizes speech-related AUs exclusively\\nfrom audio signals based on the fact that facial activities are highly\\ncorrelated with voice during speech. Specifically, dynamic and physiological\\nrelationships between AUs and phonemes are modeled through a continuous time\\nBayesian network (CTBN); then AU recognition is performed by probabilistic\\ninference via the CTBN model.\\nA pilot audiovisual AU-coded database has been constructed to evaluate the\\nproposed audio-based AU recognition framework. The database consists of a\\n\"clean\" subset with frontal and neutral faces and a challenging subset\\ncollected with large head movements and occlusions. Experimental results on\\nthis database show that the proposed CTBN model achieves promising recognition\\nperformance for 7 speech-related AUs and outperforms the state-of-the-art\\nvisual-based methods especially for those AUs that are activated at low\\nintensities or \"hardly visible\" in the visual channel. Furthermore, the CTBN\\nmodel yields more impressive recognition performance on the challenging subset,\\nwhere the visual-based approaches suffer significantly.\\n', '  We introduce a new paradigm that is important for community detection in the\\nrealm of network analysis. Networks contain a set of strong, dominant\\ncommunities, which interfere with the detection of weak, natural community\\nstructure. When most of the members of the weak communities also belong to\\nstronger communities, they are extremely hard to be uncovered. We call the weak\\ncommunities the hidden community structure.\\nWe present a novel approach called HICODE (HIdden COmmunity DEtection) that\\nidentifies the hidden community structure as well as the dominant community\\nstructure. By weakening the strength of the dominant structure, one can uncover\\nthe hidden structure beneath. Likewise, by reducing the strength of the hidden\\nstructure, one can more accurately identify the dominant structure. In this\\nway, HICODE tackles both tasks simultaneously.\\nExtensive experiments on real-world networks demonstrate that HICODE\\noutperforms several state-of-the-art community detection methods in uncovering\\nboth the dominant and the hidden structure. In the Facebook university social\\nnetworks, we find multiple non-redundant sets of communities that are strongly\\nassociated with residential hall, year of registration or career position of\\nthe faculties or students, while the state-of-the-art algorithms mainly locate\\nthe dominant ground truth category. In the Due to the difficulty of labeling\\nall ground truth communities in real-world datasets, HICODE provides a\\npromising approach to pinpoint the existing latent communities and uncover\\ncommunities for which there is no ground truth. Finding this unknown structure\\nis an extremely important community detection problem.\\n', '  Artificial neural networks have been successfully applied to a variety of\\nmachine learning tasks, including image recognition, semantic segmentation, and\\nmachine translation. However, few studies fully investigated ensembles of\\nartificial neural networks. In this work, we investigated multiple widely used\\nensemble methods, including unweighted averaging, majority voting, the Bayes\\nOptimal Classifier, and the (discrete) Super Learner, for image recognition\\ntasks, with deep neural networks as candidate algorithms. We designed several\\nexperiments, with the candidate algorithms being the same network structure\\nwith different model checkpoints within a single training process, networks\\nwith same structure but trained multiple times stochastically, and networks\\nwith different structure. In addition, we further studied the over-confidence\\nphenomenon of the neural networks, as well as its impact on the ensemble\\nmethods. Across all of our experiments, the Super Learner achieved best\\nperformance among all the ensemble methods in this study.\\n', '  Convolutional neural networks have recently demonstrated high-quality\\nreconstruction for single-image super-resolution. In this paper, we propose the\\nLaplacian Pyramid Super-Resolution Network (LapSRN) to progressively\\nreconstruct the sub-band residuals of high-resolution images. At each pyramid\\nlevel, our model takes coarse-resolution feature maps as input, predicts the\\nhigh-frequency residuals, and uses transposed convolutions for upsampling to\\nthe finer level. Our method does not require the bicubic interpolation as the\\npre-processing step and thus dramatically reduces the computational complexity.\\nWe train the proposed LapSRN with deep supervision using a robust Charbonnier\\nloss function and achieve high-quality reconstruction. Furthermore, our network\\ngenerates multi-scale predictions in one feed-forward pass through the\\nprogressive reconstruction, thereby facilitates resource-aware applications.\\nExtensive quantitative and qualitative evaluations on benchmark datasets show\\nthat the proposed algorithm performs favorably against the state-of-the-art\\nmethods in terms of speed and accuracy.\\n', '  We explore different approaches to integrating a simple convolutional neural\\nnetwork (CNN) with the Lucene search engine in a multi-stage ranking\\narchitecture. Our models are trained using the PyTorch deep learning toolkit,\\nwhich is implemented in C/C++ with a Python frontend. One obvious integration\\nstrategy is to expose the neural network directly as a service. For this, we\\nuse Apache Thrift, a software framework for building scalable cross-language\\nservices. In exploring alternative architectures, we observe that once trained,\\nthe feedforward evaluation of neural networks is quite straightforward.\\nTherefore, we can extract the parameters of a trained CNN from PyTorch and\\nimport the model into Java, taking advantage of the Java Deeplearning4J library\\nfor feedforward evaluation. This has the advantage that the entire end-to-end\\nsystem can be implemented in Java. As a third approach, we can extract the\\nneural network from PyTorch and \"compile\" it into a C++ program that exposes a\\nThrift service. We evaluate these alternatives in terms of performance (latency\\nand throughput) as well as ease of integration. Experiments show that\\nfeedforward evaluation of the convolutional neural network is significantly\\nslower in Java, while the performance of the compiled C++ network does not\\nconsistently beat the PyTorch implementation.\\n', '  Background: As most of the software development organizations are\\nmale-dominated, female developers encountering various negative workplace\\nexperiences reported feeling like they \"do not belong\". Exposures to\\ndiscriminatory expletives or negative critiques from their male colleagues may\\nfurther exacerbate those feelings. Aims: The primary goal of this study is to\\nidentify the differences in expressions of sentiments between male and female\\ndevelopers during various software engineering tasks. Method: On this goal, we\\nmined the code review repositories of six popular open source projects. We used\\na semi-automated approach leveraging the name as well as multiple social\\nnetworks to identify the gender of a developer. Using SentiSE, a customized and\\nstate-of-the-art sentiment analysis tool for the software engineering domain,\\nwe classify each communication as negative, positive, or neutral. We also\\ncompute the frequencies of sentiment words, emoticons, and expletives used by\\neach developer. Results: Our results suggest that the likelihood of using\\nsentiment words, emoticons, and expletives during code reviews varies based on\\nthe gender of a developer, as females are significantly less likely to express\\nsentiments than males. Although female developers were more neutral to their\\nmale colleagues than to another female, male developers from three out of the\\nsix projects were not only writing more frequent negative comments but also\\nwithholding positive encouragements from their female counterparts. Conclusion:\\nOur results provide empirical evidence of another factor behind the negative\\nwork place experiences encountered by the female developers that may be\\ncontributing to the diminishing number of females in the SE industry.\\n', '  Semantic segmentation and object detection research have recently achieved\\nrapid progress. However, the former task has no notion of different instances\\nof the same object, and the latter operates at a coarse, bounding-box level. We\\npropose an Instance Segmentation system that produces a segmentation map where\\neach pixel is assigned an object class and instance identity label. Most\\napproaches adapt object detectors to produce segments instead of boxes. In\\ncontrast, our method is based on an initial semantic segmentation module, which\\nfeeds into an instance subnetwork. This subnetwork uses the initial\\ncategory-level segmentation, along with cues from the output of an object\\ndetector, within an end-to-end CRF to predict instances. This part of our model\\nis dynamically instantiated to produce a variable number of instances per\\nimage. Our end-to-end approach requires no post-processing and considers the\\nimage holistically, instead of processing independent proposals. Therefore,\\nunlike some related work, a pixel cannot belong to multiple instances.\\nFurthermore, far more precise segmentations are achieved, as shown by our\\nstate-of-the-art results (particularly at high IoU thresholds) on the Pascal\\nVOC and Cityscapes datasets.\\n', '  Mild Cognitive Impairment (MCI) is a mental disorder difficult to diagnose.\\nLinguistic features, mainly from parsers, have been used to detect MCI, but\\nthis is not suitable for large-scale assessments. MCI disfluencies produce\\nnon-grammatical speech that requires manual or high precision automatic\\ncorrection of transcripts. In this paper, we modeled transcripts into complex\\nnetworks and enriched them with word embedding (CNE) to better represent short\\ntexts produced in neuropsychological assessments. The network measurements were\\napplied with well-known classifiers to automatically identify MCI in\\ntranscripts, in a binary classification task. A comparison was made with the\\nperformance of traditional approaches using Bag of Words (BoW) and linguistic\\nfeatures for three datasets: DementiaBank in English, and Cinderella and\\nArizona-Battery in Portuguese. Overall, CNE provided higher accuracy than using\\nonly complex networks, while Support Vector Machine was superior to other\\nclassifiers. CNE provided the highest accuracies for DementiaBank and\\nCinderella, but BoW was more efficient for the Arizona-Battery dataset probably\\nowing to its short narratives. The approach using linguistic features yielded\\nhigher accuracy if the transcriptions of the Cinderella dataset were manually\\nrevised. Taken together, the results indicate that complex networks enriched\\nwith embedding is promising for detecting MCI in large-scale assessments\\n', '  Software developers frequently issue generic natural language queries for\\ncode search while using code search engines (e.g., GitHub native search,\\nKrugle). Such queries often do not lead to any relevant results due to\\nvocabulary mismatch problems. In this paper, we propose a novel technique that\\nautomatically identifies relevant and specific API classes from Stack Overflow\\nQ & A site for a programming task written as a natural language query, and then\\nreformulates the query for improved code search. We first collect candidate API\\nclasses from Stack Overflow using pseudo-relevance feedback and two term\\nweighting algorithms, and then rank the candidates using Borda count and\\nsemantic proximity between query keywords and the API classes. The semantic\\nproximity has been determined by an analysis of 1.3 million questions and\\nanswers of Stack Overflow. Experiments using 310 code search queries report\\nthat our technique suggests relevant API classes with 48% precision and 58%\\nrecall which are 32% and 48% higher respectively than those of the\\nstate-of-the-art. Comparisons with two state-of-the-art studies and three\\npopular search engines (e.g., Google, Stack Overflow, and GitHub native search)\\nreport that our reformulated queries (1) outperform the queries of the\\nstate-of-the-art, and (2) significantly improve the code search results\\nprovided by these contemporary search engines.\\n', '  Most state-of-the-art information extraction approaches rely on token-level\\nlabels to find the areas of interest in text. Unfortunately, these labels are\\ntime-consuming and costly to create, and consequently, not available for many\\nreal-life IE tasks. To make matters worse, token-level labels are usually not\\nthe desired output, but just an intermediary step. End-to-end (E2E) models,\\nwhich take raw text as input and produce the desired output directly, need not\\ndepend on token-level labels. We propose an E2E model based on pointer\\nnetworks, which can be trained directly on pairs of raw input and output text.\\nWe evaluate our model on the ATIS data set, MIT restaurant corpus and the MIT\\nmovie corpus and compare to neural baselines that do use token-level labels. We\\nachieve competitive results, within a few percentage points of the baselines,\\nshowing the feasibility of E2E information extraction without the need for\\ntoken-level labels. This opens up new possibilities, as for many tasks\\ncurrently addressed by human extractors, raw input and output data are\\navailable, but not token-level labels.\\n', \"  Hardware acceleration is an enabler for ubiquitous and efficient deep\\nlearning. With hardware accelerators being introduced in datacenter and edge\\ndevices, it is time to acknowledge that hardware specialization is central to\\nthe deep learning system stack.\\nThis technical report presents the Versatile Tensor Accelerator (VTA), an\\nopen, generic, and customizable deep learning accelerator design. VTA is a\\nprogrammable accelerator that exposes a RISC-like programming abstraction to\\ndescribe operations at the tensor level. We designed VTA to expose the most\\nsalient and common characteristics of mainstream deep learning accelerators,\\nsuch as tensor operations, DMA load/stores, and explicit compute/memory\\narbitration.\\nVTA is more than a standalone accelerator design: it's an end-to-end solution\\nthat includes drivers, a JIT runtime, and an optimizing compiler stack based on\\nTVM. The current release of VTA includes a behavioral hardware simulator, as\\nwell as the infrastructure to deploy VTA on low-cost FPGA development boards\\nfor fast prototyping.\\nBy extending the TVM stack with a customizable, and open source deep learning\\nhardware accelerator design, we are exposing a transparent end-to-end deep\\nlearning stack from the high-level deep learning framework, down to the actual\\nhardware design and implementation. This forms a truly end-to-end, from\\nsoftware-to-hardware open source stack for deep learning systems.\\n\", '  Human action recognition in videos is one of the most challenging tasks in\\ncomputer vision. One important issue is how to design discriminative features\\nfor representing spatial context and temporal dynamics. Here, we introduce a\\npath signature feature to encode information from intra-frame and inter-frame\\ncontexts. A key step towards leveraging this feature is to construct the proper\\ntrajectories (paths) for the data steam. In each frame, the correlated\\nconstraints of human joints are treated as small paths, then the spatial path\\nsignature features are extracted from them. In video data, the evolution of\\nthese spatial features over time can also be regarded as paths from which the\\ntemporal path signature features are extracted. Eventually, all these features\\nare concatenated to constitute the input vector of a fully connected neural\\nnetwork for action classification. Experimental results on four standard\\nbenchmark action datasets, J-HMDB, SBU Dataset, Berkeley MHAD, and NTURGB+D\\ndemonstrate that the proposed approach achieves state-of-the-art accuracy even\\nin comparison with recent deep learning based models.\\n', \"  Recent machine learning models have shown that including attention as a\\ncomponent results in improved model accuracy and interpretability, despite the\\nconcept of attention in these approaches only loosely approximating the brain's\\nattention mechanism. Here we extend this work by building a more brain-inspired\\ndeep network model of the primate ATTention Network (ATTNet) that learns to\\nshift its attention so as to maximize the reward. Using deep reinforcement\\nlearning, ATTNet learned to shift its attention to the visual features of a\\ntarget category in the context of a search task. ATTNet's dorsal layers also\\nlearned to prioritize these shifts of attention so as to maximize success of\\nthe ventral pathway classification and receive greater reward. Model behavior\\nwas tested against the fixations made by subjects searching images for the same\\ncued category. Both subjects and ATTNet showed evidence for attention being\\npreferentially directed to target goals, behaviorally measured as oculomotor\\nguidance to targets. More fundamentally, ATTNet learned to shift its attention\\nto target like objects and spatially route its visual inputs to accomplish the\\ntask. This work makes a step toward a better understanding of the role of\\nattention in the brain and other computational systems.\\n\", '  The behavior of many complex systems is determined by a core of densely\\ninterconnected units. While many methods are available to identify the core of\\na network when connections between nodes are all of the same type, a principled\\napproach to define the core when multiple types of connectivity are allowed is\\nstill lacking. Here we introduce a general framework to define and extract the\\ncore-periphery structure of multi-layer networks by explicitly taking into\\naccount the connectivity of the nodes at each layer. We show how our method\\nworks on synthetic networks with different size, density, and overlap between\\nthe cores at the different layers. We then apply the method to multiplex brain\\nnetworks whose layers encode information both on the anatomical and the\\nfunctional connectivity among regions of the human cortex. Results confirm the\\npresence of the main known hubs, but also suggest the existence of novel brain\\ncore regions that have been discarded by previous analysis which focused\\nexclusively on the structural layer. Our work is a step forward in the\\nidentification of the core of the human connectome, and contributes to shed\\nlight to a fundamental question in modern neuroscience.\\n', '  In online discussion communities, users can interact and share information\\nand opinions on a wide variety of topics. However, some users may create\\nmultiple identities, or sockpuppets, and engage in undesired behavior by\\ndeceiving others or manipulating discussions. In this work, we study\\nsockpuppetry across nine discussion communities, and show that sockpuppets\\ndiffer from ordinary users in terms of their posting behavior, linguistic\\ntraits, as well as social network structure. Sockpuppets tend to start fewer\\ndiscussions, write shorter posts, use more personal pronouns such as \"I\", and\\nhave more clustered ego-networks. Further, pairs of sockpuppets controlled by\\nthe same individual are more likely to interact on the same discussion at the\\nsame time than pairs of ordinary users. Our analysis suggests a taxonomy of\\ndeceptive behavior in discussion communities. Pairs of sockpuppets can vary in\\ntheir deceptiveness, i.e., whether they pretend to be different users, or their\\nsupportiveness, i.e., if they support arguments of other sockpuppets controlled\\nby the same user. We apply these findings to a series of prediction tasks,\\nnotably, to identify whether a pair of accounts belongs to the same underlying\\nuser or not. Altogether, this work presents a data-driven view of deception in\\nonline discussion communities and paves the way towards the automatic detection\\nof sockpuppets.\\n', '  Recent progress in deep learning for audio synthesis opens the way to models\\nthat directly produce the waveform, shifting away from the traditional paradigm\\nof relying on vocoders or MIDI synthesizers for speech or music generation.\\nDespite their successes, current state-of-the-art neural audio synthesizers\\nsuch as WaveNet and SampleRNN suffer from prohibitive training and inference\\ntimes because they are based on autoregressive models that generate audio\\nsamples one at a time at a rate of 16kHz. In this work, we study the more\\ncomputationally efficient alternative of generating the waveform frame-by-frame\\nwith large strides. We present SING, a lightweight neural audio synthesizer for\\nthe original task of generating musical notes given desired instrument, pitch\\nand velocity. Our model is trained end-to-end to generate notes from nearly\\n1000 instruments with a single decoder, thanks to a new loss function that\\nminimizes the distances between the log spectrograms of the generated and\\ntarget waveforms. On the generalization task of synthesizing notes for pairs of\\npitch and instrument not seen during training, SING produces audio with\\nsignificantly improved perceptual quality compared to a state-of-the-art\\nautoencoder based on WaveNet as measured by a Mean Opinion Score (MOS), and is\\nabout 32 times faster for training and 2, 500 times faster for inference.\\n', '  We present a novel end-to-end trainable neural network model for\\ntask-oriented dialog systems. The model is able to track dialog state, issue\\nAPI calls to knowledge base (KB), and incorporate structured KB query results\\ninto system responses to successfully complete task-oriented dialogs. The\\nproposed model produces well-structured system responses by jointly learning\\nbelief tracking and KB result processing conditioning on the dialog history. We\\nevaluate the model in a restaurant search domain using a dataset that is\\nconverted from the second Dialog State Tracking Challenge (DSTC2) corpus.\\nExperiment results show that the proposed model can robustly track dialog state\\ngiven the dialog history. Moreover, our model demonstrates promising results in\\nproducing appropriate system responses, outperforming prior end-to-end\\ntrainable neural network models using per-response accuracy evaluation metrics.\\n', '  In the last few years, we have seen the transformative impact of deep\\nlearning in many applications, particularly in speech recognition and computer\\nvision. Inspired by Google\\'s Inception-ResNet deep convolutional neural network\\n(CNN) for image classification, we have developed \"Chemception\", a deep CNN for\\nthe prediction of chemical properties, using just the images of 2D drawings of\\nmolecules. We develop Chemception without providing any additional explicit\\nchemistry knowledge, such as basic concepts like periodicity, or advanced\\nfeatures like molecular descriptors and fingerprints. We then show how\\nChemception can serve as a general-purpose neural network architecture for\\npredicting toxicity, activity, and solvation properties when trained on a\\nmodest database of 600 to 40,000 compounds. When compared to multi-layer\\nperceptron (MLP) deep neural networks trained with ECFP fingerprints,\\nChemception slightly outperforms in activity and solvation prediction and\\nslightly underperforms in toxicity prediction. Having matched the performance\\nof expert-developed QSAR/QSPR deep learning models, our work demonstrates the\\nplausibility of using deep neural networks to assist in computational chemistry\\nresearch, where the feature engineering process is performed primarily by a\\ndeep learning algorithm.\\n', \"  Vision science, particularly machine vision, has been revolutionized by\\nintroducing large-scale image datasets and statistical learning approaches.\\nYet, human neuroimaging studies of visual perception still rely on small\\nnumbers of images (around 100) due to time-constrained experimental procedures.\\nTo apply statistical learning approaches that integrate neuroscience, the\\nnumber of images used in neuroimaging must be significantly increased. We\\npresent BOLD5000, a human functional MRI (fMRI) study that includes almost\\n5,000 distinct images depicting real-world scenes. Beyond dramatically\\nincreasing image dataset size relative to prior fMRI studies, BOLD5000 also\\naccounts for image diversity, overlapping with standard computer vision\\ndatasets by incorporating images from the Scene UNderstanding (SUN), Common\\nObjects in Context (COCO), and ImageNet datasets. The scale and diversity of\\nthese image datasets, combined with a slow event-related fMRI design, enable\\nfine-grained exploration into the neural representation of a wide range of\\nvisual features, categories, and semantics. Concurrently, BOLD5000 brings us\\ncloser to realizing Marr's dream of a singular vision science - the intertwined\\nstudy of biological and computer vision.\\n\", '  We describe a fully data driven model that learns to perform a retrosynthetic\\nreaction prediction task, which is treated as a sequence-to-sequence mapping\\nproblem. The end-to-end trained model has an encoder-decoder architecture that\\nconsists of two recurrent neural networks, which has previously shown great\\nsuccess in solving other sequence-to-sequence prediction tasks such as machine\\ntranslation. The model is trained on 50,000 experimental reaction examples from\\nthe United States patent literature, which span 10 broad reaction types that\\nare commonly used by medicinal chemists. We find that our model performs\\ncomparably with a rule-based expert system baseline model, and also overcomes\\ncertain limitations associated with rule-based expert systems and with any\\nmachine learning approach that contains a rule-based expert system component.\\nOur model provides an important first step towards solving the challenging\\nproblem of computational retrosynthetic analysis.\\n', '  Recognition of Handwritten Mathematical Expressions (HMEs) is a challenging\\nproblem because of the ambiguity and complexity of two-dimensional handwriting.\\nMoreover, the lack of large training data is a serious issue, especially for\\nacademic recognition systems. In this paper, we propose pattern generation\\nstrategies that generate shape and structural variations to improve the\\nperformance of recognition systems based on a small training set. For data\\ngeneration, we employ the public databases: CROHME 2014 and 2016 of online\\nHMEs. The first strategy employs local and global distortions to generate shape\\nvariations. The second strategy decomposes an online HME into sub-online HMEs\\nto get more structural variations. The hybrid strategy combines both these\\nstrategies to maximize shape and structural variations. The generated online\\nHMEs are converted to images for offline HME recognition. We tested our\\nstrategies in an end-to-end recognition system constructed from a recent deep\\nlearning model: Convolutional Neural Network and attention-based\\nencoder-decoder. The results of experiments on the CROHME 2014 and 2016\\ndatabases demonstrate the superiority and effectiveness of our strategies: our\\nhybrid strategy achieved classification rates of 48.78% and 45.60%,\\nrespectively, on these databases. These results are competitive compared to\\nothers reported in recent literature. Our generated datasets are openly\\navailable for research community and constitute a useful resource for the HME\\nrecognition research in future.\\n', '  Large amount of image denoising literature focuses on single channel images\\nand often experimentally validates the proposed methods on tens of images at\\nmost. In this paper, we investigate the interaction between denoising and\\nclassification on large scale dataset. Inspired by classification models, we\\npropose a novel deep learning architecture for color (multichannel) image\\ndenoising and report on thousands of images from ImageNet dataset as well as\\ncommonly used imagery. We study the importance of (sufficient) training data,\\nhow semantic class information can be traded for improved denoising results. As\\na result, our method greatly improves PSNR performance by 0.34 - 0.51 dB on\\naverage over state-of-the art methods on large scale dataset. We conclude that\\nit is beneficial to incorporate in classification models. On the other hand, we\\nalso study how noise affect classification performance. In the end, we come to\\na number of interesting conclusions, some being counter-intuitive.\\n', '  In this paper, we propose a novel application of Generative Adversarial\\nNetworks (GAN) to the synthesis of cells imaged by fluorescence microscopy.\\nCompared to natural images, cells tend to have a simpler and more geometric\\nglobal structure that facilitates image generation. However, the correlation\\nbetween the spatial pattern of different fluorescent proteins reflects\\nimportant biological functions, and synthesized images have to capture these\\nrelationships to be relevant for biological applications. We adapt GANs to the\\ntask at hand and propose new models with casual dependencies between image\\nchannels that can generate multi-channel images, which would be impossible to\\nobtain experimentally. We evaluate our approach using two independent\\ntechniques and compare it against sensible baselines. Finally, we demonstrate\\nthat by interpolating across the latent space we can mimic the known changes in\\nprotein localization that occur through time during the cell cycle, allowing us\\nto predict temporal evolution from static images.\\n', '  Spectral mapping uses a deep neural network (DNN) to map directly from noisy\\nspeech to clean speech. Our previous study found that the performance of\\nspectral mapping improves greatly when using helpful cues from an acoustic\\nmodel trained on clean speech. The mapper network learns to mimic the input\\nfavored by the spectral classifier and cleans the features accordingly. In this\\nstudy, we explore two new innovations: we replace a DNN-based spectral mapper\\nwith a residual network that is more attuned to the goal of predicting clean\\nspeech. We also examine how integrating long term context in the mimic\\ncriterion (via wide-residual biLSTM networks) affects the performance of\\nspectral mapping compared to DNNs. Our goal is to derive a model that can be\\nused as a preprocessor for any recognition system; the features derived from\\nour model are passed through the standard Kaldi ASR pipeline and achieve a WER\\nof 9.3%, which is the lowest recorded word error rate for CHiME-2 dataset using\\nonly feature adaptation.\\n', \"  The methods to access large relational databases in a distributed system are\\nwell established: the relational query language SQL often serves as a language\\nfor data access and manipulation, and in addition public interfaces are exposed\\nusing communication protocols like REST. Similarly to REST, GraphQL is the\\nquery protocol of an application layer developed by Facebook. It provides a\\nunified interface between the client and the server for data fetching and\\nmanipulation. Using GraphQL's type system, it is possible to specify data\\nhandling of various sources and to combine, e.g., relational with NoSQL\\ndatabases. In contrast to REST, GraphQL provides a single API endpoint and\\nsupports flexible queries over linked data.\\nGraphQL can also be used as an interface for deductive databases. In this\\npaper, we give an introduction of GraphQL and a comparison to REST. Using\\nlanguage features recently added to SWI-Prolog 7, we have developed the Prolog\\nlibrary GraphQL.pl, which implements the GraphQL type system and query syntax\\nas a domain-specific language with the help of definite clause grammars (DCG),\\nquasi quotations, and dicts. Using our library, the type system created for a\\ndeductive database can be validated, while the query system provides a unified\\ninterface for data access and introspection.\\n\", '  Deep learning (DL) advances state-of-the-art reinforcement learning (RL), by\\nincorporating deep neural networks in learning representations from the input\\nto RL. However, the conventional deep neural network architecture is limited in\\nlearning representations for multi-task RL (MT-RL), as multiple tasks can refer\\nto different kinds of representations. In this paper, we thus propose a novel\\ndeep neural network architecture, namely generalization tower network (GTN),\\nwhich can achieve MT-RL within a single learned model. Specifically, the\\narchitecture of GTN is composed of both horizontal and vertical streams. In our\\nGTN architecture, horizontal streams are used to learn representation shared in\\nsimilar tasks. In contrast, the vertical streams are introduced to be more\\nsuitable for handling diverse tasks, which encodes hierarchical shared\\nknowledge of these tasks. The effectiveness of the introduced vertical stream\\nis validated by experimental results. Experimental results further verify that\\nour GTN architecture is able to advance the state-of-the-art MT-RL, via being\\ntested on 51 Atari games.\\n', '  Big data streaming applications require utilization of heterogeneous parallel\\ncomputing systems, which may comprise multiple multi-core CPUs and many-core\\naccelerating devices such as NVIDIA GPUs and Intel Xeon Phis. Programming such\\nsystems require advanced knowledge of several hardware architectures and\\ndevice-specific programming models, including OpenMP and CUDA. In this paper,\\nwe present HSTREAM, a compiler directive-based language extension to support\\nprogramming stream computing applications for heterogeneous parallel computing\\nsystems. HSTREAM source-to-source compiler aims to increase the programming\\nproductivity by enabling programmers to annotate the parallel regions for\\nheterogeneous execution and generate target specific code. The HSTREAM runtime\\nautomatically distributes the workload across CPUs and accelerating devices. We\\ndemonstrate the usefulness of HSTREAM language extension with various\\napplications from the STREAM benchmark. Experimental evaluation results show\\nthat HSTREAM can keep the same programming simplicity as OpenMP, and the\\ngenerated code can deliver performance beyond what CPUs-only and GPUs-only\\nexecutions can deliver.\\n', \"  Painting is an art form that has long functioned as a major channel for the\\ncreative expression and communication of humans, its evolution taking place\\nunder an interplay with the science, technology, and social environments of the\\ntimes. Therefore, understanding the process based on comprehensive data could\\nshed light on how humans acted and manifested creatively under changing\\nconditions. Yet, there exist few systematic frameworks that characterize the\\nprocess for painting, which would require robust statistical methods for\\ndefining painting characteristics and identifying human's creative\\ndevelopments, and data of high quality and sufficient quantity. Here we propose\\nthat the color contrast of a painting image signifying the heterogeneity in\\ninter-pixel chromatic distance can be a useful representation of its style,\\nintegrating both the color and geometry. From the color contrasts of paintings\\nfrom a large-scale, comprehensive archive of 179,853 high-quality images\\nspanning several centuries we characterize the temporal evolutionary patterns\\nof paintings, and present a deep study of an extraordinary expansion in\\ncreative diversity and individuality that came to define the modern era.\\n\", '  Developer preferences, language capabilities and the persistence of older\\nlanguages contribute to the trend that large software codebases are often\\nmultilingual, that is, written in more than one computer language. While\\ndevelopers can leverage monolingual software development tools to build\\nsoftware components, companies are faced with the problem of managing the\\nresultant large, multilingual codebases to address issues with security,\\nefficiency, and quality metrics. The key challenge is to address the opaque\\nnature of the language interoperability interface: one language calling\\nprocedures in a second (which may call a third, or even back to the first),\\nresulting in a potentially tangled, inefficient and insecure codebase. An\\narchitecture is proposed for lightweight static analysis of large multilingual\\ncodebases: the MLSA architecture. Its modular and table-oriented structure\\naddresses the open-ended nature of multiple languages and language\\ninteroperability APIs. We focus here as an application on the construction of\\ncall-graphs that capture both inter-language and intra-language calls. The\\nalgorithms for extracting multilingual call-graphs from codebases are\\npresented, and several examples of multilingual software engineering analysis\\nare discussed. The state of the implementation and testing of MLSA is\\npresented, and the implications for future work are discussed.\\n', '  During software maintenance, developers usually deal with a significant\\nnumber of software change requests. As a part of this, they often formulate an\\ninitial query from the request texts, and then attempt to map the concepts\\ndiscussed in the request to relevant source code locations in the software\\nsystem (a.k.a., concept location). Unfortunately, studies suggest that they\\noften perform poorly in choosing the right search terms for a change task. In\\nthis paper, we propose a novel technique --ACER-- that takes an initial query,\\nidentifies appropriate search terms from the source code using a novel term\\nweight --CodeRank, and then suggests effective reformulation to the initial\\nquery by exploiting the source document structures, query quality analysis and\\nmachine learning. Experiments with 1,675 baseline queries from eight subject\\nsystems report that our technique can improve 71% of the baseline queries which\\nis highly promising. Comparison with five closely related existing techniques\\nin query reformulation not only validates our empirical findings but also\\ndemonstrates the superiority of our technique.\\n', \"  Convolutional neural networks (CNNs) are the core of most state-of-the-art\\ndeep learning algorithms specialized for object detection and classification.\\nCNNs are both computationally complex and embarrassingly parallel. Two\\nproperties that leave room for potential software and hardware optimizations\\nfor embedded systems. Given a programmable hardware accelerator with a CNN\\noriented custom instructions set, the compiler's task is to exploit the\\nhardware's full potential, while abiding with the hardware constraints and\\nmaintaining generality to run different CNN models with varying workload\\nproperties. Snowflake is an efficient and scalable hardware accelerator\\nimplemented on programmable logic devices. It implements a control pipeline for\\na custom instruction set. The goal of this paper is to present Snowflake's\\ncompiler that generates machine level instructions from Torch7 model\\ndescription files. The main software design points explored in this work are:\\nmodel structure parsing, CNN workload breakdown, loop rearrangement for memory\\nbandwidth optimizations and memory access balancing. The performance achieved\\nby compiler generated instructions matches against hand optimized code for\\nconvolution layers. Generated instructions also efficiently execute AlexNet and\\nResNet18 inference on Snowflake. Snowflake with $256$ processing units was\\nsynthesized on Xilinx's Zynq XC7Z045 FPGA. At $250$ MHz, AlexNet achieved in\\n$93.6$ frames/s and $1.2$ GB/s of off-chip memory bandwidth, and $21.4$\\nframes/s and $2.2$ GB/s for ResNet18. Total on-chip power is $5$ W.\\n\", '  Autoencoders have been successful in learning meaningful representations from\\nimage datasets. However, their performance on text datasets has not been widely\\nstudied. Traditional autoencoders tend to learn possibly trivial\\nrepresentations of text documents due to their confounding properties such as\\nhigh-dimensionality, sparsity and power-law word distributions. In this paper,\\nwe propose a novel k-competitive autoencoder, called KATE, for text documents.\\nDue to the competition between the neurons in the hidden layer, each neuron\\nbecomes specialized in recognizing specific data patterns, and overall the\\nmodel can learn meaningful representations of textual data. A comprehensive set\\nof experiments show that KATE can learn better representations than traditional\\nautoencoders including denoising, contractive, variational, and k-sparse\\nautoencoders. Our model also outperforms deep generative models, probabilistic\\ntopic models, and even word representation models (e.g., Word2Vec) in terms of\\nseveral downstream tasks such as document classification, regression, and\\nretrieval.\\n', '  Convolutional neural networks (CNNs) are one of the driving forces for the\\nadvancement of computer vision. Despite their promising performances on many\\ntasks, CNNs still face major obstacles on the road to achieving ideal machine\\nintelligence. One is that CNNs are complex and hard to interpret. Another is\\nthat standard CNNs require large amounts of annotated data, which is sometimes\\nhard to obtain, and it is desirable to learn to recognize objects from few\\nexamples. In this work, we address these limitations of CNNs by developing\\nnovel, flexible, and interpretable models for few-shot learning. Our models are\\nbased on the idea of encoding objects in terms of visual concepts (VCs), which\\nare interpretable visual cues represented by the feature vectors within CNNs.\\nWe first adapt the learning of VCs to the few-shot setting, and then uncover\\ntwo key properties of feature encoding using VCs, which we call category\\nsensitivity and spatial pattern. Motivated by these properties, we present two\\nintuitive models for the problem of few-shot learning. Experiments show that\\nour models achieve competitive performances, while being more flexible and\\ninterpretable than alternative state-of-the-art few-shot learning methods. We\\nconclude that using VCs helps expose the natural capability of CNNs for\\nfew-shot learning.\\n', '  Statistical relational AI (StarAI) aims at reasoning and learning in noisy\\ndomains described in terms of objects and relationships by combining\\nprobability with first-order logic. With huge advances in deep learning in the\\ncurrent years, combining deep networks with first-order logic has been the\\nfocus of several recent studies. Many of the existing attempts, however, only\\nfocus on relations and ignore object properties. The attempts that do consider\\nobject properties are limited in terms of modelling power or scalability. In\\nthis paper, we develop relational neural networks (RelNNs) by adding hidden\\nlayers to relational logistic regression (the relational counterpart of\\nlogistic regression). We learn latent properties for objects both directly and\\nthrough general rules. Back-propagation is used for training these models. A\\nmodular, layer-wise architecture facilitates utilizing the techniques developed\\nwithin deep learning community to our architecture. Initial experiments on\\neight tasks over three real-world datasets show that RelNNs are promising\\nmodels for relational learning.\\n', '  Recent studies have shown that frame-level deep speaker features can be\\nderived from a deep neural network with the training target set to discriminate\\nspeakers by a short speech segment. By pooling the frame-level features,\\nutterance-level representations, called d-vectors, can be derived and used in\\nthe automatic speaker verification (ASV) task. This simple average pooling,\\nhowever, is inherently sensitive to the phonetic content of the utterance. An\\ninteresting idea borrowed from machine translation is the attention-based\\nmechanism, where the contribution of an input word to the translation at a\\nparticular time is weighted by an attention score. This score reflects the\\nrelevance of the input word and the present translation. We can use the same\\nidea to align utterances with different phonetic contents. This paper proposes\\na phonetic-attention scoring approach for d-vector systems. By this approach,\\nan attention score is computed for each frame pair. This score reflects the\\nsimilarity of the two frames in phonetic content, and is used to weigh the\\ncontribution of this frame pair in the utterance-based scoring. This new\\nscoring approach emphasizes the frame pairs with similar phonetic contents,\\nwhich essentially provides a soft alignment for utterances with any phonetic\\ncontents. Experimental results show that compared with the naive average\\npooling, this phonetic-attention scoring approach can deliver consistent\\nperformance improvement in ASV tasks of both text-dependent and\\ntext-independent.\\n', \"  Accurate diagnosis of Alzheimer's Disease (AD) entails clinical evaluation of\\nmultiple cognition metrics and biomarkers. Metrics such as the Alzheimer's\\nDisease Assessment Scale - Cognitive test (ADAS-cog) comprise multiple\\nsubscores that quantify different aspects of a patient's cognitive state such\\nas learning, memory, and language production/comprehension. Although\\ncomputer-aided diagnostic techniques for classification of a patient's current\\ndisease state exist, they provide little insight into the relationship between\\nchanges in brain structure and different aspects of a patient's cognitive state\\nthat occur over time in AD. We have developed a Convolutional Neural Network\\narchitecture that can concurrently predict the trajectories of the 13 subscores\\ncomprised by a subject's ADAS-cog examination results from a current minimally\\npreprocessed structural MRI scan up to 36 months from image acquisition time\\nwithout resorting to manual feature extraction. Mean performance metrics are\\nwithin range of those of existing techniques that require manual feature\\nselection and are limited to predicting aggregate scores.\\n\", \"  With its origin in sociology, Social Network Analysis (SNA), quickly emerged\\nand spread to other areas of research, including anthropology, biology,\\ninformation science, organizational studies, political science, and computer\\nscience. Being it's objective the investigation of social structures through\\nthe use of networks and graph theory, Social Network Analysis is, nowadays, an\\nimportant research area in several domains. Social Network Analysis cope with\\ndifferent problems namely network metrics, models, visualization and\\ninformation spreading, each one with several approaches, methods and\\nalgorithms. One of the critical areas of Social Network Analysis involves the\\ncalculation of different centrality measures (i.e.: the most important vertices\\nwithin a graph). Today, the challenge is how to do this fast and efficiently,\\nas many increasingly larger datasets are available. Recently, the need to apply\\nsuch centrality algorithms to non static networks (i.e.: networks that evolve\\nover time) is also a new challenge. Incremental and dynamic versions of\\ncentrality measures are starting to emerge (betweenness, closeness, etc). Our\\ncontribution is the proposal of two incremental versions of the Laplacian\\nCentrality measure, that can be applied not only to large graphs but also to,\\nweighted or unweighted, dynamically changing networks. The experimental\\nevaluation was performed with several tests in different types of evolving\\nnetworks, incremental or fully dynamic. Results have shown that our incremental\\nversions of the algorithm can calculate node centralities in large networks,\\nfaster and efficiently than the corresponding batch version in both incremental\\nand full dynamic network setups.\\n\", '  The splendid success of convolutional neural networks (CNNs) in computer\\nvision is largely attributed to the availability of large annotated datasets,\\nsuch as ImageNet and Places. However, in biomedical imaging, it is very\\nchallenging to create such large annotated datasets, as annotating biomedical\\nimages is not only tedious, laborious, and time consuming, but also demanding\\nof costly, specialty-oriented skills, which are not easily accessible. To\\ndramatically reduce annotation cost, this paper presents a novel method to\\nnaturally integrate active learning and transfer learning (fine-tuning) into a\\nsingle framework, called AFT*, which starts directly with a pre-trained CNN to\\nseek \"worthy\" samples for annotation and gradually enhance the (fine-tuned) CNN\\nvia continuous fine-tuning. We have evaluated our method in three distinct\\nbiomedical imaging applications, demonstrating that it can cut the annotation\\ncost by at least half, in comparison with the state-of-the-art method. This\\nperformance is attributed to the several advantages derived from the advanced\\nactive, continuous learning capability of our method. Although AFT* was\\ninitially conceived in the context of computer-aided diagnosis in biomedical\\nimaging, it is generic and applicable to many tasks in computer vision and\\nimage analysis; we illustrate the key ideas behind AFT* with the Places\\ndatabase for scene interpretation in natural images.\\n', '  Deep learning methods have recently achieved great empirical success on\\nmachine translation, dialogue response generation, summarization, and other\\ntext generation tasks. At a high level, the technique has been to train\\nend-to-end neural network models consisting of an encoder model to produce a\\nhidden representation of the source text, followed by a decoder model to\\ngenerate the target. While such models have significantly fewer pieces than\\nearlier systems, significant tuning is still required to achieve good\\nperformance. For text generation models in particular, the decoder can behave\\nin undesired ways, such as by generating truncated or repetitive outputs,\\noutputting bland and generic responses, or in some cases producing\\nungrammatical gibberish. This paper is intended as a practical guide for\\nresolving such undesired behavior in text generation models, with the aim of\\nhelping enable real-world applications.\\n', '  Point clouds provide a flexible and natural representation usable in\\ncountless applications such as robotics or self-driving cars. Recently, deep\\nneural networks operating on raw point cloud data have shown promising results\\non supervised learning tasks such as object classification and semantic\\nsegmentation. While massive point cloud datasets can be captured using modern\\nscanning technology, manually labelling such large 3D point clouds for\\nsupervised learning tasks is a cumbersome process. This necessitates effective\\nunsupervised learning methods that can produce representations such that\\ndownstream tasks require significantly fewer annotated samples. We propose a\\nnovel method for unsupervised learning on raw point cloud data in which a\\nneural network is trained to predict the spatial relationship between two point\\ncloud segments. While solving this task, representations that capture semantic\\nproperties of the point cloud are learned. Our method outperforms previous\\nunsupervised learning approaches in downstream object classification and\\nsegmentation tasks and performs on par with fully supervised methods.\\n', \"  Convolutional neural networks (CNNs) have become the dominant neural network\\narchitecture for solving many state-of-the-art (SOA) visual processing tasks.\\nEven though Graphical Processing Units (GPUs) are most often used in training\\nand deploying CNNs, their power efficiency is less than 10 GOp/s/W for\\nsingle-frame runtime inference. We propose a flexible and efficient CNN\\naccelerator architecture called NullHop that implements SOA CNNs useful for\\nlow-power and low-latency application scenarios. NullHop exploits the sparsity\\nof neuron activations in CNNs to accelerate the computation and reduce memory\\nrequirements. The flexible architecture allows high utilization of available\\ncomputing resources across kernel sizes ranging from 1x1 to 7x7. NullHop can\\nprocess up to 128 input and 128 output feature maps per layer in a single pass.\\nWe implemented the proposed architecture on a Xilinx Zynq FPGA platform and\\npresent results showing how our implementation reduces external memory\\ntransfers and compute time in five different CNNs ranging from small ones up to\\nthe widely known large VGG16 and VGG19 CNNs. Post-synthesis simulations using\\nMentor Modelsim in a 28nm process with a clock frequency of 500 MHz show that\\nthe VGG19 network achieves over 450 GOp/s. By exploiting sparsity, NullHop\\nachieves an efficiency of 368%, maintains over 98% utilization of the MAC\\nunits, and achieves a power efficiency of over 3TOp/s/W in a core area of\\n6.3mm$^2$. As further proof of NullHop's usability, we interfaced its FPGA\\nimplementation with a neuromorphic event camera for real time interactive\\ndemonstrations.\\n\", '  Recent research has revealed that the output of Deep Neural Networks (DNN)\\ncan be easily altered by adding relatively small perturbations to the input\\nvector. In this paper, we analyze an attack in an extremely limited scenario\\nwhere only one pixel can be modified. For that we propose a novel method for\\ngenerating one-pixel adversarial perturbations based on differential\\nevolution(DE). It requires less adversarial information(a black-box attack) and\\ncan fool more types of networks due to the inherent features of DE. The results\\nshow that 68.36% of the natural images in CIFAR-10 test dataset and 41.22% of\\nthe ImageNet (ILSVRC 2012) validation images can be perturbed to at least one\\ntarget class by modifying just one pixel with 73.22% and 5.52% confidence on\\naverage. Thus, the proposed attack explores a different take on adversarial\\nmachine learning in an extreme limited scenario, showing that current DNNs are\\nalso vulnerable to such low dimension attacks. Besides, we also illustrate an\\nimportant application of DE (or broadly speaking, evolutionary computation) in\\nthe domain of adversarial machine learning: creating tools that can effectively\\ngenerate low-cost adversarial attacks against neural networks for evaluating\\nrobustness. The code is available on:\\nthis https URL\\n', \"  In this paper, we generally formulate the dynamics prediction problem of\\nvarious network systems (e.g., the prediction of mobility, traffic and\\ntopology) as the temporal link prediction task. Different from conventional\\ntechniques of temporal link prediction that ignore the potential non-linear\\ncharacteristics and the informative link weights in the dynamic network, we\\nintroduce a novel non-linear model GCN-GAN to tackle the challenging temporal\\nlink prediction task of weighted dynamic networks. The proposed model leverages\\nthe benefits of the graph convolutional network (GCN), long short-term memory\\n(LSTM) as well as the generative adversarial network (GAN). Thus, the dynamics,\\ntopology structure and evolutionary patterns of weighted dynamic networks can\\nbe fully exploited to improve the temporal link prediction performance.\\nConcretely, we first utilize GCN to explore the local topological\\ncharacteristics of each single snapshot and then employ LSTM to characterize\\nthe evolving features of the dynamic networks. Moreover, GAN is used to enhance\\nthe ability of the model to generate the next weighted network snapshot, which\\ncan effectively tackle the sparsity and the wide-value-range problem of edge\\nweights in real-life dynamic networks. To verify the model's effectiveness, we\\nconduct extensive experiments on four datasets of different network systems and\\napplication scenarios. The experimental results demonstrate that our model\\nachieves impressive results compared to the state-of-the-art competitors.\\n\", '  Explaining underlying causes or effects about events is a challenging but\\nvaluable task. We define a novel problem of generating explanations of a time\\nseries event by (1) searching cause and effect relationships of the time series\\nwith textual data and (2) constructing a connecting chain between them to\\ngenerate an explanation. To detect causal features from text, we propose a\\nnovel method based on the Granger causality of time series between features\\nextracted from text such as N-grams, topics, sentiments, and their composition.\\nThe generation of the sequence of causal entities requires a commonsense\\ncausative knowledge base with efficient reasoning. To ensure good\\ninterpretability and appropriate lexical usage we combine symbolic and neural\\nrepresentations, using a neural reasoning algorithm trained on commonsense\\ncausal tuples to predict the next cause step. Our quantitative and human\\nanalysis show empirical evidence that our method successfully extracts\\nmeaningful causality relationships between time series with textual features\\nand generates appropriate explanation between them.\\n', '  Recurrent Neural Networks (RNNs) with attention mechanisms have obtained\\nstate-of-the-art results for many sequence processing tasks. Most of these\\nmodels use a simple form of encoder with attention that looks over the entire\\nsequence and assigns a weight to each token independently. We present a\\nmechanism for focusing RNN encoders for sequence modelling tasks which allows\\nthem to attend to key parts of the input as needed. We formulate this using a\\nmulti-layer conditional sequence encoder that reads in one token at a time and\\nmakes a discrete decision on whether the token is relevant to the context or\\nquestion being asked. The discrete gating mechanism takes in the context\\nembedding and the current hidden state as inputs and controls information flow\\ninto the layer above. We train it using policy gradient methods. We evaluate\\nthis method on several types of tasks with different attributes. First, we\\nevaluate the method on synthetic tasks which allow us to evaluate the model for\\nits generalization ability and probe the behavior of the gates in more\\ncontrolled settings. We then evaluate this approach on large scale Question\\nAnswering tasks including the challenging MS MARCO and SearchQA tasks. Our\\nmodels shows consistent improvements for both tasks over prior work and our\\nbaselines. It has also shown to generalize significantly better on synthetic\\ntasks as compared to the baselines.\\n', '  Anatomical and biophysical modeling of left atrium (LA) and proximal\\npulmonary veins (PPVs) is important for clinical management of several cardiac\\ndiseases. Magnetic resonance imaging (MRI) allows qualitative assessment of LA\\nand PPVs through visualization. However, there is a strong need for an advanced\\nimage segmentation method to be applied to cardiac MRI for quantitative\\nanalysis of LA and PPVs. In this study, we address this unmet clinical need by\\nexploring a new deep learning-based segmentation strategy for quantification of\\nLA and PPVs with high accuracy and heightened efficiency. Our approach is based\\non a multi-view convolutional neural network (CNN) with an adaptive fusion\\nstrategy and a new loss function that allows fast and more accurate convergence\\nof the backpropagation based optimization. After training our network from\\nscratch by using more than 60K 2D MRI images (slices), we have evaluated our\\nsegmentation strategy to the STACOM 2013 cardiac segmentation challenge\\nbenchmark. Qualitative and quantitative evaluations, obtained from the\\nsegmentation challenge, indicate that the proposed method achieved the\\nstate-of-the-art sensitivity (90%), specificity (99%), precision (94%), and\\nefficiency levels (10 seconds in GPU, and 7.5 minutes in CPU).\\n', '  Cognitive computing systems require human labeled data for evaluation, and\\noften for training. The standard practice used in gathering this data minimizes\\ndisagreement between annotators, and we have found this results in data that\\nfails to account for the ambiguity inherent in language. We have proposed the\\nCrowdTruth method for collecting ground truth through crowdsourcing, that\\nreconsiders the role of people in machine learning based on the observation\\nthat disagreement between annotators provides a useful signal for phenomena\\nsuch as ambiguity in the text. We report on using this method to build an\\nannotated data set for medical relation extraction for the $cause$ and $treat$\\nrelations, and how this data performed in a supervised training experiment. We\\ndemonstrate that by modeling ambiguity, labeled data gathered from crowd\\nworkers can (1) reach the level of quality of domain experts for this task\\nwhile reducing the cost, and (2) provide better training data at scale than\\ndistant supervision. We further propose and validate new weighted measures for\\nprecision, recall, and F-measure, that account for ambiguity in both human and\\nmachine performance on this task.\\n', \"  Network embedding aims to find a way to encode network by learning an\\nembedding vector for each node in the network. The network often has property\\ninformation which is highly informative with respect to the node's position and\\nrole in the network. Most network embedding methods fail to utilize this\\ninformation during network representation learning. In this paper, we propose a\\nnovel framework, FANE, to integrate structure and property information in the\\nnetwork embedding process. In FANE, we design a network to unify heterogeneity\\nof the two information sources, and define a new random walking strategy to\\nleverage property information and make the two information compensate. FANE is\\nconceptually simple and empirically powerful. It improves over the\\nstate-of-the-art methods on Cora dataset classification task by over 5%, more\\nthan 10% on WebKB dataset classification task. Experiments also show that the\\nresults improve more than the state-of-the-art methods as increasing training\\nsize. Moreover, qualitative visualization show that our framework is helpful in\\nnetwork property information exploration. In all, we present a new way for\\nefficiently learning state-of-the-art task-independent representations in\\ncomplex attributed networks. The source code and datasets of this paper can be\\nobtained from this https URL.\\n\", '  Agent-Based Computing is a diverse research domain concerned with the\\nbuilding of intelligent software based on the concept of \"agents\". In this\\npaper, we use Scientometric analysis to analyze all sub-domains of agent-based\\ncomputing. Our data consists of 1,064 journal articles indexed in the ISI web\\nof knowledge published during a twenty year period: 1990-2010. These were\\nretrieved using a topic search with various keywords commonly used in\\nsub-domains of agent-based computing. In our proposed approach, we have\\nemployed a combination of two applications for analysis, namely Network\\nWorkbench and CiteSpace - wherein Network Workbench allowed for the analysis of\\ncomplex network aspects of the domain, detailed visualization-based analysis of\\nthe bibliographic data was performed using CiteSpace. Our results include the\\nidentification of the largest cluster based on keywords, the timeline of\\npublication of index terms, the core journals and key subject categories. We\\nalso identify the core authors, top countries of origin of the manuscripts\\nalong with core research institutes. Finally, our results have interestingly\\nrevealed the strong presence of agent-based computing in a number of\\nnon-computing related scientific domains including Life Sciences, Ecological\\nSciences and Social Sciences.\\n', '  Developing a dialogue agent that is capable of making autonomous decisions\\nand communicating by natural language is one of the long-term goals of machine\\nlearning research. Traditional approaches either rely on hand-crafting a small\\nstate-action set for applying reinforcement learning that is not scalable or\\nconstructing deterministic models for learning dialogue sentences that fail to\\ncapture natural conversational variability. In this paper, we propose a Latent\\nIntention Dialogue Model (LIDM) that employs a discrete latent variable to\\nlearn underlying dialogue intentions in the framework of neural variational\\ninference. In a goal-oriented dialogue scenario, these latent intentions can be\\ninterpreted as actions guiding the generation of machine responses, which can\\nbe further refined autonomously by reinforcement learning. The experimental\\nevaluation of LIDM shows that the model out-performs published benchmarks for\\nboth corpus-based and human evaluation, demonstrating the effectiveness of\\ndiscrete latent variable models for learning goal-oriented dialogues.\\n', '  The step of expert taxa recognition currently slows down the response time of\\nmany bioassessments. Shifting to quicker and cheaper state-of-the-art machine\\nlearning approaches is still met with expert scepticism towards the ability and\\nlogic of machines. In our study, we investigate both the differences in\\naccuracy and in the identification logic of taxonomic experts and machines. We\\npropose a systematic approach utilizing deep Convolutional Neural Nets with the\\ntransfer learning paradigm and extensively evaluate it over a multi-label and\\nmulti-pose taxonomic dataset specifically created for this comparison. We also\\nstudy the prediction accuracy on different ranks of taxonomic hierarchy in\\ndetail. Our results revealed that human experts using actual specimens yield\\nthe lowest classification error. However, our proposed, much faster, automated\\napproach using deep Convolutional Neural Nets comes very close to human\\naccuracy. Contrary to previous findings in the literature, we find that\\nmachines following a typical flat classification approach commonly used in\\nmachine learning performs better than forcing machines to adopt a hierarchical,\\nlocal per parent node approach used by human taxonomic experts. Finally, we\\npublicly share our unique dataset to serve as a public benchmark dataset in\\nthis field.\\n', '  Biomedical events describe complex interactions between various biomedical\\nentities. Event trigger is a word or a phrase which typically signifies the\\noccurrence of an event. Event trigger identification is an important first step\\nin all event extraction methods. However many of the current approaches either\\nrely on complex hand-crafted features or consider features only within a\\nwindow. In this paper we propose a method that takes the advantage of recurrent\\nneural network (RNN) to extract higher level features present across the\\nsentence. Thus hidden state representation of RNN along with word and entity\\ntype embedding as features avoid relying on the complex hand-crafted features\\ngenerated using various NLP toolkits. Our experiments have shown to achieve\\nstate-of-art F1-score on Multi Level Event Extraction (MLEE) corpus. We have\\nalso performed category-wise analysis of the result and discussed the\\nimportance of various features in trigger identification task.\\n', '  Deep learning models require extensive architecture design exploration and\\nhyperparameter optimization to perform well on a given task. The exploration of\\nthe model design space is often made by a human expert, and optimized using a\\ncombination of grid search and search heuristics over a large space of possible\\nchoices. Neural Architecture Search (NAS) is a Reinforcement Learning approach\\nthat has been proposed to automate architecture design. NAS has been\\nsuccessfully applied to generate Neural Networks that rival the best\\nhuman-designed architectures. However, NAS requires sampling, constructing, and\\ntraining hundreds to thousands of models to achieve well-performing\\narchitectures. This procedure needs to be executed from scratch for each new\\ntask. The application of NAS to a wide set of tasks currently lacks a way to\\ntransfer generalizable knowledge across tasks. In this paper, we present the\\nMultitask Neural Model Search (MNMS) controller. Our goal is to learn a\\ngeneralizable framework that can condition model construction on successful\\nmodel searches for previously seen tasks, thus significantly speeding up the\\nsearch for new tasks. We demonstrate that MNMS can conduct an automated\\narchitecture search for multiple tasks simultaneously while still learning\\nwell-performing, specialized models for each task. We then show that\\npre-trained MNMS controllers can transfer learning to new tasks. By leveraging\\nknowledge from previous searches, we find that pre-trained MNMS models start\\nfrom a better location in the search space and reduce search time on unseen\\ntasks, while still discovering models that outperform published human-designed\\nmodels.\\n', '  Electron Cryo-Tomography (ECT) enables 3D visualization of macromolecule\\nstructure inside single cells. Macromolecule classification approaches based on\\nconvolutional neural networks (CNN) were developed to separate millions of\\nmacromolecules captured from ECT systematically. However, given the fast\\naccumulation of ECT data, it will soon become necessary to use CNN models to\\nefficiently and accurately separate substantially more macromolecules at the\\nprediction stage, which requires additional computational costs. To speed up\\nthe prediction, we compress classification models into compact neural networks\\nwith little in accuracy for deployment. Specifically, we propose to perform\\nmodel compression through knowledge distillation. Firstly, a complex teacher\\nnetwork is trained to generate soft labels with better classification\\nfeasibility followed by training of customized student networks with simple\\narchitectures using the soft label to compress model complexity. Our tests\\ndemonstrate that our compressed models significantly reduce the number of\\nparameters and time cost while maintaining similar classification accuracy.\\n', '  The current dominant visual processing paradigm in both human and machine\\nresearch is the feedforward, layered hierarchy of neural-like processing\\nelements. Within this paradigm, visual saliency is seen by many to have a\\nspecific role, namely that of early selection. Early selection is thought to\\nenable very fast visual performance by limiting processing to only the most\\nrelevant candidate portions of an image. Though this strategy has indeed led to\\nimproved processing time efficiency in machine algorithms, at least one set of\\ncritical tests of this idea has never been performed with respect to the role\\nof early selection in human vision. How would the best of the current saliency\\nmodels perform on the stimuli used by experimentalists who first provided\\nevidence for this visual processing paradigm? Would the algorithms really\\nprovide correct candidate sub-images to enable fast categorization on those\\nsame images? Here, we report on a new series of tests of these questions whose\\nresults suggest that it is quite unlikely that such an early selection process\\nhas any role in human rapid visual categorization.\\n', \"  Deep learning models for graphs have achieved strong performance for the task\\nof node classification. Despite their proliferation, currently there is no\\nstudy of their robustness to adversarial attacks. Yet, in domains where they\\nare likely to be used, e.g. the web, adversaries are common. Can deep learning\\nmodels for graphs be easily fooled? In this work, we introduce the first study\\nof adversarial attacks on attributed graphs, specifically focusing on models\\nexploiting ideas of graph convolutions. In addition to attacks at test time, we\\ntackle the more challenging class of poisoning/causative attacks, which focus\\non the training phase of a machine learning model. We generate adversarial\\nperturbations targeting the node's features and the graph structure, thus,\\ntaking the dependencies between instances in account. Moreover, we ensure that\\nthe perturbations remain unnoticeable by preserving important data\\ncharacteristics. To cope with the underlying discrete domain we propose an\\nefficient algorithm Nettack exploiting incremental computations. Our\\nexperimental study shows that accuracy of node classification significantly\\ndrops even when performing only few perturbations. Even more, our attacks are\\ntransferable: the learned attacks generalize to other state-of-the-art node\\nclassification models and unsupervised approaches, and likewise are successful\\neven when only limited knowledge about the graph is given.\\n\", '  The understanding of variations in genome sequences assists us in identifying\\npeople who are predisposed to common diseases, solving rare diseases, and\\nfinding the corresponding population group of the individuals from a larger\\npopulation group. Although classical machine learning techniques allow\\nresearchers to identify groups (i.e. clusters) of related variables, the\\naccuracy, and effectiveness of these methods diminish for large and\\nhigh-dimensional datasets such as the whole human genome. On the other hand,\\ndeep neural network architectures (the core of deep learning) can better\\nexploit large-scale datasets to build complex models. In this paper, we use the\\nK-means clustering approach for scalable genomic data analysis aiming towards\\nclustering genotypic variants at the population scale. Finally, we train a deep\\nbelief network (DBN) for predicting the geographic ethnicity. We used the\\ngenotype data from the 1000 Genomes Project, which covers the result of genome\\nsequencing for 2504 individuals from 26 different ethnic origins and comprises\\n84 million variants. Our experimental results, with a focus on accuracy and\\nscalability, show the effectiveness and superiority compared to the\\nstate-of-the-art.\\n', \"  Knowledge distillation (KD) consists of transferring knowledge from one\\nmachine learning model (the teacher}) to another (the student). Commonly, the\\nteacher is a high-capacity model with formidable performance, while the student\\nis more compact. By transferring knowledge, one hopes to benefit from the\\nstudent's compactness. %we desire a compact model with performance close to the\\nteacher's. We study KD from a new perspective: rather than compressing models,\\nwe train students parameterized identically to their teachers. Surprisingly,\\nthese {Born-Again Networks (BANs), outperform their teachers significantly,\\nboth on computer vision and language modeling tasks. Our experiments with BANs\\nbased on DenseNets demonstrate state-of-the-art performance on the CIFAR-10\\n(3.5%) and CIFAR-100 (15.5%) datasets, by validation error. Additional\\nexperiments explore two distillation objectives: (i) Confidence-Weighted by\\nTeacher Max (CWTM) and (ii) Dark Knowledge with Permuted Predictions (DKPP).\\nBoth methods elucidate the essential components of KD, demonstrating a role of\\nthe teacher outputs on both predicted and non-predicted classes. We present\\nexperiments with students of various capacities, focusing on the under-explored\\ncase where students overpower teachers. Our experiments show significant\\nadvantages from transferring knowledge between DenseNets and ResNets in either\\ndirection.\\n\", '  Interactive reinforcement learning (IRL) extends traditional reinforcement\\nlearning (RL) by allowing an agent to interact with parent-like trainers during\\na task. In this paper, we present an IRL approach using dynamic audio-visual\\ninput in terms of vocal commands and hand gestures as feedback. Our\\narchitecture integrates multi-modal information to provide robust commands from\\nmultiple sensory cues along with a confidence value indicating the\\ntrustworthiness of the feedback. The integration process also considers the\\ncase in which the two modalities convey incongruent information. Additionally,\\nwe modulate the influence of sensory-driven feedback in the IRL task using\\ngoal-oriented knowledge in terms of contextual affordances. We implement a\\nneural network architecture to predict the effect of performed actions with\\ndifferent objects to avoid failed-states, i.e., states from which it is not\\npossible to accomplish the task. In our experimental setup, we explore the\\ninterplay of multimodal feedback and task-specific affordances in a robot\\ncleaning scenario. We compare the learning performance of the agent under four\\ndifferent conditions: traditional RL, multi-modal IRL, and each of these two\\nsetups with the use of contextual affordances. Our experiments show that the\\nbest performance is obtained by using audio-visual feedback with\\naffordancemodulated IRL. The obtained results demonstrate the importance of\\nmulti-modal sensory processing integrated with goal-oriented knowledge in IRL\\ntasks.\\n', '  The roles played by learning and memorization represent an important topic in\\ndeep learning research. Recent work on this subject has shown that the\\noptimization behavior of DNNs trained on shuffled labels is qualitatively\\ndifferent from DNNs trained with real labels. Here, we propose a novel\\npermutation approach that can differentiate memorization from learning in deep\\nneural networks (DNNs) trained as usual (i.e., using the real labels to guide\\nthe learning, rather than shuffled labels). The evaluation of weather the DNN\\nhas learned and/or memorized, happens in a separate step where we compare the\\npredictive performance of a shallow classifier trained with the features\\nlearned by the DNN, against multiple instances of the same classifier, trained\\non the same input, but using shuffled labels as outputs. By evaluating these\\nshallow classifiers in validation sets that share structure with the training\\nset, we are able to tell apart learning from memorization. Application of our\\npermutation approach to multi-layer perceptrons and convolutional neural\\nnetworks trained on image data corroborated many findings from other groups.\\nMost importantly, our illustrations also uncovered interesting dynamic patterns\\nabout how DNNs memorize over increasing numbers of training epochs, and support\\nthe surprising result that DNNs are still able to learn, rather than only\\nmemorize, when trained with pure Gaussian noise as input.\\n', '  Neural models have become ubiquitous in automatic speech recognition systems.\\nWhile neural networks are typically used as acoustic models in more complex\\nsystems, recent studies have explored end-to-end speech recognition systems\\nbased on neural networks, which can be trained to directly predict text from\\ninput acoustic features. Although such systems are conceptually elegant and\\nsimpler than traditional systems, it is less obvious how to interpret the\\ntrained models. In this work, we analyze the speech representations learned by\\na deep end-to-end model that is based on convolutional and recurrent layers,\\nand trained with a connectionist temporal classification (CTC) loss. We use a\\npre-trained model to generate frame-level features which are given to a\\nclassifier that is trained on frame classification into phones. We evaluate\\nrepresentations from different layers of the deep model and compare their\\nquality for predicting phone labels. Our experiments shed light on important\\naspects of the end-to-end model such as layer depth, model complexity, and\\nother design choices.\\n', '  As training data rapid growth, large-scale parallel training with multi-GPUs\\ncluster is widely applied in the neural network model learning currently.We\\npresent a new approach that applies exponential moving average method in\\nlarge-scale parallel training of neural network model. It is a non-interference\\nstrategy that the exponential moving average model is not broadcasted to\\ndistributed workers to update their local models after model synchronization in\\nthe training process, and it is implemented as the final model of the training\\nsystem. Fully-connected feed-forward neural networks (DNNs) and deep\\nunidirectional Long short-term memory (LSTM) recurrent neural networks (RNNs)\\nare successfully trained with proposed method for large vocabulary continuous\\nspeech recognition on Shenma voice search data in Mandarin. The character error\\nrate (CER) of Mandarin speech recognition further degrades than\\nstate-of-the-art approaches of parallel training.\\n', '  We propose a novel couple mappings method for low resolution face recognition\\nusing deep convolutional neural networks (DCNNs). The proposed architecture\\nconsists of two branches of DCNNs to map the high and low resolution face\\nimages into a common space with nonlinear transformations. The branch\\ncorresponding to transformation of high resolution images consists of 14 layers\\nand the other branch which maps the low resolution face images to the common\\nspace includes a 5-layer super-resolution network connected to a 14-layer\\nnetwork. The distance between the features of corresponding high and low\\nresolution images are backpropagated to train the networks. Our proposed method\\nis evaluated on FERET data set and compared with state-of-the-art competing\\nmethods. Our extensive experimental results show that the proposed method\\nsignificantly improves the recognition performance especially for very low\\nresolution probe face images (11.4% improvement in recognition accuracy).\\nFurthermore, it can reconstruct a high resolution image from its corresponding\\nlow resolution probe image which is comparable with state-of-the-art\\nsuper-resolution methods in terms of visual quality.\\n', '  Recurrent neural nets (RNN) and convolutional neural nets (CNN) are widely\\nused on NLP tasks to capture the long-term and local dependencies,\\nrespectively. Attention mechanisms have recently attracted enormous interest\\ndue to their highly parallelizable computation, significantly less training\\ntime, and flexibility in modeling dependencies. We propose a novel attention\\nmechanism in which the attention between elements from input sequence(s) is\\ndirectional and multi-dimensional (i.e., feature-wise). A light-weight neural\\nnet, \"Directional Self-Attention Network (DiSAN)\", is then proposed to learn\\nsentence embedding, based solely on the proposed attention without any RNN/CNN\\nstructure. DiSAN is only composed of a directional self-attention with temporal\\norder encoded, followed by a multi-dimensional attention that compresses the\\nsequence into a vector representation. Despite its simple form, DiSAN\\noutperforms complicated RNN models on both prediction quality and time\\nefficiency. It achieves the best test accuracy among all sentence encoding\\nmethods and improves the most recent best result by 1.02% on the Stanford\\nNatural Language Inference (SNLI) dataset, and shows state-of-the-art test\\naccuracy on the Stanford Sentiment Treebank (SST), Multi-Genre natural language\\ninference (MultiNLI), Sentences Involving Compositional Knowledge (SICK),\\nCustomer Review, MPQA, TREC question-type classification and Subjectivity\\n(SUBJ) datasets.\\n', '  Advances in deep learning for natural images have prompted a surge of\\ninterest in applying similar techniques to medical images. The majority of the\\ninitial attempts focused on replacing the input of a deep convolutional neural\\nnetwork with a medical image, which does not take into consideration the\\nfundamental differences between these two types of images. Specifically, fine\\ndetails are necessary for detection in medical images, unlike in natural images\\nwhere coarse structures matter most. This difference makes it inadequate to use\\nthe existing network architectures developed for natural images, because they\\nwork on heavily downscaled images to reduce the memory requirements. This hides\\ndetails necessary to make accurate predictions. Additionally, a single exam in\\nmedical imaging often comes with a set of views which must be fused in order to\\nreach a correct conclusion. In our work, we propose to use a multi-view deep\\nconvolutional neural network that handles a set of high-resolution medical\\nimages. We evaluate it on large-scale mammography-based breast cancer screening\\n(BI-RADS prediction) using 886,000 images. We focus on investigating the impact\\nof the training set size and image size on the prediction accuracy. Our results\\nhighlight that performance increases with the size of training set, and that\\nthe best performance can only be achieved using the original resolution. In the\\nreader study, performed on a random subset of the test set, we confirmed the\\nefficacy of our model, which achieved performance comparable to a committee of\\nradiologists when presented with the same data.\\n', '  In this work, we investigate the value of employing deep learning for the\\ntask of wireless signal modulation recognition. Recently in [1], a framework\\nhas been introduced by generating a dataset using GNU radio that mimics the\\nimperfections in a real wireless channel, and uses 10 different modulation\\ntypes. Further, a convolutional neural network (CNN) architecture was developed\\nand shown to deliver performance that exceeds that of expert-based approaches.\\nHere, we follow the framework of [1] and find deep neural network architectures\\nthat deliver higher accuracy than the state of the art. We tested the\\narchitecture of [1] and found it to achieve an accuracy of approximately 75% of\\ncorrectly recognizing the modulation type. We first tune the CNN architecture\\nof [1] and find a design with four convolutional layers and two dense layers\\nthat gives an accuracy of approximately 83.8% at high SNR. We then develop\\narchitectures based on the recently introduced ideas of Residual Networks\\n(ResNet [2]) and Densely Connected Networks (DenseNet [3]) to achieve high SNR\\naccuracies of approximately 83.5% and 86.6%, respectively. Finally, we\\nintroduce a Convolutional Long Short-term Deep Neural Network (CLDNN [4]) to\\nachieve an accuracy of approximately 88.5% at high SNR.\\n', '  Spiking neural networks (SNNs) possess energy-efficient potential due to\\nevent-based computation. However, supervised training of SNNs remains a\\nchallenge as spike activities are non-differentiable. Previous SNNs training\\nmethods can basically be categorized into two classes, backpropagation-like\\ntraining methods and plasticity-based learning methods. The former methods are\\ndependent on energy-inefficient real-valued computation and non-local\\ntransmission, as also required in artificial neural networks (ANNs), while the\\nlatter either be considered biologically implausible or exhibit poor\\nperformance. Hence, biologically plausible (bio-plausible) high-performance\\nsupervised learning (SL) methods for SNNs remain deficient. In this paper, we\\nproposed a novel bio-plausible SNN model for SL based on the symmetric\\nspike-timing dependent plasticity (sym-STDP) rule found in neuroscience. By\\ncombining the sym-STDP rule with bio-plausible synaptic scaling and intrinsic\\nplasticity of the dynamic threshold, our SNN model implemented SL well and\\nachieved good performance in the benchmark recognition task (MNIST). To reveal\\nthe underlying mechanism of our SL model, we visualized both layer-based\\nactivities and synaptic weights using the t-distributed stochastic neighbor\\nembedding (t-SNE) method after training and found that they were well\\nclustered, thereby demonstrating excellent classification ability. As the\\nlearning rules were bio-plausible and based purely on local spike events, our\\nmodel could be easily applied to neuromorphic hardware for online training and\\nmay be helpful for understanding SL information processing at the synaptic\\nlevel in biological neural systems.\\n', \"  This paper proposes a principled information theoretic analysis of\\nclassification for deep neural network structures, e.g. convolutional neural\\nnetworks (CNN). The output of convolutional filters is modeled as a random\\nvariable Y conditioned on the object class C and network filter bank F. The\\nconditional entropy (CENT) H(Y |C,F) is shown in theory and experiments to be a\\nhighly compact and class-informative code, that can be computed from the filter\\noutputs throughout an existing CNN and used to obtain higher classification\\nresults than the original CNN itself. Experiments demonstrate the effectiveness\\nof CENT feature analysis in two separate CNN classification contexts. 1) In the\\nclassification of neurodegeneration due to Alzheimer's disease (AD) and natural\\naging from 3D magnetic resonance image (MRI) volumes, 3 CENT features result in\\nan AUC=94.6% for whole-brain AD classification, the highest reported accuracy\\non the public OASIS dataset used and 12% higher than the softmax output of the\\noriginal CNN trained for the task. 2) In the context of visual object\\nclassification from 2D photographs, transfer learning based on a small set of\\nCENT features identified throughout an existing CNN leads to AUC values\\ncomparable to the 1000-feature softmax output of the original network when\\nclassifying previously unseen object categories. The general information\\ntheoretical analysis explains various recent CNN design successes, e.g. densely\\nconnected CNN architectures, and provides insights for future research\\ndirections in deep learning.\\n\", '  Applications which use human speech as an input require a speech interface\\nwith high recognition accuracy. The words or phrases in the recognised text are\\nannotated with a machine-understandable meaning and linked to knowledge graphs\\nfor further processing by the target application. These semantic annotations of\\nrecognised words can be represented as a subject-predicate-object triples which\\ncollectively form a graph often referred to as a knowledge graph. This type of\\nknowledge representation facilitates to use speech interfaces with any spoken\\ninput application, since the information is represented in logical, semantic\\nform, retrieving and storing can be followed using any web standard query\\nlanguages. In this work, we develop a methodology for linking speech input to\\nknowledge graphs and study the impact of recognition errors in the overall\\nprocess. We show that for a corpus with lower WER, the annotation and linking\\nof entities to the DBpedia knowledge graph is considerable. DBpedia Spotlight,\\na tool to interlink text documents with the linked open data is used to link\\nthe speech recognition output to the DBpedia knowledge graph. Such a\\nknowledge-based speech recognition interface is useful for applications such as\\nquestion answering or spoken dialog systems.\\n', \"  At the core of many important machine learning problems faced by online\\nstreaming services is a need to model how users interact with the content.\\nThese problems can often be reduced to a combination of 1) sequentially\\nrecommending items to the user, and 2) exploiting the user's interactions with\\nthe items as feedback for the machine learning model. Unfortunately, there are\\nno public datasets currently available that enable researchers to explore this\\ntopic. In order to spur that research, we release the Music Streaming Sessions\\nDataset (MSSD), which consists of approximately 150 million listening sessions\\nand associated user actions. Furthermore, we provide audio features and\\nmetadata for the approximately 3.7 million unique tracks referred to in the\\nlogs. This is the largest collection of such track metadata currently available\\nto the public. This dataset enables research on important problems including\\nhow to model user listening and interaction behaviour in streaming, as well as\\nMusic Information Retrieval (MIR), and session-based sequential\\nrecommendations.\\n\", '  Process discovery techniques return process models that are either formal\\n(precisely describing the possible behaviors) or informal (merely a \"picture\"\\nnot allowing for any form of formal reasoning). Formal models are able to\\nclassify traces (i.e., sequences of events) as fitting or non-fitting. Most\\nprocess mining approaches described in the literature produce such models. This\\nis in stark contrast with the over 25 available commercial process mining tools\\nthat only discover informal process models that remain deliberately vague on\\nthe precise set of possible traces. There are two main reasons why vendors\\nresort to such models: scalability and simplicity. In this paper, we propose to\\ncombine the best of both worlds: discovering hybrid process models that have\\nformal and informal elements. As a proof of concept we present a discovery\\ntechnique based on hybrid Petri nets. These models allow for formal reasoning,\\nbut also reveal information that cannot be captured in mainstream formal\\nmodels. A novel discovery algorithm returning hybrid Petri nets has been\\nimplemented in ProM and has been applied to several real-life event logs. The\\nresults clearly demonstrate the advantages of remaining \"vague\" when there is\\nnot enough \"evidence\" in the data or standard modeling constructs do not \"fit\".\\nMoreover, the approach is scalable enough to be incorporated in\\nindustrial-strength process mining tools.\\n', \"  Polyphonic sound event detection (polyphonic SED) is an interesting but\\nchallenging task due to the concurrence of multiple sound events. Recently, SED\\nmethods based on convolutional neural networks (CNN) and recurrent neural\\nnetworks (RNN) have shown promising performance. Generally, CNN are designed\\nfor local feature extraction while RNN are used to model the temporal\\ndependency among these local features. Despite their success, it is still\\ninsufficient for existing deep learning techniques to separate individual sound\\nevent from their mixture, largely due to the overlapping characteristic of\\nfeatures. Motivated by the success of Capsule Networks (CapsNet), we propose a\\nmore suitable capsule based approach for polyphonic SED. Specifically, several\\ncapsule layers are designed to effectively select representative frequency\\nbands for each individual sound event. The temporal dependency of capsule's\\noutputs is then modeled by a RNN. And a dynamic threshold method is proposed\\nfor making the final decision based on RNN outputs. Experiments on the TUT-SED\\nSynthetic 2016 dataset show that the proposed approach obtains an F1-score of\\n68.8% and an error rate of 0.45, outperforming the previous state-of-the-art\\nmethod of 66.4% and 0.48, respectively.\\n\", '  Deep convolutional neural networks have achieved great success in various\\napplications. However, training an effective DNN model for a specific task is\\nrather challenging because it requires a prior knowledge or experience to\\ndesign the network architecture, repeated trial-and-error process to tune the\\nparameters, and a large set of labeled data to train the model. In this paper,\\nwe propose to overcome these challenges by actively adapting a pre-trained\\nmodel to a new task with less labeled examples. Specifically, the pre-trained\\nmodel is iteratively fine tuned based on the most useful examples. The examples\\nare actively selected based on a novel criterion, which jointly estimates the\\npotential contribution of an instance on optimizing the feature representation\\nas well as improving the classification model for the target task. On one hand,\\nthe pre-trained model brings plentiful information from its original task,\\navoiding redesign of the network architecture or training from scratch; and on\\nthe other hand, the labeling cost can be significantly reduced by active label\\nquerying. Experiments on multiple datasets and different pre-trained models\\ndemonstrate that the proposed approach can achieve cost-effective training of\\nDNNs.\\n', \"  We present a method that automatically evaluates emotional response from\\nspontaneous facial activity recorded by a depth camera. The automatic\\nevaluation of emotional response, or affect, is a fascinating challenge with\\nmany applications, including human-computer interaction, media tagging and\\nhuman affect prediction. Our approach in addressing this problem is based on\\nthe inferred activity of facial muscles over time, as captured by a depth\\ncamera recording an individual's facial activity. Our contribution is two-fold:\\nFirst, we constructed a database of publicly available short video clips, which\\nelicit a strong emotional response in a consistent manner across different\\nindividuals. Each video was tagged by its characteristic emotional response\\nalong 4 scales: \\\\emph{Valence, Arousal, Likability} and \\\\emph{Rewatch} (the\\ndesire to watch again). The second contribution is a two-step prediction\\nmethod, based on learning, which was trained and tested using this database of\\ntagged video clips. Our method was able to successfully predict the\\naforementioned 4 dimensional representation of affect, as well as to identify\\nthe period of strongest emotional response in the viewing recordings, in a\\nmethod that is blind to the video clip being watch, revealing a significantly\\nhigh agreement between the recordings of independent viewers.\\n\", '  Legal professionals worldwide are currently trying to get up-to-pace with the\\nexplosive growth in legal document availability through digital means. This\\ndrives a need for high efficiency Legal Information Retrieval (IR) and Question\\nAnswering (QA) methods. The IR task in particular has a set of unique\\nchallenges that invite the use of semantic motivated NLP techniques. In this\\nwork, a two-stage method for Legal Information Retrieval is proposed, combining\\nlexical statistics and distributional sentence representations in the context\\nof Competition on Legal Information Extraction/Entailment (COLIEE). The\\ncombination is done with the use of disambiguation rules, applied over the\\nrankings obtained through n-gram statistics. After the ranking is done, its\\nresults are evaluated for ambiguity, and disambiguation is done if a result is\\ndecided to be unreliable for a given query. Competition and experimental\\nresults indicate small gains in overall retrieval performance using the\\nproposed approach. Additionally, an analysis of error and improvement cases is\\npresented for a better understanding of the contributions.\\n', '  We propose a novel computational strategy for de novo design of molecules\\nwith desired properties termed ReLeaSE (Reinforcement Learning for Structural\\nEvolution). Based on deep and reinforcement learning approaches, ReLeaSE\\nintegrates two deep neural networks - generative and predictive - that are\\ntrained separately but employed jointly to generate novel targeted chemical\\nlibraries. ReLeaSE employs simple representation of molecules by their SMILES\\nstrings only. Generative models are trained with stack-augmented memory network\\nto produce chemically feasible SMILES strings, and predictive models are\\nderived to forecast the desired properties of the de novo generated compounds.\\nIn the first phase of the method, generative and predictive models are trained\\nseparately with a supervised learning algorithm. In the second phase, both\\nmodels are trained jointly with the reinforcement learning approach to bias the\\ngeneration of new chemical structures towards those with the desired physical\\nand/or biological properties. In the proof-of-concept study, we have employed\\nthe ReLeaSE method to design chemical libraries with a bias toward structural\\ncomplexity or biased toward compounds with either maximal, minimal, or specific\\nrange of physical properties such as melting point or hydrophobicity, as well\\nas to develop novel putative inhibitors of JAK2. The approach proposed herein\\ncan find a general use for generating targeted chemical libraries of novel\\ncompounds optimized for either a single desired property or multiple\\nproperties.\\n', \"  We present an approach for building an active agent that learns to segment\\nits visual observations into individual objects by interacting with its\\nenvironment in a completely self-supervised manner. The agent uses its current\\nsegmentation model to infer pixels that constitute objects and refines the\\nsegmentation model by interacting with these pixels. The model learned from\\nover 50K interactions generalizes to novel objects and backgrounds. To deal\\nwith noisy training signal for segmenting objects obtained by self-supervised\\ninteractions, we propose robust set loss. A dataset of robot's interactions\\nalong-with a few human labeled examples is provided as a benchmark for future\\nresearch. We test the utility of the learned segmentation model by providing\\nresults on a downstream vision-based control task of rearranging multiple\\nobjects into target configurations from visual inputs alone. Videos, code, and\\nrobotic interaction dataset are available at\\nthis https URL\\n\", '  Deep learning models can take weeks to train on a single GPU-equipped\\nmachine, necessitating scaling out DL training to a GPU-cluster. However,\\ncurrent distributed DL implementations can scale poorly due to substantial\\nparameter synchronization over the network, because the high throughput of GPUs\\nallows more data batches to be processed per unit time than CPUs, leading to\\nmore frequent network synchronization. We present Poseidon, an efficient\\ncommunication architecture for distributed DL on GPUs. Poseidon exploits the\\nlayered model structures in DL programs to overlap communication and\\ncomputation, reducing bursty network communication. Moreover, Poseidon uses a\\nhybrid communication scheme that optimizes the number of bytes required to\\nsynchronize each layer, according to layer properties and the number of\\nmachines. We show that Poseidon is applicable to different DL frameworks by\\nplugging Poseidon into Caffe and TensorFlow. We show that Poseidon enables\\nCaffe and TensorFlow to achieve 15.5x speed-up on 16 single-GPU machines, even\\nwith limited bandwidth (10GbE) and the challenging VGG19-22K network for image\\nclassification. Moreover, Poseidon-enabled TensorFlow achieves 31.5x speed-up\\nwith 32 single-GPU machines on Inception-V3, a 50% improvement over the\\nopen-source TensorFlow (20x speed-up).\\n', '  A minimal constructed language (conlang) is useful for experiments and\\ncomfortable for making tools. The Toki Pona (TP) conlang is minimal both in the\\nvocabulary (with only 14 letters and 124 lemmas) and in the (about) 10 syntax\\nrules. The language is useful for being a used and somewhat established minimal\\nconlang with at least hundreds of fluent speakers. This article exposes current\\nconcepts and resources for TP, and makes available Python (and Vim) scripted\\nroutines for the analysis of the language, synthesis of texts, syntax\\nhighlighting schemes, and the achievement of a preliminary TP Wordnet. Focus is\\non the analysis of the basic vocabulary, as corpus analyses were found. The\\nsynthesis is based on sentence templates, relates to context by keeping track\\nof used words, and renders larger texts by using a fixed number of phonemes\\n(e.g. for poems) and number of sentences, words and letters (e.g. for\\nparagraphs). Syntax highlighting reflects morphosyntactic classes given in the\\nofficial dictionary and different solutions are described and implemented in\\nthe well-established Vim text editor. The tentative TP Wordnet is made\\navailable in three patterns of relations between synsets and word lemmas. In\\nsummary, this text holds potentially novel conceptualizations about, and tools\\nand results in analyzing, synthesizing and syntax highlighting the TP language.\\n', '  In this paper, we investigate the multi-variate sequence classification\\nproblem from a multi-instance learning perspective. Real-world sequential data\\ncommonly show discriminative patterns only at specific time periods. For\\ninstance, we can identify a cropland during its growing season, but it looks\\nsimilar to a barren land after harvest or before planting. Besides, even within\\nthe same class, the discriminative patterns can appear in different periods of\\nsequential data. Due to such property, these discriminative patterns are also\\nreferred to as shifting patterns. The shifting patterns in sequential data\\nseverely degrade the performance of traditional classification methods without\\nsufficient training data.\\nWe propose a novel sequence classification method by automatically mining\\nshifting patterns from multi-variate sequence. The method employs a\\nmulti-instance learning approach to detect shifting patterns while also\\nmodeling temporal relationships within each multi-instance bag by an LSTM model\\nto further improve the classification performance. We extensively evaluate our\\nmethod on two real-world applications - cropland mapping and affective state\\nrecognition. The experiments demonstrate the superiority of our proposed method\\nin sequence classification performance and in detecting discriminative shifting\\npatterns.\\n', \"  The predictive power of neural networks often costs model interpretability.\\nSeveral techniques have been developed for explaining model outputs in terms of\\ninput features; however, it is difficult to translate such interpretations into\\nactionable insight. Here, we propose a framework to analyze predictions in\\nterms of the model's internal features by inspecting information flow through\\nthe network. Given a trained network and a test image, we select neurons by two\\nmetrics, both measured over a set of images created by perturbations to the\\ninput image: (1) magnitude of the correlation between the neuron activation and\\nthe network output and (2) precision of the neuron activation. We show that the\\nformer metric selects neurons that exert large influence over the network\\noutput while the latter metric selects neurons that activate on generalizable\\nfeatures. By comparing the sets of neurons selected by these two metrics, our\\nframework suggests a way to investigate the internal attention mechanisms of\\nconvolutional neural networks.\\n\", '  Finding semantically rich and computer-understandable representations for\\ntextual dialogues, utterances and words is crucial for dialogue systems (or\\nconversational agents), as their performance mostly depends on understanding\\nthe context of conversations. Recent research aims at finding distributed\\nvector representations (embeddings) for words, such that semantically similar\\nwords are relatively close within the vector-space. Encoding the \"meaning\" of\\ntext into vectors is a current trend, and text can range from words, phrases\\nand documents to actual human-to-human conversations. In recent research\\napproaches, responses have been generated utilizing a decoder architecture,\\ngiven the vector representation of the current conversation. In this paper, the\\nutilization of embeddings for answer retrieval is explored by using\\nLocality-Sensitive Hashing Forest (LSH Forest), an Approximate Nearest Neighbor\\n(ANN) model, to find similar conversations in a corpus and rank possible\\ncandidates. Experimental results on the well-known Ubuntu Corpus (in English)\\nand a customer service chat dataset (in Dutch) show that, in combination with a\\ncandidate selection method, retrieval-based approaches outperform generative\\nones and reveal promising future research directions towards the usability of\\nsuch a system.\\n', '  Associating image regions with text queries has been recently explored as a\\nnew way to bridge visual and linguistic representations. A few pioneering\\napproaches have been proposed based on recurrent neural language models trained\\ngeneratively (e.g., generating captions), but achieving somewhat limited\\nlocalization accuracy. To better address natural-language-based visual entity\\nlocalization, we propose a discriminative approach. We formulate a\\ndiscriminative bimodal neural network (DBNet), which can be trained by a\\nclassifier with extensive use of negative samples. Our training objective\\nencourages better localization on single images, incorporates text phrases in a\\nbroad range, and properly pairs image regions with text phrases into positive\\nand negative examples. Experiments on the Visual Genome dataset demonstrate the\\nproposed DBNet significantly outperforms previous state-of-the-art methods both\\nfor localization on single images and for detection on multiple images. We we\\nalso establish an evaluation protocol for natural-language visual detection.\\n', '  In this paper, a deep domain adaptation based method for video smoke\\ndetection is proposed to extract a powerful feature representation of smoke.\\nDue to the smoke image samples limited in scale and diversity for deep CNN\\ntraining, we systematically produced adequate synthetic smoke images with a\\nwide variation in the smoke shape, background and lighting conditions.\\nConsidering that the appearance gap (dataset bias) between synthetic and real\\nsmoke images degrades significantly the performance of the trained model on the\\ntest set composed fully of real images, we build deep architectures based on\\ndomain adaptation to confuse the distributions of features extracted from\\nsynthetic and real smoke images. This approach expands the domain-invariant\\nfeature space for smoke image samples. With their approximate feature\\ndistribution off non-smoke images, the recognition rate of the trained model is\\nimproved significantly compared to the model trained directly on mixed dataset\\nof synthetic and real images. Experimentally, several deep architectures with\\ndifferent design choices are applied to the smoke detector. The ultimate\\nframework can get a satisfactory result on the test set. We believe that our\\napproach is a start in the direction of utilizing deep neural networks enhanced\\nwith synthetic smoke images for video smoke detection.\\n', '  The same concept can mean different things or be instantiated in different\\nforms depending on context, suggesting a degree of flexibility within the\\nconceptual system. We propose that a compositional network model can be used to\\ncapture and predict this flexibility. We modeled individual concepts (e.g.,\\nBANANA, BOTTLE) as graph-theoretical networks, in which properties (e.g.,\\nYELLOW, SWEET) were represented as nodes and their associations as edges. In\\nthis framework, networks capture the within-concept statistics that reflect how\\nproperties correlate with each other across instances of a concept. We ran a\\nclassification analysis using graph eigendecomposition to validate these\\nmodels, and find that these models can successfully discriminate between object\\nconcepts. We then computed formal measures from these concept networks and\\nexplored their relationship to conceptual structure. We find that diversity\\ncoefficients and core-periphery structure can be interpreted as network-based\\nmeasures of conceptual flexibility and stability, respectively. These results\\nsupport the feasibility of a concept network framework and highlight its\\nability to formally capture important characteristics of the conceptual system.\\n', '  We address the problem of \\\\emph{instance label stability} in multiple\\ninstance learning (MIL) classifiers. These classifiers are trained only on\\nglobally annotated images (bags), but often can provide fine-grained\\nannotations for image pixels or patches (instances). This is interesting for\\ncomputer aided diagnosis (CAD) and other medical image analysis tasks for which\\nonly a coarse labeling is provided. Unfortunately, the instance labels may be\\nunstable. This means that a slight change in training data could potentially\\nlead to abnormalities being detected in different parts of the image, which is\\nundesirable from a CAD point of view. Despite MIL gaining popularity in the CAD\\nliterature, this issue has not yet been addressed. We investigate the stability\\nof instance labels provided by several MIL classifiers on 5 different datasets,\\nof which 3 are medical image datasets (breast histopathology, diabetic\\nretinopathy and computed tomography lung images). We propose an unsupervised\\nmeasure to evaluate instance stability, and demonstrate that a\\nperformance-stability trade-off can be made when comparing MIL classifiers.\\n', '  Diagnosis and risk stratification of cancer and many other diseases require\\nthe detection of genomic breakpoints as a prerequisite of calling copy number\\nalterations (CNA). This, however, is still challenging and requires\\ntime-consuming manual curation. As deep-learning methods outperformed classical\\nstate-of-the-art algorithms in various domains and have also been successfully\\napplied to life science problems including medicine and biology, we here\\npropose Deep SNP, a novel Deep Neural Network to learn from genomic data.\\nSpecifically, we used a manually curated dataset from 12 genomic single\\nnucleotide polymorphism array (SNPa) profiles as truth-set and aimed at\\npredicting the presence or absence of genomic breakpoints, an indicator of\\nstructural chromosomal variations, in windows of 40,000 probes. We compare our\\nresults with well-known neural network models as well as Rawcopy though this\\ntool is designed to predict breakpoints and in addition genomic segments with\\nhigh sensitivity. We show, that Deep SNP is capable of successfully predicting\\nthe presence or absence of a breakpoint in large genomic windows and\\noutperforms state-of-the-art neural network models. Qualitative examples\\nsuggest that integration of a localization unit may enable breakpoint detection\\nand prediction of genomic segments, even if the breakpoint coordinates were not\\nprovided for network training. These results warrant further evaluation of\\nDeepSNP for breakpoint localization and subsequent calling of genomic segments.\\n', '  Accurately predicting and detecting interstitial lung disease (ILD) patterns\\ngiven any computed tomography (CT) slice without any pre-processing\\nprerequisites, such as manually delineated regions of interest (ROIs), is a\\nclinically desirable, yet challenging goal. The majority of existing work\\nrelies on manually-provided ILD ROIs to extract sampled 2D image patches from\\nCT slices and, from there, performs patch-based ILD categorization. Acquiring\\nmanual ROIs is labor intensive and serves as a bottleneck towards\\nfully-automated CT imaging ILD screening over large-scale populations.\\nFurthermore, despite the considerable high frequency of more than one ILD\\npattern on a single CT slice, previous works are only designed to detect one\\nILD pattern per slice or patch.\\nTo tackle these two critical challenges, we present multi-label deep\\nconvolutional neural networks (CNNs) for detecting ILDs from holistic CT slices\\n(instead of ROIs or sub-images). Conventional single-labeled CNN models can be\\naugmented to cope with the possible presence of multiple ILD pattern labels,\\nvia 1) continuous-valued deep regression based robust norm loss functions or 2)\\na categorical objective as the sum of element-wise binary logistic losses. Our\\nmethods are evaluated and validated using a publicly available database of 658\\npatient CT scans under five-fold cross-validation, achieving promising\\nperformance on detecting four major ILD patterns: Ground Glass, Reticular,\\nHoneycomb, and Emphysema. We also investigate the effectiveness of a CNN\\nactivation-based deep-feature encoding scheme using Fisher vector encoding,\\nwhich treats ILD detection as spatially-unordered deep texture classification.\\n', '  This paper considers the challenging task of long-term video interpolation.\\nUnlike most existing methods that only generate few intermediate frames between\\nexisting adjacent ones, we attempt to speculate or imagine the procedure of an\\nepisode and further generate multiple frames between two non-consecutive frames\\nin videos. In this paper, we present a novel deep architecture called\\nbidirectional predictive network (BiPN) that predicts intermediate frames from\\ntwo opposite directions. The bidirectional architecture allows the model to\\nlearn scene transformation with time as well as generate longer video\\nsequences. Besides, our model can be extended to predict multiple possible\\nprocedures by sampling different noise vectors. A joint loss composed of clues\\nin image and feature spaces and adversarial loss is designed to train our\\nmodel. We demonstrate the advantages of BiPN on two benchmarks Moving 2D Shapes\\nand UCF101 and report competitive results to recent approaches.\\n', \"  In this project, we propose a novel approach for estimating depth from RGB\\nimages. Traditionally, most work uses a single RGB image to estimate depth,\\nwhich is inherently difficult and generally results in poor performance, even\\nwith thousands of data examples. In this work, we alternatively use multiple\\nRGB images that were captured while changing the focus of the camera's lens.\\nThis method leverages the natural depth information correlated to the different\\npatterns of clarity/blur in the sequence of focal images, which helps\\ndistinguish objects at different depths. Since no such data set exists for\\nlearning this mapping, we collect our own data set using customized hardware.\\nWe then use a convolutional neural network for learning the depth from the\\nstacked focal images. Comparative studies were conducted on both a standard\\nRGBD data set and our own data set (learning from both single and multiple\\nimages), and results verified that stacked focal images yield better depth\\nestimation than using just single RGB image.\\n\", '  Convolutional neural networks (CNNs) have recently emerged as a popular\\nbuilding block for natural language processing (NLP). Despite their success,\\nmost existing CNN models employed in NLP share the same learned (and static)\\nset of filters for all input sentences. In this paper, we consider an approach\\nof using a small meta network to learn context-sensitive convolutional filters\\nfor text processing. The role of meta network is to abstract the contextual\\ninformation of a sentence or document into a set of input-aware filters. We\\nfurther generalize this framework to model sentence pairs, where a\\nbidirectional filter generation mechanism is introduced to encapsulate\\nco-dependent sentence representations. In our benchmarks on four different\\ntasks, including ontology classification, sentiment analysis, answer sentence\\nselection, and paraphrase identification, our proposed model, a modified CNN\\nwith context-sensitive filters, consistently outperforms the standard CNN and\\nattention-based CNN baselines. By visualizing the learned context-sensitive\\nfilters, we further validate and rationalize the effectiveness of proposed\\nframework.\\n', '  Deep neural networks (DNN) excel at extracting patterns. Through\\nrepresentation learning and automated feature engineering on large datasets,\\nsuch models have been highly successful in computer vision and natural language\\napplications. Designing optimal network architectures from a principled or\\nrational approach however has been less than successful, with the best\\nsuccessful approaches utilizing an additional machine learning algorithm to\\ntune the network hyperparameters. However, in many technical fields, there\\nexist established domain knowledge and understanding about the subject matter.\\nIn this work, we develop a novel furcated neural network architecture that\\nutilizes domain knowledge as high-level design principles of the network. We\\ndemonstrate proof-of-concept by developing IL-Net, a furcated network for\\npredicting the properties of ionic liquids, which is a class of complex\\nmulti-chemicals entities. Compared to existing state-of-the-art approaches, we\\nshow that furcated networks can improve model accuracy by approximately 20-35%,\\nwithout using additional labeled data. Lastly, we distill two key design\\nprinciples for furcated networks that can be adapted to other domains.\\n', '  The analysis of neuroimaging data poses several strong challenges, in\\nparticular, due to its high dimensionality, its strong spatio-temporal\\ncorrelation and the comparably small sample sizes of the respective datasets.\\nTo address these challenges, conventional decoding approaches such as the\\nsearchlight reduce the complexity of the decoding problem by considering local\\nclusters of voxels only. Thereby, neglecting the distributed spatial patterns\\nof brain activity underlying many cognitive states. In this work, we introduce\\nthe DLight framework, which overcomes these challenges by utilizing a long\\nshort-term memory unit (LSTM) based deep neural network architecture to analyze\\nthe spatial dependency structure of whole-brain fMRI data. In order to maintain\\ninterpretability of the neuroimaging data, we adapt the layer-wise relevance\\npropagation (LRP) method. Thereby, we enable the neuroscientist user to study\\nthe learned association of the LSTM between the data and the cognitive state of\\nthe individual. We demonstrate the versatility of DLight by applying it to a\\nlarge fMRI dataset of the Human Connectome Project. We show that the decoding\\nperformance of our method scales better with large datasets, and moreover\\noutperforms conventional decoding approaches, while still detecting\\nphysiologically appropriate brain areas for the cognitive states classified. We\\nalso demonstrate that DLight is able to detect these areas on several levels of\\ndata granularity (i.e., group, subject, trial, time point).\\n', '  With the proliferation of social media, fashion inspired from celebrities,\\nreputed designers as well as fashion influencers has shortened the cycle of\\nfashion design and manufacturing. However, with the explosion of fashion\\nrelated content and large number of user generated fashion photos, it is an\\narduous task for fashion designers to wade through social media photos and\\ncreate a digest of trending fashion. This necessitates deep parsing of fashion\\nphotos on social media to localize and classify multiple fashion items from a\\ngiven fashion photo. While object detection competitions such as MSCOCO have\\nthousands of samples for each of the object categories, it is quite difficult\\nto get large labeled datasets for fast fashion items. Moreover,\\nstate-of-the-art object detectors do not have any functionality to ingest large\\namount of unlabeled data available on social media in order to fine tune object\\ndetectors with labeled datasets. In this work, we show application of a generic\\nobject detector, that can be pretrained in an unsupervised manner, on 24\\ncategories from recently released Open Images V4 dataset. We first train the\\nbase architecture of the object detector using unsupervisd learning on 60K\\nunlabeled photos from 24 categories gathered from social media, and then\\nsubsequently fine tune it on 8.2K labeled photos from Open Images V4 dataset.\\nOn 300 X 300 image inputs, we achieve 72.7% mAP on a test dataset of 2.4K\\nphotos while performing 11% to 17% better as compared to the state-of-the-art\\nobject detectors. We show that this improvement is due to our choice of\\narchitecture that lets us do unsupervised learning and that performs\\nsignificantly better in identifying small objects.\\n', '  Despite rapid advances in face recognition, there remains a clear gap between\\nthe performance of still image-based face recognition and video-based face\\nrecognition, due to the vast difference in visual quality between the domains\\nand the difficulty of curating diverse large-scale video datasets. This paper\\naddresses both of those challenges, through an image to video feature-level\\ndomain adaptation approach, to learn discriminative video frame\\nrepresentations. The framework utilizes large-scale unlabeled video data to\\nreduce the gap between different domains while transferring discriminative\\nknowledge from large-scale labeled still images. Given a face recognition\\nnetwork that is pretrained in the image domain, the adaptation is achieved by\\n(i) distilling knowledge from the network to a video adaptation network through\\nfeature matching, (ii) performing feature restoration through synthetic data\\naugmentation and (iii) learning a domain-invariant feature through a domain\\nadversarial discriminator. We further improve performance through a\\ndiscriminator-guided feature fusion that boosts high-quality frames while\\neliminating those degraded by video domain-specific factors. Experiments on the\\nYouTube Faces and IJB-A datasets demonstrate that each module contributes to\\nour feature-level domain adaptation framework and substantially improves video\\nface recognition performance to achieve state-of-the-art accuracy. We\\ndemonstrate qualitatively that the network learns to suppress diverse artifacts\\nin videos such as pose, illumination or occlusion without being explicitly\\ntrained for them.\\n', '  We propose a Label Propagation based algorithm for weakly supervised text\\nclassification. We construct a graph where each document is represented by a\\nnode and edge weights represent similarities among the documents. Additionally,\\nwe discover underlying topics using Latent Dirichlet Allocation (LDA) and\\nenrich the document graph by including the topics in the form of additional\\nnodes. The edge weights between a topic and a text document represent level of\\n\"affinity\" between them. Our approach does not require document level\\nlabelling, instead it expects manual labels only for topic nodes. This\\nsignificantly minimizes the level of supervision needed as only a few topics\\nare observed to be enough for achieving sufficiently high accuracy. The Label\\nPropagation Algorithm is employed on this enriched graph to propagate labels\\namong the nodes. Our approach combines the advantages of Label Propagation\\n(through document-document similarities) and Topic Modelling (for minimal but\\nsmart supervision). We demonstrate the effectiveness of our approach on various\\ndatasets and compare with state-of-the-art weakly supervised text\\nclassification approaches.\\n', '  Experimental determination of protein function is resource-consuming. As an\\nalternative, computational prediction of protein function has received\\nattention. In this context, protein structural classification (PSC) can help,\\nby allowing for determining structural classes of currently unclassified\\nproteins based on their features, and then relying on the fact that proteins\\nwith similar structures have similar functions. Existing PSC approaches rely on\\nsequence-based or direct (\"raw\") 3-dimensional (3D) structure-based protein\\nfeatures. In contrast, we first model 3D structures as protein structure\\nnetworks (PSNs). Then, we use (\"processed\") network-based features for PSC. We\\npropose the use of graphlets, state-of-the-art features in many domains of\\nnetwork science, in the task of PSC. Moreover, because graphlets can deal only\\nwith unweighted PSNs, and because accounting for edge weights when constructing\\nPSNs could improve PSC accuracy, we also propose a deep learning framework that\\nautomatically learns network features from the weighted PSNs. When evaluated on\\na large set of ~9,400 CATH and ~12,800 SCOP protein domains (spanning 36 PSN\\nsets), our proposed approaches are superior to existing PSC approaches in terms\\nof accuracy, with comparable running time.\\n', '  Global Style Tokens (GSTs) are a recently-proposed method to learn latent\\ndisentangled representations of high-dimensional data. GSTs can be used within\\nTacotron, a state-of-the-art end-to-end text-to-speech synthesis system, to\\nuncover expressive factors of variation in speaking style. In this work, we\\nintroduce the Text-Predicted Global Style Token (TP-GST) architecture, which\\ntreats GST combination weights or style embeddings as \"virtual\" speaking style\\nlabels within Tacotron. TP-GST learns to predict stylistic renderings from text\\nalone, requiring neither explicit labels during training nor auxiliary inputs\\nfor inference. We show that, when trained on a dataset of expressive speech,\\nour system generates audio with more pitch and energy variation than two\\nstate-of-the-art baseline models. We further demonstrate that TP-GSTs can\\nsynthesize speech with background noise removed, and corroborate these analyses\\nwith positive results on human-rated listener preference audiobook tasks.\\nFinally, we demonstrate that multi-speaker TP-GST models successfully factorize\\nspeaker identity and speaking style. We provide a website with audio samples\\nfor each of our findings.\\n', '  Sentiment analysis is the Natural Language Processing (NLP) task dealing with\\nthe detection and classification of sentiments in texts. While some tasks deal\\nwith identifying the presence of sentiment in the text (Subjectivity analysis),\\nother tasks aim at determining the polarity of the text categorizing them as\\npositive, negative and neutral. Whenever there is a presence of sentiment in\\nthe text, it has a source (people, group of people or any entity) and the\\nsentiment is directed towards some entity, object, event or person. Sentiment\\nanalysis tasks aim to determine the subject, the target and the polarity or\\nvalence of the sentiment. In our work, we try to automatically extract\\nsentiment (positive or negative) from Facebook posts using a machine learning\\napproach.While some works have been done in code-mixed social media data and in\\nsentiment analysis separately, our work is the first attempt (as of now) which\\naims at performing sentiment analysis of code-mixed social media text. We have\\nused extensive pre-processing to remove noise from raw text. Multilayer\\nPerceptron model has been used to determine the polarity of the sentiment. We\\nhave also developed the corpus for this task by manually labeling Facebook\\nposts with their associated sentiments.\\n', \"  Controlled generation of text is of high practical use. Recent efforts have\\nmade impressive progress in generating or editing sentences with given textual\\nattributes (e.g., sentiment). This work studies a new practical setting of text\\ncontent manipulation. Given a structured record, such as `(PLAYER: Lebron,\\nPOINTS: 20, ASSISTS: 10)', and a reference sentence, such as `Kobe easily\\ndropped 30 points', we aim to generate a sentence that accurately describes the\\nfull content in the record, with the same writing style (e.g., wording,\\ntransitions) of the reference. The problem is unsupervised due to lack of\\nparallel data in practice, and is challenging to minimally yet effectively\\nmanipulate the text (by rewriting/adding/deleting text portions) to ensure\\nfidelity to the structured content. We derive a dataset from a basketball game\\nreport corpus as our testbed, and develop a neural method with unsupervised\\ncompeting objectives and explicit content coverage constraints. Automatic and\\nhuman evaluations show superiority of our approach over competitive methods\\nincluding a strong rule-based baseline and prior approaches designed for style\\ntransfer.\\n\", '  Social learning, i.e., students learning from each other through social\\ninteractions, has the potential to significantly scale up instruction in online\\neducation. In many cases, such as in massive open online courses (MOOCs),\\nsocial learning is facilitated through discussion forums hosted by course\\nproviders. In this paper, we propose a probabilistic model for the process of\\nlearners posting on such forums, using point processes. Different from existing\\nworks, our method integrates topic modeling of the post text, timescale\\nmodeling of the decay in post activity over time, and learner topic interest\\nmodeling into a single model, and infers this information from user data. Our\\nmethod also varies the excitation levels induced by posts according to the\\nthread structure, to reflect typical notification settings in discussion\\nforums. We experimentally validate the proposed model on three real-world MOOC\\ndatasets, with the largest one containing up to 6,000 learners making 40,000\\nposts in 5,000 threads. Results show that our model excels at thread\\nrecommendation, achieving significant improvement over a number of baselines,\\nthus showing promise of being able to direct learners to threads that they are\\ninterested in more efficiently. Moreover, we demonstrate analytics that our\\nmodel parameters can provide, such as the timescales of different topic\\ncategories in a course.\\n', '  Background: Widespread adoption of electronic health records (EHRs) has\\nenabled secondary use of EHR data for clinical research and healthcare\\ndelivery. Natural language processing (NLP) techniques have shown promise in\\ntheir capability to extract the embedded information in unstructured clinical\\ndata, and information retrieval (IR) techniques provide flexible and scalable\\nsolutions that can augment the NLP systems for retrieving and ranking relevant\\nrecords. Methods: In this paper, we present the implementation of Cohort\\nRetrieval Enhanced by Analysis of Text from EHRs (CREATE), a cohort retrieval\\nsystem that can execute textual cohort selection queries on both structured and\\nunstructured EHR data. CREATE is a proof-of-concept system that leverages a\\ncombination of structured queries and IR techniques on NLP results to improve\\ncohort retrieval performance while adopting the Observational Medical Outcomes\\nPartnership (OMOP) Common Data Model (CDM) to enhance model portability. The\\nNLP component empowered by cTAKES is used to extract CDM concepts from textual\\nqueries. We design a hierarchical index in Elasticsearch to support CDM concept\\nsearch utilizing IR techniques and frameworks. Results: Our case study on 5\\ncohort identification queries evaluated using the IR metric, P@5 (Precision at\\n5) at both the patient-level and document-level, demonstrates that CREATE\\nachieves an average P@5 of 0.90, which outperforms systems using only\\nstructured data or only unstructured data with average P@5s of 0.54 and 0.74,\\nrespectively.\\n', '  Deep reinforcement learning (DRL) has shown incredible performance in\\nlearning various tasks to the human level. However, unlike human perception,\\ncurrent DRL models connect the entire low-level sensory input to the\\nstate-action values rather than exploiting the relationship between and among\\nentities that constitute the sensory input. Because of this difference, DRL\\nneeds vast amount of experience samples to learn. In this paper, we propose a\\nMulti-focus Attention Network (MANet) which mimics human ability to spatially\\nabstract the low-level sensory input into multiple entities and attend to them\\nsimultaneously. The proposed method first divides the low-level input into\\nseveral segments which we refer to as partial states. After this segmentation,\\nparallel attention layers attend to the partial states relevant to solving the\\ntask. Our model estimates state-action values using these attended partial\\nstates. In our experiments, MANet attains highest scores with significantly\\nless experience samples. Additionally, the model shows higher performance\\ncompared to the Deep Q-network and the single attention model as benchmarks.\\nFurthermore, we extend our model to attentive communication model for\\nperforming multi-agent cooperative tasks. In multi-agent cooperative task\\nexperiments, our model shows 20% faster learning than existing state-of-the-art\\nmodel.\\n', '  This paper proposes a practical approach for automatic sleep stage\\nclassification based on a multi-level feature learning framework and Recurrent\\nNeural Network (RNN) classifier using heart rate and wrist actigraphy derived\\nfrom a wearable device. The feature learning framework is designed to extract\\nlow- and mid-level features. Low-level features capture temporal and frequency\\ndomain properties and mid-level features learn compositions and structural\\ninformation of signals. Since sleep staging is a sequential problem with\\nlong-term dependencies, we take advantage of RNNs with Bidirectional Long\\nShort-Term Memory (BLSTM) architectures for sequence data learning. To simulate\\nthe actual situation of daily sleep, experiments are conducted with a resting\\ngroup in which sleep is recorded in resting state, and a comprehensive group in\\nwhich both resting sleep and non-resting sleep are included.We evaluate the\\nalgorithm based on an eight-fold cross validation to classify five sleep stages\\n(W, N1, N2, N3, and REM). The proposed algorithm achieves weighted precision,\\nrecall and F1 score of 58.0%, 60.3%, and 58.2% in the resting group and 58.5%,\\n61.1%, and 58.5% in the comprehensive group, respectively. Various comparison\\nexperiments demonstrate the effectiveness of feature learning and BLSTM. We\\nfurther explore the influence of depth and width of RNNs on performance. Our\\nmethod is specially proposed for wearable devices and is expected to be\\napplicable for long-term sleep monitoring at home. Without using too much prior\\ndomain knowledge, our method has the potential to generalize sleep disorder\\ndetection.\\n', '  Generative adversarial networks (GAN) have been effective for learning\\ngenerative models for real-world data. However, existing GANs (GAN and its\\nvariants) tend to suffer from training problems such as instability and mode\\ncollapse. In this paper, we propose a novel GAN framework called evolutionary\\ngenerative adversarial networks (E-GAN) for stable GAN training and improved\\ngenerative performance. Unlike existing GANs, which employ a pre-defined\\nadversarial objective function alternately training a generator and a\\ndiscriminator, we utilize different adversarial training objectives as mutation\\noperations and evolve a population of generators to adapt to the environment\\n(i.e., the discriminator). We also utilize an evaluation mechanism to measure\\nthe quality and diversity of generated samples, such that only well-performing\\ngenerator(s) are preserved and used for further training. In this way, E-GAN\\novercomes the limitations of an individual adversarial training objective and\\nalways preserves the best offspring, contributing to progress in and the\\nsuccess of GANs. Experiments on several datasets demonstrate that E-GAN\\nachieves convincing generative performance and reduces the training problems\\ninherent in existing GANs.\\n', '  Improvements of entity-relationship (E-R) search techniques have been\\nhampered by a lack of test collections, particularly for complex queries\\ninvolving multiple entities and relationships. In this paper we describe a\\nmethod for generating E-R test queries to support comprehensive E-R search\\nexperiments. Queries and relevance judgments are created from content that\\nexists in a tabular form where columns represent entity types and the table\\nstructure implies one or more relationships among the entities. Editorial work\\ninvolves creating natural language queries based on relationships represented\\nby the entries in the table. We have publicly released the RELink test\\ncollection comprising 600 queries and relevance judgments obtained from a\\nsample of Wikipedia List-of-lists-of-lists tables. The latter comprise tuples\\nof entities that are extracted from columns and labelled by corresponding\\nentity types and relationships they represent. In order to facilitate research\\nin complex E-R retrieval, we have created and released as open source the\\nRELink Framework that includes Apache Lucene indexing and search specifically\\ntailored to E-R retrieval. RELink includes entity and relationship indexing\\nbased on the ClueWeb-09-B Web collection with FACC1 text span annotations\\nlinked to Wikipedia entities. With ready to use search resources and a\\ncomprehensive test collection, we support community in pursuing E-R research at\\nscale.\\n', '  While most schemes for automatic cover song identification have focused on\\nnote-based features such as HPCP and chord profiles, a few recent papers\\nsurprisingly showed that local self-similarities of MFCC-based features also\\nhave classification power for this task. Since MFCC and HPCP capture\\ncomplementary information, we design an unsupervised algorithm that combines\\nnormalized, beat-synchronous blocks of these features using cross-similarity\\nfusion before attempting to locally align a pair of songs. As an added bonus,\\nour scheme naturally incorporates structural information in each song to fill\\nin alignment gaps where both feature sets fail. We show a striking jump in\\nperformance over MFCC and HPCP alone, achieving a state of the art mean\\nreciprocal rank of 0.87 on the Covers80 dataset. We also introduce a new\\nmedium-sized hand designed benchmark dataset called \"Covers 1000,\" which\\nconsists of 395 cliques of cover songs for a total of 1000 songs, and we show\\nthat our algorithm achieves an MRR of 0.9 on this dataset for the first\\ncorrectly identified song in a clique. We provide the precomputed HPCP and MFCC\\nfeatures, as well as beat intervals, for all songs in the Covers 1000 dataset\\nfor use in further research.\\n', '  Many machine intelligence techniques are developed in E-commerce and one of\\nthe most essential components is the representation of IDs, including user ID,\\nitem ID, product ID, store ID, brand ID, category ID etc. The classical\\nencoding based methods (like one-hot encoding) are inefficient in that it\\nsuffers sparsity problems due to its high dimension, and it cannot reflect the\\nrelationships among IDs, either homogeneous or heterogeneous ones. In this\\npaper, we propose an embedding based framework to learn and transfer the\\nrepresentation of IDs. As the implicit feedbacks of users, a tremendous amount\\nof item ID sequences can be easily collected from the interactive sessions. By\\njointly using these informative sequences and the structural connections among\\nIDs, all types of IDs can be embedded into one low-dimensional semantic space.\\nSubsequently, the learned representations are utilized and transferred in four\\nscenarios: (i) measuring the similarity between items, (ii) transferring from\\nseen items to unseen items, (iii) transferring across different domains, (iv)\\ntransferring across different tasks. We deploy and evaluate the proposed\\napproach in Hema App and the results validate its effectiveness.\\n', '  In this paper, we propose a new differentiable neural network alignment\\nmechanism for text-dependent speaker verification which uses alignment models\\nto produce a supervector representation of an utterance. Unlike previous works\\nwith similar approaches, we do not extract the embedding of an utterance from\\nthe mean reduction of the temporal dimension. Our system replaces the mean by a\\nphrase alignment model to keep the temporal structure of each phrase which is\\nrelevant in this application since the phonetic information is part of the\\nidentity in the verification task. Moreover, we can apply a convolutional\\nneural network as front-end, and thanks to the alignment process being\\ndifferentiable, we can train the whole network to produce a supervector for\\neach utterance which will be discriminative with respect to the speaker and the\\nphrase simultaneously. As we show, this choice has the advantage that the\\nsupervector encodes the phrase and speaker information providing good\\nperformance in text-dependent speaker verification tasks. In this work, the\\nprocess of verification is performed using a basic similarity metric, due to\\nsimplicity, compared to other more elaborate models that are commonly used. The\\nnew model using alignment to produce supervectors was tested on the\\nRSR2015-Part I database for text-dependent speaker verification, providing\\ncompetitive results compared to similar size networks using the mean to extract\\nembeddings.\\n', \"  This paper aims to bridge the affective gap between image content and the\\nemotional response of the viewer it elicits by using High-Level Concepts\\n(HLCs). In contrast to previous work that relied solely on low-level features\\nor used convolutional neural network (CNN) as a black-box, we use HLCs\\ngenerated by pretrained CNNs in an explicit way to investigate the\\nrelations/associations between these HLCs and a (small) set of Ekman's\\nemotional classes. As a proof-of-concept, we first propose a linear admixture\\nmodel for modeling these relations, and the resulting computational framework\\nallows us to determine the associations between each emotion class and certain\\nHLCs (objects and places). This linear model is further extended to a nonlinear\\nmodel using support vector regression (SVR) that aims to predict the viewer's\\nemotional response using both low-level image features and HLCs extracted from\\nimages. These class-specific regressors are then assembled into a regressor\\nensemble that provide a flexible and effective predictor for predicting\\nviewer's emotional responses from images. Experimental results have\\ndemonstrated that our results are comparable to existing methods, with a clear\\nview of the association between HLCs and emotional classes that is ostensibly\\nmissing in most existing work.\\n\", '  Nowadays, a big part of people rely on available content in social media in\\ntheir decisions (e.g. reviews and feedback on a topic or product). The\\npossibility that anybody can leave a review provide a golden opportunity for\\nspammers to write spam reviews about products and services for different\\ninterests. Identifying these spammers and the spam content is a hot topic of\\nresearch and although a considerable number of studies have been done recently\\ntoward this end, but so far the methodologies put forth still barely detect\\nspam reviews, and none of them show the importance of each extracted feature\\ntype. In this study, we propose a novel framework, named NetSpam, which\\nutilizes spam features for modeling review datasets as heterogeneous\\ninformation networks to map spam detection procedure into a classification\\nproblem in such networks. Using the importance of spam features help us to\\nobtain better results in terms of different metrics experimented on real-world\\nreview datasets from Yelp and Amazon websites. The results show that NetSpam\\noutperforms the existing methods and among four categories of features;\\nincluding review-behavioral, user-behavioral, reviewlinguistic,\\nuser-linguistic, the first type of features performs better than the other\\ncategories.\\n', \"  We address the problem of efficient acoustic-model refinement (continuous\\nretraining) using semi-supervised and active learning for a low resource Indian\\nlanguage, wherein the low resource constraints are having i) a small labeled\\ncorpus from which to train a baseline `seed' acoustic model and ii) a large\\ntraining corpus without orthographic labeling or from which to perform a data\\nselection for manual labeling at low costs. The proposed semi-supervised\\nlearning decodes the unlabeled large training corpus using the seed model and\\nthrough various protocols, selects the decoded utterances with high reliability\\nusing confidence levels (that correlate to the WER of the decoded utterances)\\nand iterative bootstrapping. The proposed active learning protocol uses\\nconfidence level based metric to select the decoded utterances from the large\\nunlabeled corpus for further labeling. The semi-supervised learning protocols\\ncan offer a WER reduction, from a poorly trained seed model, by as much as 50%\\nof the best WER-reduction realizable from the seed model's WER, if the large\\ncorpus were labeled and used for acoustic-model training. The active learning\\nprotocols allow that only 60% of the entire training corpus be manually\\nlabeled, to reach the same performance as the entire data.\\n\", '  Impressive image captioning results are achieved in domains with plenty of\\ntraining image and sentence pairs (e.g., MSCOCO). However, transferring to a\\ntarget domain with significant domain shifts but no paired training data\\n(referred to as cross-domain image captioning) remains largely unexplored. We\\npropose a novel adversarial training procedure to leverage unpaired data in the\\ntarget domain. Two critic networks are introduced to guide the captioner,\\nnamely domain critic and multi-modal critic. The domain critic assesses whether\\nthe generated sentences are indistinguishable from sentences in the target\\ndomain. The multi-modal critic assesses whether an image and its generated\\nsentence are a valid pair. During training, the critics and captioner act as\\nadversaries -- captioner aims to generate indistinguishable sentences, whereas\\ncritics aim at distinguishing them. The assessment improves the captioner\\nthrough policy gradient updates. During inference, we further propose a novel\\ncritic-based planning method to select high-quality sentences without\\nadditional supervision (e.g., tags). To evaluate, we use MSCOCO as the source\\ndomain and four other datasets (CUB-200-2011, Oxford-102, TGIF, and Flickr30k)\\nas the target domains. Our method consistently performs well on all datasets.\\nIn particular, on CUB-200-2011, we achieve 21.8% CIDEr-D improvement after\\nadaptation. Utilizing critics during inference further gives another 4.5%\\nboost.\\n', '  Data diversity is critical to success when training deep learning models.\\nMedical imaging data sets are often imbalanced as pathologic findings are\\ngenerally rare, which introduces significant challenges when training deep\\nlearning models. In this work, we propose a method to generate synthetic\\nabnormal MRI images with brain tumors by training a generative adversarial\\nnetwork using two publicly available data sets of brain MRI. We demonstrate two\\nunique benefits that the synthetic images provide. First, we illustrate\\nimproved performance on tumor segmentation by leveraging the synthetic images\\nas a form of data augmentation. Second, we demonstrate the value of generative\\nmodels as an anonymization tool, achieving comparable tumor segmentation\\nresults when trained on the synthetic data versus when trained on real subject\\ndata. Together, these results offer a potential solution to two of the largest\\nchallenges facing machine learning in medical imaging, namely the small\\nincidence of pathological findings, and the restrictions around sharing of\\npatient data.\\n', '  Even though active learning forms an important pillar of machine learning,\\ndeep learning tools are not prevalent within it. Deep learning poses several\\ndifficulties when used in an active learning setting. First, active learning\\n(AL) methods generally rely on being able to learn and update models from small\\namounts of data. Recent advances in deep learning, on the other hand, are\\nnotorious for their dependence on large amounts of data. Second, many AL\\nacquisition functions rely on model uncertainty, yet deep learning methods\\nrarely represent such model uncertainty. In this paper we combine recent\\nadvances in Bayesian deep learning into the active learning framework in a\\npractical way. We develop an active learning framework for high dimensional\\ndata, a task which has been extremely challenging so far, with very sparse\\nexisting literature. Taking advantage of specialised models such as Bayesian\\nconvolutional neural networks, we demonstrate our active learning techniques\\nwith image data, obtaining a significant improvement on existing active\\nlearning approaches. We demonstrate this on both the MNIST dataset, as well as\\nfor skin cancer diagnosis from lesion images (ISIC2016 task).\\n', \"  We describe DyNet, a toolkit for implementing neural network models based on\\ndynamic declaration of network structure. In the static declaration strategy\\nthat is used in toolkits like Theano, CNTK, and TensorFlow, the user first\\ndefines a computation graph (a symbolic representation of the computation), and\\nthen examples are fed into an engine that executes this computation and\\ncomputes its derivatives. In DyNet's dynamic declaration strategy, computation\\ngraph construction is mostly transparent, being implicitly constructed by\\nexecuting procedural code that computes the network outputs, and the user is\\nfree to use different network structures for each input. Dynamic declaration\\nthus facilitates the implementation of more complicated network architectures,\\nand DyNet is specifically designed to allow users to implement their models in\\na way that is idiomatic in their preferred programming language (C++ or\\nPython). One challenge with dynamic declaration is that because the symbolic\\ncomputation graph is defined anew for every training example, its construction\\nmust have low overhead. To achieve this, DyNet has an optimized C++ backend and\\nlightweight graph representation. Experiments show that DyNet's speeds are\\nfaster than or comparable with static declaration toolkits, and significantly\\nfaster than Chainer, another dynamic declaration toolkit. DyNet is released\\nopen-source under the Apache 2.0 license and available at\\nthis http URL.\\n\", '  The computer-aided analysis of medical scans is a longstanding goal in the\\nmedical imaging field. Currently, deep learning has became a dominant\\nmethodology for supporting pathologists and radiologist. Deep learning\\nalgorithms have been successfully applied to digital pathology and radiology,\\nnevertheless, there are still practical issues that prevent these tools to be\\nwidely used in practice. The main obstacles are low number of available cases\\nand large size of images (a.k.a. the small n, large p problem in machine\\nlearning), and a very limited access to annotation at a pixel level that can\\nlead to severe overfitting and large computational requirements. We propose to\\nhandle these issues by introducing a framework that processes a medical image\\nas a collection of small patches using a single, shared neural network. The\\nfinal diagnosis is provided by combining scores of individual patches using a\\npermutation-invariant operator (combination). In machine learning community\\nsuch approach is called a multi-instance learning (MIL).\\n', '  Conventional automatic speech recognition (ASR) typically performs\\nmulti-level pattern recognition tasks that map the acoustic speech waveform\\ninto a hierarchy of speech units. But, it is widely known that information loss\\nin the earlier stage can propagate through the later stages. After the\\nresurgence of deep learning, interest has emerged in the possibility of\\ndeveloping a purely end-to-end ASR system from the raw waveform to the\\ntranscription without any predefined alignments and hand-engineered models.\\nHowever, the successful attempts in end-to-end architecture still used\\nspectral-based features, while the successful attempts in using raw waveform\\nwere still based on the hybrid deep neural network - Hidden Markov model\\n(DNN-HMM) framework. In this paper, we construct the first end-to-end\\nattention-based encoder-decoder model to process directly from raw speech\\nwaveform to the text transcription. We called the model as \"Attention-based\\nWav2Text\". To assist the training process of the end-to-end model, we propose\\nto utilize a feature transfer learning. Experimental results also reveal that\\nthe proposed Attention-based Wav2Text model directly with raw waveform could\\nachieve a better result in comparison with the attentional encoder-decoder\\nmodel trained on standard front-end filterbank features.\\n', '  Machine understanding of complex images is a key goal of artificial\\nintelligence. One challenge underlying this task is that visual scenes contain\\nmultiple inter-related objects, and that global context plays an important role\\nin interpreting the scene. A natural modeling framework for capturing such\\neffects is structured prediction, which optimizes over complex labels, while\\nmodeling within-label interactions. However, it is unclear what principles\\nshould guide the design of a structured prediction model that utilizes the\\npower of deep learning components. Here we propose a design principle for such\\narchitectures that follows from a natural requirement of permutation\\ninvariance. We prove a necessary and sufficient characterization for\\narchitectures that follow this invariance, and discuss its implication on model\\ndesign. Finally, we show that the resulting model achieves new state of the art\\nresults on the Visual Genome scene graph labeling benchmark, outperforming all\\nrecent approaches.\\n', '  Follower count is a factor that quantifies the popularity of celebrities. It\\nis a reflection of their power, prestige and overall social reach. In this\\npaper we investigate whether the social connectivity or the language choice is\\nmore correlated to the future follower count of a celebrity. We collect data\\nabout tweets, retweets and mentions of 471 Indian celebrities with verified\\nTwitter accounts. We build two novel networks to approximate social\\nconnectivity of the celebrities. We study various structural properties of\\nthese two networks and observe their correlations with future follower counts.\\nIn parallel, we analyze the linguistic structure of the tweets (LIWC features,\\nsyntax and sentiment features and style and readability features) and observe\\nthe correlations of each of these with the future follower count of a\\ncelebrity. As a final step we use there features to classify a celebrity in a\\nspecific bucket of future follower count (HIGH, MID or LOW). We observe that\\nthe network features alone achieve an accuracy of 0.52 while the linguistic\\nfeatures alone achieve an accuracy of 0.69 grossly outperforming the network\\nfeatures. The network and linguistic features in conjunction produce an\\naccuracy of 0.76. We also discuss some final insights that we obtain from\\nfurther data analysis celebrities with larger follower counts post tweets that\\nhave (i) more words from friend and family LIWC categories, (ii) more positive\\nsentiment laden words, (iii) have better language constructs and are (iv) more\\nreadable.\\n', \"  As the intermediate level task connecting image captioning and object\\ndetection, visual relationship detection started to catch researchers'\\nattention because of its descriptive power and clear structure. It detects the\\nobjects and captures their pair-wise interactions with a\\nsubject-predicate-object triplet, e.g. person-ride-horse. In this paper, each\\nvisual relationship is considered as a phrase with three components. We\\nformulate the visual relationship detection as three inter-connected\\nrecognition problems and propose a Visual Phrase guided Convolutional Neural\\nNetwork (ViP-CNN) to address them simultaneously. In ViP-CNN, we present a\\nPhrase-guided Message Passing Structure (PMPS) to establish the connection\\namong relationship components and help the model consider the three problems\\njointly. Corresponding non-maximum suppression method and model training\\nstrategy are also proposed. Experimental results show that our ViP-CNN\\noutperforms the state-of-art method both in speed and accuracy. We further\\npretrain ViP-CNN on our cleansed Visual Genome Relationship dataset, which is\\nfound to perform better than the pretraining on the ImageNet for this task.\\n\", '  Advances in image processing and computer vision in the latest years have\\nbrought about the use of visual features in artwork recommendation. Recent\\nworks have shown that visual features obtained from pre-trained deep neural\\nnetworks (DNNs) perform very well for recommending digital art. Other recent\\nworks have shown that explicit visual features (EVF) based on attractiveness\\ncan perform well in preference prediction tasks, but no previous work has\\ncompared DNN features versus specific attractiveness-based visual features\\n(e.g. brightness, texture) in terms of recommendation performance. In this\\nwork, we study and compare the performance of DNN and EVF features for the\\npurpose of physical artwork recommendation using transactional data from\\nUGallery, an online store of physical paintings. In addition, we perform an\\nexploratory analysis to understand if DNN embedded features have some relation\\nwith certain EVF. Our results show that DNN features outperform EVF, that\\ncertain EVF features are more suited for physical artwork recommendation and,\\nfinally, we show evidence that certain neurons in the DNN might be partially\\nencoding visual features such as brightness, providing an opportunity for\\nexplaining recommendations based on visual neural models.\\n', '  This paper investigates a novel task of generating texture images from\\nperceptual descriptions. Previous work on texture generation focused on either\\nsynthesis from examples or generation from procedural models. Generating\\ntextures from perceptual attributes have not been well studied yet. Meanwhile,\\nperceptual attributes, such as directionality, regularity and roughness are\\nimportant factors for human observers to describe a texture. In this paper, we\\npropose a joint deep network model that combines adversarial training and\\nperceptual feature regression for texture generation, while only random noise\\nand user-defined perceptual attributes are required as input. In this model, a\\npreliminary trained convolutional neural network is essentially integrated with\\nthe adversarial framework, which can drive the generated textures to possess\\ngiven perceptual attributes. An important aspect of the proposed model is that,\\nif we change one of the input perceptual features, the corresponding appearance\\nof the generated textures will also be changed. We design several experiments\\nto validate the effectiveness of the proposed method. The results show that the\\nproposed method can produce high quality texture images with desired perceptual\\nproperties.\\n', '  While going deeper has been witnessed to improve the performance of\\nconvolutional neural networks (CNN), going smaller for CNN has received\\nincreasing attention recently due to its attractiveness for mobile/embedded\\napplications. It remains an active and important topic how to design a small\\nnetwork while retaining the performance of large and deep CNNs (e.g., Inception\\nNets, ResNets). Albeit there are already intensive studies on compressing the\\nsize of CNNs, the considerable drop of performance is still a key concern in\\nmany designs. This paper addresses this concern with several new contributions.\\nFirst, we propose a simple yet powerful method for compressing the size of deep\\nCNNs based on parameter binarization. The striking difference from most\\nprevious work on parameter binarization/quantization lies at different\\ntreatments of $1\\\\times 1$ convolutions and $k\\\\times k$ convolutions ($k>1$),\\nwhere we only binarize $k\\\\times k$ convolutions into binary patterns. The\\nresulting networks are referred to as pattern networks. By doing this, we show\\nthat previous deep CNNs such as GoogLeNet and Inception-type Nets can be\\ncompressed dramatically with marginal drop in performance. Second, in light of\\nthe different functionalities of $1\\\\times 1$ (data projection/transformation)\\nand $k\\\\times k$ convolutions (pattern extraction), we propose a new block\\nstructure codenamed the pattern residual block that adds transformed feature\\nmaps generated by $1\\\\times 1$ convolutions to the pattern feature maps\\ngenerated by $k\\\\times k$ convolutions, based on which we design a small network\\nwith $\\\\sim 1$ million parameters. Combining with our parameter binarization, we\\nachieve better performance on ImageNet than using similar sized networks\\nincluding recently released Google MobileNets.\\n', '  Deep learning (DL) has recently achieved tremendous success in a variety of\\ncutting-edge applications, e.g., image recognition, speech and natural language\\nprocessing, and autonomous driving. Besides the available big data and hardware\\nevolution, DL frameworks and platforms play a key role to catalyze the\\nresearch, development, and deployment of DL intelligent solutions. However, the\\ndifference in computation paradigm, architecture design and implementation of\\nexisting DL frameworks and platforms brings challenges for DL software\\ndevelopment, deployment, maintenance, and migration. Up to the present, it\\nstill lacks a comprehensive study on how current diverse DL frameworks and\\nplatforms influence the DL software development process.\\nIn this paper, we initiate the first step towards the investigation on how\\nexisting state-of-the-art DL frameworks (i.e., TensorFlow, Theano, and Torch)\\nand platforms (i.e., server/desktop, web, and mobile) support the DL software\\ndevelopment activities. We perform an in-depth and comparative evaluation on\\nmetrics such as learning accuracy, DL model size, robustness, and performance,\\non state-of-the-art DL frameworks across platforms using two popular datasets\\nMNIST and CIFAR-10. Our study reveals that existing DL frameworks still suffer\\nfrom compatibility issues, which becomes even more severe when it comes to\\ndifferent platforms. We pinpoint the current challenges and opportunities\\ntowards developing high quality and compatible DL systems. To ignite further\\ninvestigation along this direction to address urgent industrial demands of\\nintelligent solutions, we make all of our assembled feasible toolchain and\\ndataset publicly available.\\n', '  Temporal object detection has attracted significant attention, but most\\npopular detection methods can not leverage the rich temporal information in\\nvideos. Very recently, many different algorithms have been developed for video\\ndetection task, but real-time online approaches are frequently deficient. In\\nthis paper, based on attention mechanism and convolutional long short-term\\nmemory (ConvLSTM), we propose a temporal signal-shot detector (TSSD) for\\nreal-world detection. Distinct from previous methods, we take aim at temporally\\nintegrating pyramidal feature hierarchy using ConvLSTM, and design a novel\\nstructure including a low-level temporal unit as well as a high-level one\\n(HL-TU) for multi-scale feature maps. Moreover, we develop a creative temporal\\nanalysis unit, namely, attentional ConvLSTM (AC-LSTM), in which a temporal\\nattention module is specially tailored for background suppression and scale\\nsuppression while a ConvLSTM integrates attention-aware features through time.\\nAn association loss is designed for temporal coherence. Besides, online tubelet\\nanalysis (OTA) is exploited for identification. Finally, our method is\\nevaluated on ImageNet VID dataset and 2DMOT15 dataset. Extensive comparisons on\\nthe detection and tracking capability validate the superiority of the proposed\\napproach. Consequently, the developed TSSD-OTA is fairly faster and achieves an\\noverall competitive performance in terms of detection and tracking. The source\\ncode will be made available.\\n', '  Dynamic topic modeling facilitates the identification of topical trends over\\ntime in temporal collections of unstructured documents. We introduce a novel\\nunsupervised neural dynamic topic model named as Recurrent Neural\\nNetwork-Replicated Softmax Model (RNNRSM), where the discovered topics at each\\ntime influence the topic discovery in the subsequent time steps. We account for\\nthe temporal ordering of documents by explicitly modeling a joint distribution\\nof latent topical dependencies over time, using distributional estimators with\\ntemporal recurrent connections. Applying RNN-RSM to 19 years of articles on NLP\\nresearch, we demonstrate that compared to state-of-the art topic models, RNNRSM\\nshows better generalization, topic interpretation, evolution and trends. We\\nalso introduce a metric (named as SPAN) to quantify the capability of dynamic\\ntopic model to capture word evolution in topics over time.\\n', '  Recurrent Neural Networks (RNNs) with sophisticated units that implement a\\ngating mechanism have emerged as powerful technique for modeling sequential\\nsignals such as speech or electroencephalography (EEG). The latter is the focus\\non this paper. A significant big data resource, known as the TUH EEG Corpus\\n(TUEEG), has recently become available for EEG research, creating a unique\\nopportunity to evaluate these recurrent units on the task of seizure detection.\\nIn this study, we compare two types of recurrent units: long short-term memory\\nunits (LSTM) and gated recurrent units (GRU). These are evaluated using a state\\nof the art hybrid architecture that integrates Convolutional Neural Networks\\n(CNNs) with RNNs. We also investigate a variety of initialization methods and\\nshow that initialization is crucial since poorly initialized networks cannot be\\ntrained. Furthermore, we explore regularization of these convolutional gated\\nrecurrent networks to address the problem of overfitting. Our experiments\\nrevealed that convolutional LSTM networks can achieve significantly better\\nperformance than convolutional GRU networks. The convolutional LSTM\\narchitecture with proper initialization and regularization delivers 30%\\nsensitivity at 6 false alarms per 24 hours.\\n', '  This paper is concerned with paraphrase detection. The ability to detect\\nsimilar sentences written in natural language is crucial for several\\napplications, such as text mining, text summarization, plagiarism detection,\\nauthorship authentication and question answering. Given two sentences, the\\nobjective is to detect whether they are semantically identical. An important\\ninsight from this work is that existing paraphrase systems perform well when\\napplied on clean texts, but they do not necessarily deliver good performance\\nagainst noisy texts. Challenges with paraphrase detection on user generated\\nshort texts, such as Twitter, include language irregularity and noise. To cope\\nwith these challenges, we propose a novel deep neural network-based approach\\nthat relies on coarse-grained sentence modeling using a convolutional neural\\nnetwork and a long short-term memory model, combined with a specific\\nfine-grained word-level similarity matching model. Our experimental results\\nshow that the proposed approach outperforms existing state-of-the-art\\napproaches on user-generated noisy social media data, such as Twitter texts,\\nand achieves highly competitive performance on a cleaner corpus.\\n', '  Training automatic speech recognition (ASR) systems requires large amounts of\\ndata in the target language in order to achieve good performance. Whereas large\\ntraining corpora are readily available for languages like English, there exists\\na long tail of languages which do suffer from a lack of resources. One method\\nto handle data sparsity is to use data from additional source languages and\\nbuild a multilingual system. Recently, ASR systems based on recurrent neural\\nnetworks (RNNs) trained with connectionist temporal classification (CTC) have\\ngained substantial research interest. In this work, we extended our previous\\napproach towards training CTC-based systems multilingually. Our systems feature\\na global phone set, based on the joint phone sets of each source language. We\\nevaluated the use of different language combinations as well as the addition of\\nLanguage Feature Vectors (LFVs). As contrastive experiment, we built systems\\nbased on graphemes as well. Systems having a multilingual phone set are known\\nto suffer in performance compared to their monolingual counterparts. With our\\nproposed approach, we could reduce the gap between these mono- and multilingual\\nsetups, using either graphemes or phonemes.\\n', \"  We propose a novel hierarchical generative model with a simple Markovian\\nstructure and a corresponding inference model. Both the generative and\\ninference model are trained using the adversarial learning paradigm. We\\ndemonstrate that the hierarchical structure supports the learning of\\nprogressively more abstract representations as well as providing semantically\\nmeaningful reconstructions with different levels of fidelity. Furthermore, we\\nshow that minimizing the Jensen-Shanon divergence between the generative and\\ninference network is enough to minimize the reconstruction error. The resulting\\nsemantically meaningful hierarchical latent structure discovery is exemplified\\non the CelebA dataset. There, we show that the features learned by our model in\\nan unsupervised way outperform the best handcrafted features. Furthermore, the\\nextracted features remain competitive when compared to several recent deep\\nsupervised approaches on an attribute prediction task on CelebA. Finally, we\\nleverage the model's inference network to achieve state-of-the-art performance\\non a semi-supervised variant of the MNIST digit classification task.\\n\", '  University curriculum, both on a campus level and on a per-major level, are\\naffected in a complex way by many decisions of many administrators and faculty\\nover time. As universities across the United States share an urgency to\\nsignificantly improve student success and success retention, there is a\\npressing need to better understand how the student population is progressing\\nthrough the curriculum, and how to provide better supporting infrastructure and\\nrefine the curriculum for the purpose of improving student outcomes. This work\\nhas developed a visual knowledge discovery system called eCamp that pulls\\ntogether a variety of populationscale data products, including student grades,\\nmajor descriptions, and graduation records. These datasets were previously\\ndisconnected and only available to and maintained by independent campus\\noffices. The framework models and analyzes the multi-level relationships hidden\\nwithin these data products, and visualizes the student flow patterns through\\nindividual majors as well as through a hierarchy of majors. These results\\nsupport analytical tasks involving student outcomes, student retention, and\\ncurriculum design. It is shown how eCamp has revealed student progression\\ninformation that was previously unavailable.\\n', \"  Recent advances in neural word embedding provide significant benefit to\\nvarious information retrieval tasks. However as shown by recent studies,\\nadapting the embedding models for the needs of IR tasks can bring considerable\\nfurther improvements. The embedding models in general define the term\\nrelatedness by exploiting the terms' co-occurrences in short-window contexts.\\nAn alternative (and well-studied) approach in IR for related terms to a query\\nis using local information i.e. a set of top-retrieved documents. In view of\\nthese two methods of term relatedness, in this work, we report our study on\\nincorporating the local information of the query in the word embeddings. One\\nmain challenge in this direction is that the dense vectors of word embeddings\\nand their estimation of term-to-term relatedness remain difficult to interpret\\nand hard to analyze. As an alternative, explicit word representations propose\\nvectors whose dimensions are easily interpretable, and recent methods show\\ncompetitive performance to the dense vectors. We introduce a neural-based\\nexplicit representation, rooted in the conceptual ideas of the word2vec\\nSkip-Gram model. The method provides interpretable explicit vectors while\\nkeeping the effectiveness of the Skip-Gram model. The evaluation of various\\nexplicit representations on word association collections shows that the newly\\nproposed method out- performs the state-of-the-art explicit representations\\nwhen tasked with ranking highly similar terms. Based on the introduced ex-\\nplicit representation, we discuss our approaches on integrating local documents\\nin globally-trained embedding models and discuss the preliminary results.\\n\", '  Steady-State Visual Evoked Potentials (SSVEPs) are neural oscillations from\\nthe parietal and occipital regions of the brain that are evoked from flickering\\nvisual stimuli. SSVEPs are robust signals measurable in the\\nelectroencephalogram (EEG) and are commonly used in brain-computer interfaces\\n(BCIs). However, methods for high-accuracy decoding of SSVEPs usually require\\nhand-crafted approaches that leverage domain-specific knowledge of the stimulus\\nsignals, such as specific temporal frequencies in the visual stimuli and their\\nrelative spatial arrangement. When this knowledge is unavailable, such as when\\nSSVEP signals are acquired asynchronously, such approaches tend to fail. In\\nthis paper, we show how a compact convolutional neural network (Compact-CNN),\\nwhich only requires raw EEG signals for automatic feature extraction, can be\\nused to decode signals from a 12-class SSVEP dataset without the need for any\\ndomain-specific knowledge or calibration data. We report across subject mean\\naccuracy of approximately 80% (chance being 8.3%) and show this is\\nsubstantially better than current state-of-the-art hand-crafted approaches\\nusing canonical correlation analysis (CCA) and Combined-CCA. Furthermore, we\\nanalyze our Compact-CNN to examine the underlying feature representation,\\ndiscovering that the deep learner extracts additional phase and amplitude\\nrelated features associated with the structure of the dataset. We discuss how\\nour Compact-CNN shows promise for BCI applications that allow users to freely\\ngaze/attend to any stimulus at any time (e.g., asynchronous BCI) as well as\\nprovides a method for analyzing SSVEP signals in a way that might augment our\\nunderstanding about the basic processing in the visual cortex.\\n', \"  Understanding the dynamics of social interactions is crucial to comprehend\\nhuman behavior. The emergence of online social media has enabled access to data\\nregarding people relationships at a large scale. Twitter, specifically, is an\\ninformation oriented network, with users sharing and consuming information. In\\nthis work, we study whether users tend to be in contact with people interested\\nin similar topics, i.e., topical homophily. To do so, we propose an approach\\nbased on the use of hashtags to extract information topics from Twitter\\nmessages and model users' interests. Our results show that, on average, users\\nare connected with other users similar to them and stronger relationships are\\ndue to a higher topical similarity. Furthermore, we show that topical homophily\\nprovides interesting information that can eventually allow inferring users'\\nconnectivity. Our work, besides providing a way to assess the topical\\nsimilarity of users, quantifies topical homophily among individuals,\\ncontributing to a better understanding of how complex social systems are\\nstructured.\\n\", '  An electroencephalography (EEG) based Brain Computer Interface (BCI) enables\\npeople to communicate with the outside world by interpreting the EEG signals of\\ntheir brains to interact with devices such as wheelchairs and intelligent\\nrobots. More specifically, motor imagery EEG (MI-EEG), which reflects a\\nsubjects active intent, is attracting increasing attention for a variety of BCI\\napplications. Accurate classification of MI-EEG signals while essential for\\neffective operation of BCI systems, is challenging due to the significant noise\\ninherent in the signals and the lack of informative correlation between the\\nsignals and brain activities. In this paper, we propose a novel deep neural\\nnetwork based learning framework that affords perceptive insights into the\\nrelationship between the MI-EEG data and brain activities. We design a joint\\nconvolutional recurrent neural network that simultaneously learns robust\\nhigh-level feature presentations through low-dimensional dense embeddings from\\nraw MI-EEG signals. We also employ an Autoencoder layer to eliminate various\\nartifacts such as background activities. The proposed approach has been\\nevaluated extensively on a large- scale public MI-EEG dataset and a limited but\\neasy-to-deploy dataset collected in our lab. The results show that our approach\\noutperforms a series of baselines and the competitive state-of-the- art\\nmethods, yielding a classification accuracy of 95.53%. The applicability of our\\nproposed approach is further demonstrated with a practical BCI system for\\ntyping.\\n', '  The growing popularity of autonomous systems creates a need for reliable and\\nefficient metric pose retrieval algorithms. Currently used approaches tend to\\nrely on nearest neighbor search of binary descriptors to perform the 2D-3D\\nmatching and guarantee realtime capabilities on mobile platforms. These methods\\nstruggle, however, with the growing size of the map, changes in viewpoint or\\nappearance, and visual aliasing present in the environment. The rigidly defined\\ndescriptor patterns only capture a limited neighborhood of the keypoint and\\ncompletely ignore the overall visual context.\\nWe propose LandmarkBoost - an approach that, in contrast to the conventional\\n2D-3D matching methods, casts the search problem as a landmark classification\\ntask. We use a boosted classifier to classify landmark observations and\\ndirectly obtain correspondences as classifier scores. We also introduce a\\nformulation of visual context that is flexible, efficient to compute, and can\\ncapture relationships in the entire image plane. The original binary\\ndescriptors are augmented with contextual information and informative features\\nare selected by the boosting framework. Through detailed experiments, we\\nevaluate the retrieval quality and performance of LandmarkBoost, demonstrating\\nthat it outperforms common state-of-the-art descriptor matching methods.\\n', \"  Domain adaptation is crucial in many real-world applications where the\\ndistribution of the training data differs from the distribution of the test\\ndata. Previous Deep Learning-based approaches to domain adaptation need to be\\ntrained jointly on source and target domain data and are therefore unappealing\\nin scenarios where models need to be adapted to a large number of domains or\\nwhere a domain is evolving, e.g. spam detection where attackers continuously\\nchange their tactics.\\nTo fill this gap, we propose Knowledge Adaptation, an extension of Knowledge\\nDistillation (Bucilua et al., 2006; Hinton et al., 2015) to the domain\\nadaptation scenario. We show how a student model achieves state-of-the-art\\nresults on unsupervised domain adaptation from multiple sources on a standard\\nsentiment analysis benchmark by taking into account the domain-specific\\nexpertise of multiple teachers and the similarities between their domains.\\nWhen learning from a single teacher, using domain similarity to gauge\\ntrustworthiness is inadequate. To this end, we propose a simple metric that\\ncorrelates well with the teacher's accuracy in the target domain. We\\ndemonstrate that incorporating high-confidence examples selected by this metric\\nenables the student model to achieve state-of-the-art performance in the\\nsingle-source scenario.\\n\", '  The problem of automatically generating a computer program from some\\nspecification has been studied since the early days of AI. Recently, two\\ncompeting approaches for automatic program learning have received significant\\nattention: (1) neural program synthesis, where a neural network is conditioned\\non input/output (I/O) examples and learns to generate a program, and (2) neural\\nprogram induction, where a neural network generates new outputs directly using\\na latent program representation.\\nHere, for the first time, we directly compare both approaches on a\\nlarge-scale, real-world learning task. We additionally contrast to rule-based\\nprogram synthesis, which uses hand-crafted semantics to guide the program\\ngeneration. Our neural models use a modified attention RNN to allow encoding of\\nvariable-sized sets of I/O pairs. Our best synthesis model achieves 92%\\naccuracy on a real-world test set, compared to the 34% accuracy of the previous\\nbest neural synthesis approach. The synthesis model also outperforms a\\ncomparable induction model on this task, but we more importantly demonstrate\\nthat the strength of each approach is highly dependent on the evaluation metric\\nand end-user application. Finally, we show that we can train our neural models\\nto remain very robust to the type of noise expected in real-world data (e.g.,\\ntypos), while a highly-engineered rule-based system fails entirely.\\n', \"  Bots, social media accounts controlled by software rather than by humans,\\nhave recently been under the spotlight for their association with various forms\\nof online manipulation. To date, much work has focused on social bot detection,\\nbut little attention has been devoted to the characterization and measurement\\nof the behavior and activity of bots, as opposed to humans'. Over the course of\\nthe years, bots have become more sophisticated, and capable to reflect some\\nshort-term behavior, emulating that of human users. The goal of this paper is\\nto study the behavioral dynamics that bots exhibit over the course of one\\nactivity session, and highlight if and how these differ from human activity\\nsignatures. By using a large Twitter dataset associated with recent political\\nevents, we first separate bots and humans, then isolate their activity\\nsessions. We compile a list of quantities to be measured, like the propensity\\nof users to engage in social interactions or to produce content. Our analysis\\nhighlights the presence of short-term behavioral trends in humans, which can be\\nassociated with a cognitive origin, that are absent in bots, intuitively due to\\ntheir automated activity. These findings are finally codified to create and\\nevaluate a machine learning algorithm to detect activity sessions produced by\\nbots and humans, to allow for more nuanced bot detection strategies.\\n\", '  Text representations using neural word embeddings have proven effective in\\nmany NLP applications. Recent researches adapt the traditional word embedding\\nmodels to learn vectors of multiword expressions (concepts/entities). However,\\nthese methods are limited to textual knowledge bases (e.g., Wikipedia). In this\\npaper, we propose a novel and simple technique for integrating the knowledge\\nabout concepts from two large scale knowledge bases of different structure\\n(Wikipedia and Probase) in order to learn concept representations. We adapt the\\nefficient skip-gram model to seamlessly learn from the knowledge in Wikipedia\\ntext and Probase concept graph. We evaluate our concept embedding models on two\\ntasks: (1) analogical reasoning, where we achieve a state-of-the-art\\nperformance of 91% on semantic analogies, (2) concept categorization, where we\\nachieve a state-of-the-art performance on two benchmark datasets achieving\\ncategorization accuracy of 100% on one and 98% on the other. Additionally, we\\npresent a case study to evaluate our model on unsupervised argument type\\nidentification for neural semantic parsing. We demonstrate the competitive\\naccuracy of our unsupervised method and its ability to better generalize to out\\nof vocabulary entity mentions compared to the tedious and error prone methods\\nwhich depend on gazetteers and regular expressions.\\n', \"  Opinion polls have been the bridge between public opinion and politicians in\\nelections. However, developing surveys to disclose people's feedback with\\nrespect to economic issues is limited, expensive, and time-consuming. In recent\\nyears, social media such as Twitter has enabled people to share their opinions\\nregarding elections. Social media has provided a platform for collecting a\\nlarge amount of social media data. This paper proposes a computational public\\nopinion mining approach to explore the discussion of economic issues in social\\nmedia during an election. Current related studies use text mining methods\\nindependently for election analysis and election prediction; this research\\ncombines two text mining methods: sentiment analysis and topic modeling. The\\nproposed approach has effectively been deployed on millions of tweets to\\nanalyze economic concerns of people during the 2012 US presidential election.\\n\", '  This study addresses the problem of identifying the meaning of unknown words\\nor entities in a discourse with respect to the word embedding approaches used\\nin neural language models. We proposed a method for on-the-fly construction and\\nexploitation of word embeddings in both the input and output layers of a neural\\nmodel by tracking contexts. This extends the dynamic entity representation used\\nin Kobayashi et al. (2016) and incorporates a copy mechanism proposed\\nindependently by Gu et al. (2016) and Gulcehre et al. (2016). In addition, we\\nconstruct a new task and dataset called Anonymized Language Modeling for\\nevaluating the ability to capture word meanings while reading. Experiments\\nconducted using our novel dataset show that the proposed variant of RNN\\nlanguage model outperformed the baseline model. Furthermore, the experiments\\nalso demonstrate that dynamic updates of an output layer help a model predict\\nreappearing entities, whereas those of an input layer are effective to predict\\nwords following reappearing entities.\\n', '  With the spreading prevalence of Big Data, many advances have recently been\\nmade in this field. Frameworks such as Apache Hadoop and Apache Spark have\\ngained a lot of traction over the past decades and have become massively\\npopular, especially in industries. It is becoming increasingly evident that\\neffective big data analysis is key to solving artificial intelligence problems.\\nThus, a multi-algorithm library was implemented in the Spark framework, called\\nMLlib. While this library supports multiple machine learning algorithms, there\\nis still scope to use the Spark setup efficiently for highly time-intensive and\\ncomputationally expensive procedures like deep learning. In this paper, we\\npropose a novel framework that combines the distributive computational\\nabilities of Apache Spark and the advanced machine learning architecture of a\\ndeep multi-layer perceptron (MLP), using the popular concept of Cascade\\nLearning. We conduct empirical analysis of our framework on two real world\\ndatasets. The results are encouraging and corroborate our proposed framework,\\nin turn proving that it is an improvement over traditional big data analysis\\nmethods that use either Spark or Deep learning as individual elements.\\n', '  We investigate deep generative models that can exchange multiple modalities\\nbi-directionally, e.g., generating images from corresponding texts and vice\\nversa. A major approach to achieve this objective is to train a model that\\nintegrates all the information of different modalities into a joint\\nrepresentation and then to generate one modality from the corresponding other\\nmodality via this joint representation. We simply applied this approach to\\nvariational autoencoders (VAEs), which we call a joint multimodal variational\\nautoencoder (JMVAE). However, we found that when this model attempts to\\ngenerate a large dimensional modality missing at the input, the joint\\nrepresentation collapses and this modality cannot be generated successfully.\\nFurthermore, we confirmed that this difficulty cannot be resolved even using a\\nknown solution. Therefore, in this study, we propose two models to prevent this\\ndifficulty: JMVAE-kl and JMVAE-h. Results of our experiments demonstrate that\\nthese methods can prevent the difficulty above and that they generate\\nmodalities bi-directionally with equal or higher likelihood than conventional\\nVAE methods, which generate in only one direction. Moreover, we confirm that\\nthese methods can obtain the joint representation appropriately, so that they\\ncan generate various variations of modality by moving over the joint\\nrepresentation or changing the value of another modality.\\n', '  Over recent years, emerging interest has occurred in integrating computer\\nvision technology into the retail industry. Automatic checkout (ACO) is one of\\nthe critical problems in this area which aims to automatically generate the\\nshopping list from the images of the products to purchase. The main challenge\\nof this problem comes from the large scale and the fine-grained nature of the\\nproduct categories as well as the difficulty for collecting training images\\nthat reflect the realistic checkout scenarios due to continuous update of the\\nproducts. Despite its significant practical and research value, this problem is\\nnot extensively studied in the computer vision community, largely due to the\\nlack of a high-quality dataset. To fill this gap, in this work we propose a new\\ndataset to facilitate relevant research. Our dataset enjoys the following\\ncharacteristics: (1) It is by far the largest dataset in terms of both product\\nimage quantity and product categories. (2) It includes single-product images\\ntaken in a controlled environment and multi-product images taken by the\\ncheckout system. (3) It provides different levels of annotations for the\\ncheck-out images. Comparing with the existing datasets, ours is closer to the\\nrealistic setting and can derive a variety of research problems. Besides the\\ndataset, we also benchmark the performance on this dataset with various\\napproaches. The dataset and related resources can be found at\\n\\\\url{this https URL}.\\n', '  We propose a new sentence simplification task (Split-and-Rephrase) where the\\naim is to split a complex sentence into a meaning preserving sequence of\\nshorter sentences. Like sentence simplification, splitting-and-rephrasing has\\nthe potential of benefiting both natural language processing and societal\\napplications. Because shorter sentences are generally better processed by NLP\\nsystems, it could be used as a preprocessing step which facilitates and\\nimproves the performance of parsers, semantic role labellers and machine\\ntranslation systems. It should also be of use for people with reading\\ndisabilities because it allows the conversion of longer sentences into shorter\\nones. This paper makes two contributions towards this new task. First, we\\ncreate and make available a benchmark consisting of 1,066,115 tuples mapping a\\nsingle complex sentence to a sequence of sentences expressing the same meaning.\\nSecond, we propose five models (vanilla sequence-to-sequence to\\nsemantically-motivated models) to understand the difficulty of the proposed\\ntask.\\n', '  We present a novel approach for the prediction of anticancer compound\\nsensitivity by means of multi-modal attention-based neural networks (PaccMann).\\nIn our approach, we integrate three key pillars of drug sensitivity, namely,\\nthe molecular structure of compounds, transcriptomic profiles of cancer cells\\nas well as prior knowledge about interactions among proteins within cells. Our\\nmodels ingest a drug-cell pair consisting of SMILES encoding of a compound and\\nthe gene expression profile of a cancer cell and predicts an IC50 sensitivity\\nvalue. Gene expression profiles are encoded using an attention-based encoding\\nmechanism that assigns high weights to the most informative genes. We present\\nand study three encoders for SMILES string of compounds: 1) bidirectional\\nrecurrent 2) convolutional 3) attention-based encoders. We compare our devised\\nmodels against a baseline model that ingests engineered fingerprints to\\nrepresent the molecular structure. We demonstrate that using our\\nattention-based encoders, we can surpass the baseline model. The use of\\nattention-based encoders enhance interpretability and enable us to identify\\ngenes, bonds and atoms that were used by the network to make a prediction.\\n', '  Large-scale variations still pose a challenge in unconstrained face\\ndetection. To the best of our knowledge, no current face detection algorithm\\ncan detect a face as large as 800 x 800 pixels while simultaneously detecting\\nanother one as small as 8 x 8 pixels within a single image with equally high\\naccuracy. We propose a two-stage cascaded face detection framework, Multi-Path\\nRegion-based Convolutional Neural Network (MP-RCNN), that seamlessly combines a\\ndeep neural network with a classic learning strategy, to tackle this challenge.\\nThe first stage is a Multi-Path Region Proposal Network (MP-RPN) that proposes\\nfaces at three different scales. It simultaneously utilizes three parallel\\noutputs of the convolutional feature maps to predict multi-scale candidate face\\nregions. The \"atrous\" convolution trick (convolution with up-sampled filters)\\nand a newly proposed sampling layer for \"hard\" examples are embedded in MP-RPN\\nto further boost its performance. The second stage is a Boosted Forests\\nclassifier, which utilizes deep facial features pooled from inside the\\ncandidate face regions as well as deep contextual features pooled from a larger\\nregion surrounding the candidate face regions. This step is included to further\\nremove hard negative samples. Experiments show that this approach achieves\\nstate-of-the-art face detection performance on the WIDER FACE dataset \"hard\"\\npartition, outperforming the former best result by 9.6% for the Average\\nPrecision.\\n', '  Knights Landing (KNL) is the code name for the second-generation Intel Xeon\\nPhi product family. KNL has generated significant interest in the data analysis\\nand machine learning communities because its new many-core architecture targets\\nboth of these workloads. The KNL many-core vector processor design enables it\\nto exploit much higher levels of parallelism. At the Lincoln Laboratory\\nSupercomputing Center (LLSC), the majority of users are running data analysis\\napplications such as MATLAB and Octave. More recently, machine learning\\napplications, such as the UC Berkeley Caffe deep learning framework, have\\nbecome increasingly important to LLSC users. Thus, the performance of these\\napplications on KNL systems is of high interest to LLSC users and the broader\\ndata analysis and machine learning communities. Our data analysis benchmarks of\\nthese application on the Intel KNL processor indicate that single-core\\ndouble-precision generalized matrix multiply (DGEMM) performance on KNL systems\\nhas improved by ~3.5x compared to prior Intel Xeon technologies. Our data\\nanalysis applications also achieved ~60% of the theoretical peak performance.\\nAlso a performance comparison of a machine learning application, Caffe, between\\nthe two different Intel CPUs, Xeon E5 v3 and Xeon Phi 7210, demonstrated a 2.7x\\nimprovement on a KNL node.\\n', '  The rise and fall of artificial neural networks is well documented in the\\nscientific literature of both computer science and computational chemistry. Yet\\nalmost two decades later, we are now seeing a resurgence of interest in deep\\nlearning, a machine learning algorithm based on multilayer neural networks.\\nWithin the last few years, we have seen the transformative impact of deep\\nlearning in many domains, particularly in speech recognition and computer\\nvision, to the extent that the majority of expert practitioners in those field\\nare now regularly eschewing prior established models in favor of deep learning\\nmodels. In this review, we provide an introductory overview into the theory of\\ndeep neural networks and their unique properties that distinguish them from\\ntraditional machine learning algorithms used in cheminformatics. By providing\\nan overview of the variety of emerging applications of deep neural networks, we\\nhighlight its ubiquity and broad applicability to a wide range of challenges in\\nthe field, including QSAR, virtual screening, protein structure prediction,\\nquantum chemistry, materials design and property prediction. In reviewing the\\nperformance of deep neural networks, we observed a consistent outperformance\\nagainst non-neural networks state-of-the-art models across disparate research\\ntopics, and deep neural network based models often exceeded the \"glass ceiling\"\\nexpectations of their respective tasks. Coupled with the maturity of\\nGPU-accelerated computing for training deep neural networks and the exponential\\ngrowth of chemical data on which to train these networks on, we anticipate that\\ndeep learning algorithms will be a valuable tool for computational chemistry.\\n', '  The availability of large idea repositories (e.g., the U.S. patent database)\\ncould significantly accelerate innovation and discovery by providing people\\nwith inspiration from solutions to analogous problems. However, finding useful\\nanalogies in these large, messy, real-world repositories remains a persistent\\nchallenge for either human or automated methods. Previous approaches include\\ncostly hand-created databases that have high relational structure (e.g.,\\npredicate calculus representations) but are very sparse. Simpler\\nmachine-learning/information-retrieval similarity metrics can scale to large,\\nnatural-language datasets, but struggle to account for structural similarity,\\nwhich is central to analogy. In this paper we explore the viability and value\\nof learning simpler structural representations, specifically, \"problem\\nschemas\", which specify the purpose of a product and the mechanisms by which it\\nachieves that purpose. Our approach combines crowdsourcing and recurrent neural\\nnetworks to extract purpose and mechanism vector representations from product\\ndescriptions. We demonstrate that these learned vectors allow us to find\\nanalogies with higher precision and recall than traditional\\ninformation-retrieval methods. In an ideation experiment, analogies retrieved\\nby our models significantly increased people\\'s likelihood of generating\\ncreative ideas compared to analogies retrieved by traditional methods. Our\\nresults suggest a promising approach to enabling computational analogy at scale\\nis to learn and leverage weaker structural representations.\\n', '  Deep Neural Networks (DNNs) and Convolutional Neural Networks (CNNs) are\\nuseful for many practical tasks in machine learning. Synaptic weights, as well\\nas neuron activation functions within the deep network are typically stored\\nwith high-precision formats, e.g. 32 bit floating point. However, since storage\\ncapacity is limited and each memory access consumes power, both storage\\ncapacity and memory access are two crucial factors in these networks. Here we\\npresent a method and present the ADaPTION toolbox to extend the popular deep\\nlearning library Caffe to support training of deep CNNs with reduced numerical\\nprecision of weights and activations using fixed point notation. ADaPTION\\nincludes tools to measure the dynamic range of weights and activations. Using\\nthe ADaPTION tools, we quantized several CNNs including VGG16 down to 16-bit\\nweights and activations with only 0.8% drop in Top-1 accuracy. The\\nquantization, especially of the activations, leads to increase of up to 50% of\\nsparsity especially in early and intermediate layers, which we exploit to skip\\nmultiplications with zero, thus performing faster and computationally cheaper\\ninference.\\n', \"  We present graph attention networks (GATs), novel neural network\\narchitectures that operate on graph-structured data, leveraging masked\\nself-attentional layers to address the shortcomings of prior methods based on\\ngraph convolutions or their approximations. By stacking layers in which nodes\\nare able to attend over their neighborhoods' features, we enable (implicitly)\\nspecifying different weights to different nodes in a neighborhood, without\\nrequiring any kind of costly matrix operation (such as inversion) or depending\\non knowing the graph structure upfront. In this way, we address several key\\nchallenges of spectral-based graph neural networks simultaneously, and make our\\nmodel readily applicable to inductive as well as transductive problems. Our GAT\\nmodels have achieved or matched state-of-the-art results across four\\nestablished transductive and inductive graph benchmarks: the Cora, Citeseer and\\nPubmed citation network datasets, as well as a protein-protein interaction\\ndataset (wherein test graphs remain unseen during training).\\n\", '  Machine learning and deep learning in particular has advanced tremendously on\\nperceptual tasks in recent years. However, it remains vulnerable against\\nadversarial perturbations of the input that have been crafted specifically to\\nfool the system while being quasi-imperceptible to a human. In this work, we\\npropose to augment deep neural networks with a small \"detector\" subnetwork\\nwhich is trained on the binary classification task of distinguishing genuine\\ndata from data containing adversarial perturbations. Our method is orthogonal\\nto prior work on addressing adversarial perturbations, which has mostly focused\\non making the classification network itself more robust. We show empirically\\nthat adversarial perturbations can be detected surprisingly well even though\\nthey are quasi-imperceptible to humans. Moreover, while the detectors have been\\ntrained to detect only a specific adversary, they generalize to similar and\\nweaker adversaries. In addition, we propose an adversarial attack that fools\\nboth the classifier and the detector and a novel training procedure for the\\ndetector that counteracts this attack.\\n', '  Our work presented in this paper focuses on the translation of terminological\\nexpressions represented in semantically structured resources, like ontologies\\nor knowledge graphs. The challenge of translating ontology labels or\\nterminological expressions represented in knowledge bases lies in the highly\\nspecific vocabulary and the lack of contextual information, which can guide a\\nmachine translation system to translate ambiguous words into the targeted\\ndomain. Due to these challenges, we evaluate the translation quality of\\ndomain-specific expressions in the medical and financial domain with\\nstatistical (SMT) as well as with neural machine translation (NMT) methods and\\nexperiment domain adaptation of the translation models with terminological\\nexpressions only. Furthermore, we perform experiments on the injection of\\nexternal terminological expressions into the translation systems. Through these\\nexperiments, we observed a significant advantage in domain adaptation for the\\ndomain-specific resource in the medical and financial domain and the benefit of\\nsubword models over word-based NMT models for terminology translation.\\n', '  We explore methods of producing adversarial examples on deep generative\\nmodels such as the variational autoencoder (VAE) and the VAE-GAN. Deep learning\\narchitectures are known to be vulnerable to adversarial examples, but previous\\nwork has focused on the application of adversarial examples to classification\\ntasks. Deep generative models have recently become popular due to their ability\\nto model input data distributions and generate realistic examples from those\\ndistributions. We present three classes of attacks on the VAE and VAE-GAN\\narchitectures and demonstrate them against networks trained on MNIST, SVHN and\\nCelebA. Our first attack leverages classification-based adversaries by\\nattaching a classifier to the trained encoder of the target generative model,\\nwhich can then be used to indirectly manipulate the latent representation. Our\\nsecond attack directly uses the VAE loss function to generate a target\\nreconstruction image from the adversarial example. Our third attack moves\\nbeyond relying on classification or the standard loss for the gradient and\\ndirectly optimizes against differences in source and target latent\\nrepresentations. We also motivate why an attacker might be interested in\\ndeploying such techniques against a target generative network.\\n', '  GPUs and other accelerators are popular devices for accelerating\\ncompute-intensive, parallelizable applications. However, programming these\\ndevices is a difficult task. Writing efficient device code is challenging, and\\nis typically done in a low-level programming language. High-level languages are\\nrarely supported, or do not integrate with the rest of the high-level language\\necosystem. To overcome this, we propose compiler infrastructure to efficiently\\nadd support for new hardware or environments to an existing programming\\nlanguage.\\nWe evaluate our approach by adding support for NVIDIA GPUs to the Julia\\nprogramming language. By integrating with the existing compiler, we\\nsignificantly lower the cost to implement and maintain the new compiler, and\\nfacilitate reuse of existing application code. Moreover, use of the high-level\\nJulia programming language enables new and dynamic approaches for GPU\\nprogramming. This greatly improves programmer productivity, while maintaining\\napplication performance similar to that of the official NVIDIA CUDA toolkit.\\n', '  End-to-end (E2E) systems have achieved competitive results compared to\\nconventional hybrid hidden Markov model (HMM)-deep neural network based\\nautomatic speech recognition (ASR) systems. Such E2E systems are attractive due\\nto the lack of dependence on alignments between input acoustic and output\\ngrapheme or HMM state sequence during training. This paper explores the design\\nof an ASR-free end-to-end system for text query-based keyword search (KWS) from\\nspeech trained with minimal supervision. Our E2E KWS system consists of three\\nsub-systems. The first sub-system is a recurrent neural network (RNN)-based\\nacoustic auto-encoder trained to reconstruct the audio through a\\nfinite-dimensional representation. The second sub-system is a character-level\\nRNN language model using embeddings learned from a convolutional neural\\nnetwork. Since the acoustic and text query embeddings occupy different\\nrepresentation spaces, they are input to a third feed-forward neural network\\nthat predicts whether the query occurs in the acoustic utterance or not. This\\nE2E ASR-free KWS system performs respectably despite lacking a conventional ASR\\nsystem and trains much faster.\\n', '  One of the major drawbacks of modularized task-completion dialogue systems is\\nthat each module is trained individually, which presents several challenges.\\nFor example, downstream modules are affected by earlier modules, and the\\nperformance of the entire system is not robust to the accumulated errors. This\\npaper presents a novel end-to-end learning framework for task-completion\\ndialogue systems to tackle such issues. Our neural dialogue system can directly\\ninteract with a structured database to assist users in accessing information\\nand accomplishing certain tasks. The reinforcement learning based dialogue\\nmanager offers robust capabilities to handle noises caused by other components\\nof the dialogue system. Our experiments in a movie-ticket booking domain show\\nthat our end-to-end system not only outperforms modularized dialogue system\\nbaselines for both objective and subjective evaluation, but also is robust to\\nnoises as demonstrated by several systematic experiments with different error\\ngranularity and rates specific to the language understanding module.\\n', '  It remains a challenge to efficiently extract spatialtemporal information\\nfrom skeleton sequences for 3D human action recognition. Although most recent\\naction recognition methods are based on Recurrent Neural Networks which present\\noutstanding performance, one of the shortcomings of these methods is the\\ntendency to overemphasize the temporal information. Since 3D convolutional\\nneural network(3D CNN) is a powerful tool to simultaneously learn features from\\nboth spatial and temporal dimensions through capturing the correlations between\\nthree dimensional signals, this paper proposes a novel two-stream model using\\n3D CNN. To our best knowledge, this is the first application of 3D CNN in\\nskeleton-based action recognition. Our method consists of three stages. First,\\nskeleton joints are mapped into a 3D coordinate space and then encoding the\\nspatial and temporal information, respectively. Second, 3D CNN models are\\nseperately adopted to extract deep features from two streams. Third, to enhance\\nthe ability of deep features to capture global relationships, we extend every\\nstream into multitemporal version. Extensive experiments on the SmartHome\\ndataset and the large-scale NTU RGB-D dataset demonstrate that our method\\noutperforms most of RNN-based methods, which verify the complementary property\\nbetween spatial and temporal information and the robustness to noise.\\n', '  Machine learning approaches hold great potential for the automated detection\\nof lung nodules in chest radiographs, but training the algorithms requires vary\\nlarge amounts of manually annotated images, which are difficult to obtain. Weak\\nlabels indicating whether a radiograph is likely to contain pulmonary nodules\\nare typically easier to obtain at scale by parsing historical free-text\\nradiological reports associated to the radiographs. Using a repositotory of\\nover 700,000 chest radiographs, in this study we demonstrate that promising\\nnodule detection performance can be achieved using weak labels through\\nconvolutional neural networks for radiograph classification. We propose two\\nnetwork architectures for the classification of images likely to contain\\npulmonary nodules using both weak labels and manually-delineated bounding\\nboxes, when these are available. Annotated nodules are used at training time to\\ndeliver a visual attention mechanism informing the model about its localisation\\nperformance. The first architecture extracts saliency maps from high-level\\nconvolutional layers and compares the estimated position of a nodule against\\nthe ground truth, when this is available. A corresponding localisation error is\\nthen back-propagated along with the softmax classification error. The second\\napproach consists of a recurrent attention model that learns to observe a short\\nsequence of smaller image portions through reinforcement learning. When a\\nnodule annotation is available at training time, the reward function is\\nmodified accordingly so that exploring portions of the radiographs away from a\\nnodule incurs a larger penalty. Our empirical results demonstrate the potential\\nadvantages of these architectures in comparison to competing methodologies.\\n', '  Understanding protein function is one of the keys to understanding life at\\nthe molecular level. It is also important in several scenarios including human\\ndisease and drug discovery. In this age of rapid and affordable biological\\nsequencing, the number of sequences accumulating in databases is rising with an\\nincreasing rate. This presents many challenges for biologists and computer\\nscientists alike. In order to make sense of this huge quantity of data, these\\nsequences should be annotated with functional properties. UniProtKB consists of\\ntwo components: i) the UniProtKB/Swiss-Prot database containing protein\\nsequences with reliable information manually reviewed by expert bio-curators\\nand ii) the UniProtKB/TrEMBL database that is used for storing and processing\\nthe unknown sequences. Hence, for all proteins we have available the sequence\\nalong with few more information such as the taxon and some structural domains.\\nPairwise similarity can be defined and computed on proteins based on such\\nattributes. Other important attributes, while present for proteins in\\nSwiss-Prot, are often missing for proteins in TrEMBL, such as their function\\nand cellular localization. The enormous number of protein sequences now in\\nTrEMBL calls for rapid procedures to annotate them automatically. In this work,\\nwe present DistNBLP, a novel Distributed Neighborhood-Based Label Propagation\\napproach for large-scale annotation of proteins. To do this, the functional\\nannotations of reviewed proteins are used to predict those of non-reviewed\\nproteins using label propagation on a graph representation of the protein\\ndatabase. DistNBLP is built on top of the \"akka\" toolkit for building resilient\\ndistributed message-driven applications.\\n', '  Currently, third-generation sequencing techniques, which allow to obtain much\\nlonger DNA reads compared to the next-generation sequencing technologies, are\\nbecoming more and more popular. There are many possibilities to combine data\\nfrom next-generation and third-generation sequencing.\\nHerein, we present a new application called dnaasm-link for linking contigs,\\na result of \\\\textit{de novo} assembly of second-generation sequencing data,\\nwith long DNA reads. Our tool includes an integrated module to fill gaps with a\\nsuitable fragment of appropriate long DNA read, which improves the consistency\\nof the resulting DNA sequences. This feature is very important, in particular\\nfor complex DNA regions, as presented in the paper. Finally, our implementation\\noutperforms other state-of-the-art tools in terms of speed and memory\\nrequirements, which may enable the usage of the presented application for\\norganisms with a large genome, which is not possible in~existing applications.\\nThe presented application has many advantages as (i) significant memory\\noptimization and reduction of computation time (ii) filling the gaps through\\nthe appropriate fragment of a specified long DNA read (iii) reducing number of\\nspanned and unspanned gaps in the existing genome drafts.\\nThe application is freely available to all users under GNU Library or Lesser\\nGeneral Public License version 3.0 (LGPLv3). The demo application, docker image\\nand source code are available at this http URL.\\n', '  This paper proposes ReBNet, an end-to-end framework for training\\nreconfigurable binary neural networks on software and developing efficient\\naccelerators for execution on FPGA. Binary neural networks offer an intriguing\\nopportunity for deploying large-scale deep learning models on\\nresource-constrained devices. Binarization reduces the memory footprint and\\nreplaces the power-hungry matrix-multiplication with light-weight XnorPopcount\\noperations. However, binary networks suffer from a degraded accuracy compared\\nto their fixed-point counterparts. We show that the state-of-the-art methods\\nfor optimizing binary networks accuracy, significantly increase the\\nimplementation cost and complexity. To compensate for the degraded accuracy\\nwhile adhering to the simplicity of binary networks, we devise the first\\nreconfigurable scheme that can adjust the classification accuracy based on the\\napplication. Our proposition improves the classification accuracy by\\nrepresenting features with multiple levels of residual binarization. Unlike\\nprevious methods, our approach does not exacerbate the area cost of the\\nhardware accelerator. Instead, it provides a tradeoff between throughput and\\naccuracy while the area overhead of multi-level binarization is negligible.\\n', \"  Compared with artificial neural networks (ANNs), spiking neural networks\\n(SNNs) are promising to explore the brain-like behaviors since the spikes could\\nencode more spatio-temporal information. Although pre-training from ANN or\\ndirect training based on backpropagation (BP) makes the supervised training of\\nSNNs possible, these methods only exploit the networks' spatial domain\\ninformation which leads to the performance bottleneck and requires many\\ncomplicated training skills. Another fundamental issue is that the spike\\nactivity is naturally non-differentiable which causes great difficulties in\\ntraining SNNs. To this end, we build an iterative LIF model that is more\\nfriendly for gradient descent training. By simultaneously considering the\\nlayer-by-layer spatial domain (SD) and the timing-dependent temporal domain\\n(TD) in the training phase, as well as an approximated derivative for the spike\\nactivity, we propose a spatio-temporal backpropagation (STBP) training\\nframework without using any complicated technology. We achieve the best\\nperformance of multi-layered perceptron (MLP) compared with existing\\nstate-of-the-art algorithms over the static MNIST and the dynamic N-MNIST\\ndataset as well as a custom object detection dataset. This work provides a new\\nperspective to explore the high-performance SNNs for future brain-like\\ncomputing paradigm with rich spatio-temporal dynamics.\\n\", '  The recent advances in deep neural networks (DNNs) make them attractive for\\nembedded systems. However, it can take a long time for DNNs to make an\\ninference on resource-constrained computing devices. Model compression\\ntechniques can address the computation issue of deep inference on embedded\\ndevices. This technique is highly attractive, as it does not rely on\\nspecialized hardware, or computation-offloading that is often infeasible due to\\nprivacy concerns or high latency. However, it remains unclear how model\\ncompression techniques perform across a wide range of DNNs. To design efficient\\nembedded deep learning solutions, we need to understand their behaviors. This\\nwork develops a quantitative approach to characterize model compression\\ntechniques on a representative embedded deep learning architecture, the NVIDIA\\nJetson Tx2. We perform extensive experiments by considering 11 influential\\nneural network architectures from the image classification and the natural\\nlanguage processing domains. We experimentally show that how two mainstream\\ncompression techniques, data quantization and pruning, perform on these network\\narchitectures and the implications of compression techniques to the model\\nstorage size, inference time, energy consumption and performance metrics. We\\ndemonstrate that there are opportunities to achieve fast deep inference on\\nembedded systems, but one must carefully choose the compression settings. Our\\nresults provide insights on when and how to apply model compression techniques\\nand guidelines for designing efficient embedded deep learning systems.\\n', '  Face detection methods have relied on face datasets for training. However,\\nexisting face datasets tend to be in small scales for face learning in both\\nconstrained and unconstrained environments. In this paper, we first introduce\\nour large-scale image datasets, Large-scale Labeled Face (LSLF) and noisy\\nLarge-scale Labeled Non-face (LSLNF). Our LSLF dataset consists of a large\\nnumber of unconstrained multi-view and partially occluded faces. The faces have\\nmany variations in color and grayscale, image quality, image resolution, image\\nillumination, image background, image illusion, human face, cartoon face,\\nfacial expression, light and severe partial facial occlusion, make up, gender,\\nage, and race. Many of these faces are partially occluded with accessories such\\nas tattoos, hats, glasses, sunglasses, hands, hair, beards, scarves,\\nmicrophones, or other objects or persons. The LSLF dataset is currently the\\nlargest labeled face image dataset in the literature in terms of the number of\\nlabeled images and the number of individuals compared to other existing labeled\\nface image datasets. Second, we introduce our CrowedFaces and CrowedNonFaces\\nimage datasets. The crowedFaces and CrowedNonFaces datasets include faces and\\nnon-faces images from crowed scenes. These datasets essentially aim for\\nresearchers to provide a large number of training examples with many variations\\nfor large scale face learning and face recognition tasks.\\n', '  Sequence-to-sequence models provide a simple and elegant solution for\\nbuilding speech recognition systems by folding separate components of a typical\\nsystem, namely acoustic (AM), pronunciation (PM) and language (LM) models into\\na single neural network. In this work, we look at one such sequence-to-sequence\\nmodel, namely listen, attend and spell (LAS), and explore the possibility of\\ntraining a single model to serve different English dialects, which simplifies\\nthe process of training multi-dialect systems without the need for separate AM,\\nPM and LMs for each dialect. We show that simply pooling the data from all\\ndialects into one LAS model falls behind the performance of a model fine-tuned\\non each dialect. We then look at incorporating dialect-specific information\\ninto the model, both by modifying the training targets by inserting the dialect\\nsymbol at the end of the original grapheme sequence and also feeding a 1-hot\\nrepresentation of the dialect information into all layers of the model.\\nExperimental results on seven English dialects show that our proposed system is\\neffective in modeling dialect variations within a single LAS model,\\noutperforming a LAS model trained individually on each of the seven dialects by\\n3.1 ~ 16.5% relative.\\n', '  Accurate real time crime prediction is a fundamental issue for public safety,\\nbut remains a challenging problem for the scientific community. Crime\\noccurrences depend on many complex factors. Compared to many predictable\\nevents, crime is sparse. At different spatio-temporal scales, crime\\ndistributions display dramatically different patterns. These distributions are\\nof very low regularity in both space and time. In this work, we adapt the\\nstate-of-the-art deep learning spatio-temporal predictor, ST-ResNet [Zhang et\\nal, AAAI, 2017], to collectively predict crime distribution over the Los\\nAngeles area. Our models are two staged. First, we preprocess the raw crime\\ndata. This includes regularization in both space and time to enhance\\npredictable signals. Second, we adapt hierarchical structures of residual\\nconvolutional units to train multi-factor crime prediction models. Experiments\\nover a half year period in Los Angeles reveal highly accurate predictive power\\nof our models.\\n', '  Creating and modeling real-world graphs is a crucial problem in various\\napplications of engineering, biology, and social sciences; however, learning\\nthe distributions of nodes/edges and sampling from them to generate realistic\\ngraphs is still challenging. Moreover, generating a diverse set of synthetic\\ngraphs that all imitate a real network is not addressed. In this paper, the\\nnovel problem of creating diverse synthetic graphs is solved. First, we devise\\nthe deep supervised subset selection (DeepS3) algorithm; Given a ground-truth\\nset of data points, DeepS3 selects a diverse subset of all items (i.e. data\\npoints) that best represent the items in the ground-truth set. Furthermore, we\\npropose the deep graph representation recurrent network (GRRN) as a novel\\ngenerative model that learns a probabilistic representation of a real weighted\\ngraph. Training the GRRN, we generate a large set of synthetic graphs that are\\nlikely to follow the same features and adjacency patterns as the original one.\\nIncorporating GRRN with DeepS3, we select a diverse subset of generated graphs\\nthat best represent the behaviors of the real graph (i.e. our ground-truth). We\\napply our model to the novel problem of power grid synthesis, where a synthetic\\npower network is created with the same physical/geometric properties as a real\\npower system without revealing the real locations of the substations (nodes)\\nand the lines (edges), since such data is confidential. Experiments on the\\nSynthetic Power Grid Data Set show accurate synthetic networks that follow\\nsimilar structural and spatial properties as the real power grid.\\n', '  Chatbots are one class of intelligent, conversational software agents\\nactivated by natural language input (which can be in the form of text, voice,\\nor both). They provide conversational output in response, and if commanded, can\\nsometimes also execute tasks. Although chatbot technologies have existed since\\nthe 1960s and have influenced user interface development in games since the\\nearly 1980s, chatbots are now easier to train and implement. This is due to\\nplentiful open source code, widely available development platforms, and\\nimplementation options via Software as a Service (SaaS). In addition to\\nenhancing customer experiences and supporting learning, chatbots can also be\\nused to engineer social harm - that is, to spread rumors and misinformation, or\\nattack people for posting their thoughts and opinions online. This paper\\npresents a literature review of quality issues and attributes as they relate to\\nthe contemporary issue of chatbot development and implementation. Finally,\\nquality assessment approaches are reviewed, and a quality assessment method\\nbased on these attributes and the Analytic Hierarchy Process (AHP) is proposed\\nand examined.\\n', '  Tissue characterization has long been an important component of Computer\\nAided Diagnosis (CAD) systems for automatic lesion detection and further\\nclinical planning. Motivated by the superior performance of deep learning\\nmethods on various computer vision problems, there has been increasing work\\napplying deep learning to medical image analysis. However, the development of a\\nrobust and reliable deep learning model for computer-aided diagnosis is still\\nhighly challenging due to the combination of the high heterogeneity in the\\nmedical images and the relative lack of training samples. Specifically,\\nannotation and labeling of the medical images is much more expensive and\\ntime-consuming than other applications and often involves manual labor from\\nmultiple domain experts. In this work, we propose a multi-stage, self-paced\\nlearning framework utilizing a convolutional neural network (CNN) to classify\\nComputed Tomography (CT) image patches. The key contribution of this approach\\nis that we augment the size of training samples by refining the unlabeled\\ninstances with a self-paced learning CNN. By implementing the framework on high\\nperformance computing servers including the NVIDIA DGX1 machine, we obtained\\nthe experimental result, showing that the self-pace boosted network\\nconsistently outperformed the original network even with very scarce manual\\nlabels. The performance gain indicates that applications with limited training\\nsamples such as medical image analysis can benefit from using the proposed\\nframework.\\n', '  Generating versatile and appropriate synthetic speech requires control over\\nthe output expression separate from the spoken text. Important non-textual\\nspeech variation is seldom annotated, in which case output control must be\\nlearned in an unsupervised fashion. In this paper, we perform an in-depth study\\nof methods for unsupervised learning of control in statistical speech\\nsynthesis. For example, we show that popular unsupervised training heuristics\\ncan be interpreted as variational inference in certain autoencoder models. We\\nadditionally connect these models to VQ-VAEs, another, recently-proposed class\\nof deep variational autoencoders, which we show can be derived from a very\\nsimilar mathematical argument. The implications of these new probabilistic\\ninterpretations are discussed. We illustrate the utility of the various\\napproaches with an application to acoustic modelling for emotional speech\\nsynthesis, where the unsupervised methods for learning expression control\\n(without access to emotional labels) are found to give results that in many\\naspects match or surpass the previous best supervised approach.\\n', '  We present a co-segmentation technique for space-time co-located image\\ncollections. These prevalent collections capture various dynamic events,\\nusually by multiple photographers, and may contain multiple co-occurring\\nobjects which are not necessarily part of the intended foreground object,\\nresulting in ambiguities for traditional co-segmentation techniques. Thus, to\\ndisambiguate what the common foreground object is, we introduce a\\nweakly-supervised technique, where we assume only a small seed, given in the\\nform of a single segmented image. We take a distributed approach, where local\\nbelief models are propagated and reinforced with similar images. Our technique\\nprogressively expands the foreground and background belief models across the\\nentire collection. The technique exploits the power of the entire set of image\\nwithout building a global model, and thus successfully overcomes large\\nvariability in appearance of the common foreground object. We demonstrate that\\nour method outperforms previous co-segmentation techniques on challenging\\nspace-time co-located collections, including dense benchmark datasets which\\nwere adapted for our novel problem setting.\\n', '  For many years, i-vector based audio embedding techniques were the dominant\\napproach for speaker verification and speaker diarization applications.\\nHowever, mirroring the rise of deep learning in various domains, neural network\\nbased audio embeddings, also known as d-vectors, have consistently demonstrated\\nsuperior speaker verification performance. In this paper, we build on the\\nsuccess of d-vector based speaker verification systems to develop a new\\nd-vector based approach to speaker diarization. Specifically, we combine\\nLSTM-based d-vector audio embeddings with recent work in non-parametric\\nclustering to obtain a state-of-the-art speaker diarization system. Our system\\nis evaluated on three standard public datasets, suggesting that d-vector based\\ndiarization systems offer significant advantages over traditional i-vector\\nbased systems. We achieved a 12.0% diarization error rate on NIST SRE 2000\\nCALLHOME, while our model is trained with out-of-domain data from voice search\\nlogs.\\n', '  Amino acid sequence portrays most intrinsic form of a protein and expresses\\nprimary structure of protein. The order of amino acids in a sequence enables a\\nprotein to acquire a particular stable conformation that is responsible for the\\nfunctions of the protein. This relationship between a sequence and its function\\nmotivates the need to analyse the sequences for predicting protein functions.\\nEarly generation computational methods using BLAST, FASTA, etc. perform\\nfunction transfer based on sequence similarity with existing databases and are\\ncomputationally slow. Although machine learning based approaches are fast, they\\nfail to perform well for long protein sequences (i.e., protein sequences with\\nmore than 300 amino acid residues). In this paper, we introduce a novel method\\nfor construction of two separate feature sets for protein sequences based on\\nanalysis of 1) single fixed-sized segments and 2) multi-sized segments, using\\nbi-directional long short-term memory network. Further, model based on proposed\\nfeature set is combined with the state of the art Multi-lable Linear\\nDiscriminant Analysis (MLDA) features based model to improve the accuracy.\\nExtensive evaluations using separate datasets for biological processes and\\nmolecular functions demonstrate promising results for both single-sized and\\nmulti-sized segments based feature sets. While former showed an improvement of\\n+3.37% and +5.48%, the latter produces an improvement of +5.38% and +8.00%\\nrespectively for two datasets over the state of the art MLDA based classifier.\\nAfter combining two models, there is a significant improvement of +7.41% and\\n+9.21% respectively for two datasets compared to MLDA based classifier.\\nSpecifically, the proposed approach performed well for the long protein\\nsequences and superior overall performance.\\n', \"  Lesion segmentation is the first step in most automatic melanoma recognition\\nsystems. Deficiencies and difficulties in dermoscopic images such as color\\ninconstancy, hair occlusion, dark corners and color charts make lesion\\nsegmentation an intricate task. In order to detect the lesion in the presence\\nof these problems, we propose a supervised saliency detection method tailored\\nfor dermoscopic images based on the discriminative regional feature integration\\n(DRFI). DRFI method incorporates multi-level segmentation, regional contrast,\\nproperty, background descriptors, and a random forest regressor to create\\nsaliency scores for each region in the image. In our improved saliency\\ndetection method, mDRFI, we have added some new features to regional property\\ndescriptors. Also, in order to achieve more robust regional background\\ndescriptors, a thresholding algorithm is proposed to obtain a new\\npseudo-background region. Findings reveal that mDRFI is superior to DRFI in\\ndetecting the lesion as the salient object in dermoscopic images. The proposed\\noverall lesion segmentation framework uses detected saliency map to construct\\nan initial mask of the lesion through thresholding and post-processing\\noperations. The initial mask is then evolving in a level set framework to fit\\nbetter on the lesion's boundaries. The results of evaluation tests on three\\npublic datasets show that our proposed segmentation method outperforms the\\nother conventional state-of-the-art segmentation algorithms and its performance\\nis comparable with most recent approaches that are based on deep convolutional\\nneural networks.\\n\", '  In recent years, supervised learning using Convolutional Neural Networks\\n(CNNs) has achieved great success in image classification tasks, and large\\nscale labeled datasets have contributed significantly to this achievement.\\nHowever, the definition of a label is often application dependent. For example,\\nan image of a cat can be labeled as \"cat\" or perhaps more specifically \"Persian\\ncat.\" We refer to this as label granularity. In this paper, we conduct\\nextensive experiments using various datasets to demonstrate and analyze how and\\nwhy training based on fine-grain labeling, such as \"Persian cat\" can improve\\nCNN accuracy on classifying coarse-grain classes, in this case \"cat.\" The\\nexperimental results show that training CNNs with fine-grain labels improves\\nboth network\\'s optimization and generalization capabilities, as intuitively it\\nencourages the network to learn more features, and hence increases\\nclassification accuracy on coarse-grain classes under all datasets considered.\\nMoreover, fine-grain labels enhance data efficiency in CNN training. For\\nexample, a CNN trained with fine-grain labels and only 40% of the total\\ntraining data can achieve higher accuracy than a CNN trained with the full\\ntraining dataset and coarse-grain labels. These results point to two possible\\napplications of this work: (i) with sufficient human resources, one can improve\\nCNN performance by re-labeling the dataset with fine-grain labels, and (ii)\\nwith limited human resources, to improve CNN performance, rather than\\ncollecting more training data, one may instead use fine-grain labels for the\\ndataset. We further propose a metric called Average Confusion Ratio to\\ncharacterize the effectiveness of fine-grain labeling, and show its use through\\nextensive experimentation. Code is available at\\nthis https URL.\\n', '  We create and release the first publicly available commercial customer\\nservice corpus with annotated relational segments. Human-computer data from\\nthree live customer service Intelligent Virtual Agents (IVAs) in the domains of\\ntravel and telecommunications were collected, and reviewers marked all text\\nthat was deemed unnecessary to the determination of user intention. After\\nmerging the selections of multiple reviewers to create highlighted texts, a\\nsecond round of annotation was done to determine the classes of language\\npresent in the highlighted sections such as the presence of Greetings,\\nBackstory, Justification, Gratitude, Rants, or Emotions. This resulting corpus\\nis a valuable resource for improving the quality and relational abilities of\\nIVAs. As well as discussing the corpus itself, we compare the usage of such\\nlanguage in human-human interactions on TripAdvisor forums. We show that\\nremoval of this language from task-based inputs has a positive effect on IVA\\nunderstanding by both an increase in confidence and improvement in responses,\\ndemonstrating the need for automated methods of its discovery.\\n', '  This paper investigates how far a very deep neural network is from attaining\\nclose to saturating performance on existing 2D and 3D face alignment datasets.\\nTo this end, we make the following 5 contributions: (a) we construct, for the\\nfirst time, a very strong baseline by combining a state-of-the-art architecture\\nfor landmark localization with a state-of-the-art residual block, train it on a\\nvery large yet synthetically expanded 2D facial landmark dataset and finally\\nevaluate it on all other 2D facial landmark datasets. (b) We create a guided by\\n2D landmarks network which converts 2D landmark annotations to 3D and unifies\\nall existing datasets, leading to the creation of LS3D-W, the largest and most\\nchallenging 3D facial landmark dataset to date ~230,000 images. (c) Following\\nthat, we train a neural network for 3D face alignment and evaluate it on the\\nnewly introduced LS3D-W. (d) We further look into the effect of all\\n\"traditional\" factors affecting face alignment performance like large pose,\\ninitialization and resolution, and introduce a \"new\" one, namely the size of\\nthe network. (e) We show that both 2D and 3D face alignment networks achieve\\nperformance of remarkable accuracy which is probably close to saturating the\\ndatasets used. Training and testing code as well as the dataset can be\\ndownloaded from this https URL\\n', '  In this paper, drawing intuition from the Turing test, we propose using\\nadversarial training for open-domain dialogue generation: the system is trained\\nto produce sequences that are indistinguishable from human-generated dialogue\\nutterances. We cast the task as a reinforcement learning (RL) problem where we\\njointly train two systems, a generative model to produce response sequences,\\nand a discriminator---analagous to the human evaluator in the Turing test--- to\\ndistinguish between the human-generated dialogues and the machine-generated\\nones. The outputs from the discriminator are then used as rewards for the\\ngenerative model, pushing the system to generate dialogues that mostly resemble\\nhuman dialogues.\\nIn addition to adversarial training we describe a model for adversarial {\\\\em\\nevaluation} that uses success in fooling an adversary as a dialogue evaluation\\nmetric, while avoiding a number of potential pitfalls. Experimental results on\\nseveral metrics, including adversarial evaluation, demonstrate that the\\nadversarially-trained system generates higher-quality responses than previous\\nbaselines.\\n', '  For decades, context-dependent phonemes have been the dominant sub-word unit\\nfor conventional acoustic modeling systems. This status quo has begun to be\\nchallenged recently by end-to-end models which seek to combine acoustic,\\npronunciation, and language model components into a single neural network. Such\\nsystems, which typically predict graphemes or words, simplify the recognition\\nprocess since they remove the need for a separate expert-curated pronunciation\\nlexicon to map from phoneme-based units to words. However, there has been\\nlittle previous work comparing phoneme-based versus grapheme-based sub-word\\nunits in the end-to-end modeling framework, to determine whether the gains from\\nsuch approaches are primarily due to the new probabilistic model, or from the\\njoint learning of the various components with grapheme-based units.\\nIn this work, we conduct detailed experiments which are aimed at quantifying\\nthe value of phoneme-based pronunciation lexica in the context of end-to-end\\nmodels. We examine phoneme-based end-to-end models, which are contrasted\\nagainst grapheme-based ones on a large vocabulary English Voice-search task,\\nwhere we find that graphemes do indeed outperform phonemes. We also compare\\ngrapheme and phoneme-based approaches on a multi-dialect English task, which\\nonce again confirm the superiority of graphemes, greatly simplifying the system\\nfor recognizing multiple dialects.\\n', '  Music creation is typically composed of two parts: composing the musical\\nscore, and then performing the score with instruments to make sounds. While\\nrecent work has made much progress in automatic music generation in the\\nsymbolic domain, few attempts have been made to build an AI model that can\\nrender realistic music audio from musical scores. Directly synthesizing audio\\nwith sound sample libraries often leads to mechanical and deadpan results,\\nsince musical scores do not contain performance-level information, such as\\nsubtle changes in timing and dynamics. Moreover, while the task may sound like\\na text-to-speech synthesis problem, there are fundamental differences since\\nmusic audio has rich polyphonic sounds. To build such an AI performer, we\\npropose in this paper a deep convolutional model that learns in an end-to-end\\nmanner the score-to-audio mapping between a symbolic representation of music\\ncalled the piano rolls and an audio representation of music called the\\nspectrograms. The model consists of two subnets: the ContourNet, which uses a\\nU-Net structure to learn the correspondence between piano rolls and\\nspectrograms and to give an initial result; and the TextureNet, which further\\nuses a multi-band residual network to refine the result by adding the spectral\\ntexture of overtones and timbre. We train the model to generate music clips of\\nthe violin, cello, and flute, with a dataset of moderate size. We also present\\nthe result of a user study that shows our model achieves higher mean opinion\\nscore (MOS) in naturalness and emotional expressivity than a WaveNet-based\\nmodel and two commercial sound libraries. We open our source code at\\nthis https URL\\n', '  We propose a novel denoising framework for task functional Magnetic Resonance\\nImaging (tfMRI) data to delineate the high-resolution spatial pattern of the\\nbrain functional connectivity via dictionary learning and sparse coding (DLSC).\\nIn order to address the limitations of the unsupervised DLSC-based fMRI\\nstudies, we utilize the prior knowledge of task paradigm in the learning step\\nto train a data-driven dictionary and to model the sparse representation. We\\napply the proposed DLSC-based method to Human Connectome Project (HCP) motor\\ntfMRI dataset. Studies on the functional connectivity of cerebrocerebellar\\ncircuits in somatomotor networks show that the DLSC-based denoising framework\\ncan significantly improve the prominent connectivity patterns, in comparison to\\nthe temporal non-local means (tNLM)-based denoising method as well as the case\\nwithout denoising, which is consistent and neuroscientifically meaningful\\nwithin motor area. The promising results show that the proposed method can\\nprovide an important foundation for the high-resolution functional connectivity\\nanalysis, and provide a better approach for fMRI preprocessing.\\n', '  The context of this research is testing and building software systems and,\\nspecifically, software language repositories (SLRs), i.e., repositories with\\ncomponents for language processing (interpreters, translators, analyzers,\\ntransformers, pretty printers, etc.). SLRs are typically set up for developing\\nand using metaprogramming systems, language workbenches, language definition\\nframeworks, executable semantic frameworks, and modeling frameworks. This work\\nis an inquiry into testing and building SLRs in a manner that the repository is\\nseen as a collection of language-typed artifacts being related by the\\napplications of language-typed functions or relations which serve language\\nprocessing. The notion of language is used in a broad sense to include text-,\\ntree-, graph-based languages as well as representations based on interchange\\nformats and also proprietary formats for serialization. The overall approach\\nunderlying this research is one of language design driven by a complex case\\nstudy, i.e., a specific SLR with a significant number of processed languages\\nand language processors as well as a noteworthy heterogeneity in terms of\\nrepresentation types and implementation languages. The knowledge gained by our\\nresearch is best understood as a declarative language design for regression\\ntesting and build management, we introduce a corresponding language Ueber with\\nan executable semantics which maintains relationships between language-typed\\nartifacts in an SLR. The grounding of the reported research is based on the\\ncomprehensive, formal, executable (logic programming-based) definition of the\\nUeber language and its systematic application to the management of the SLR YAS\\nwhich consists of hundreds of language definition and processing components\\n(such as interpreters and transformations) for more than thirty languages (not\\ncounting different representation types) with Prolog, Haskell, Java, and Python\\nbeing used as implementation languages. The importance of this work follows\\nfrom the significant costs implied by regression testing and build management\\nand also from the complexity of SLRs which calls for means to help with\\nunderstanding.\\n', '  Modern search techniques either cannot efficiently incorporate human feedback\\nto refine search results or to express structural or semantic properties of\\ndesired code. The key insight of our interactive code search technique ALICE is\\nthat user feedback could be actively incorporated to allow users to easily\\nexpress and refine search queries. We design a query language to model the\\nstructure and semantics of code as logic facts. Given a code example with user\\nannotations, ALICE automatically extracts a logic query from features that are\\ntagged as important. Users can refine the search query by labeling one or more\\nexamples as desired (positive) or irrelevant (negative). ALICE then infers a\\nnew logic query that separates the positives from negative examples via active\\ninductive logic programming. Our comprehensive and systematic simulation\\nexperiment shows that ALICE removes a large number of false positives quickly\\nby actively incorporating user feedback. Its search algorithm is also robust to\\nnoise and user labeling mistakes. Our choice of leveraging both positive and\\nnegative examples and the nested containment structure of selected code is\\neffective in refining search queries. Compared with an existing technique,\\nCritics, ALICE does not require a user to manually construct a search pattern\\nand yet achieves comparable precision and recall with fewer search iterations\\non average. A case study with users shows that ALICE is easy to use and helps\\nexpress complex code patterns.\\n', '  A variety of real-world processes (over networks) produce sequences of data\\nwhose complex temporal dynamics need to be studied. More especially, the event\\ntimestamps can carry important information about the underlying network\\ndynamics, which otherwise are not available from the time-series evenly sampled\\nfrom continuous signals. Moreover, in most complex processes, event sequences\\nand evenly-sampled times series data can interact with each other, which\\nrenders joint modeling of those two sources of data necessary. To tackle the\\nabove problems, in this paper, we utilize the rich framework of (temporal)\\npoint processes to model event data and timely update its intensity function by\\nthe synergic twin Recurrent Neural Networks (RNNs). In the proposed\\narchitecture, the intensity function is synergistically modulated by one RNN\\nwith asynchronous events as input and another RNN with time series as input.\\nFurthermore, to enhance the interpretability of the model, the attention\\nmechanism for the neural point process is introduced. The whole model with\\nevent type and timestamp prediction output layers can be trained end-to-end and\\nallows a black-box treatment for modeling the intensity. We substantiate the\\nsuperiority of our model in synthetic data and three real-world benchmark\\ndatasets.\\n', \"  We examine the relationship between social structure and sentiment through\\nthe analysis of a large collection of tweets about the Irish Marriage\\nReferendum of 2015. We obtain the sentiment of every tweet with the hashtags\\n#marref and #marriageref that was posted in the days leading to the referendum,\\nand construct networks to aggregate sentiment and use it to study the\\ninteractions among users. Our results show that the sentiment of mention tweets\\nposted by users is correlated with the sentiment of received mentions, and\\nthere are significantly more connections between users with similar sentiment\\nscores than among users with opposite scores in the mention and follower\\nnetworks. We combine the community structure of the two networks with the\\nactivity level of the users and sentiment scores to find groups of users who\\nsupport voting `yes' or `no' in the referendum. There were numerous\\nconversations between users on opposing sides of the debate in the absence of\\nfollower connections, which suggests that there were efforts by some users to\\nestablish dialogue and debate across ideological divisions. Our analysis shows\\nthat social structure can be integrated successfully with sentiment to analyse\\nand understand the disposition of social media users. These results have\\npotential applications in the integration of data and meta-data to study\\nopinion dynamics, public opinion modelling, and polling.\\n\", '  Word embeddings are representations of individual words of a text document in\\na vector space and they are often use- ful for performing natural language pro-\\ncessing tasks. Current state of the art al- gorithms for learning word\\nembeddings learn vector representations from large corpora of text documents in\\nan unsu- pervised fashion. This paper introduces SWESA (Supervised Word\\nEmbeddings for Sentiment Analysis), an algorithm for sentiment analysis via\\nword embeddings. SWESA leverages document label infor- mation to learn vector\\nrepresentations of words from a modest corpus of text doc- uments by solving an\\noptimization prob- lem that minimizes a cost function with respect to both word\\nembeddings as well as classification accuracy. Analysis re- veals that SWESA\\nprovides an efficient way of estimating the dimension of the word embeddings\\nthat are to be learned. Experiments on several real world data sets show that\\nSWESA has superior per- formance when compared to previously suggested\\napproaches to word embeddings and sentiment analysis tasks.\\n', '  Automatic sleep staging is a challenging problem and state-of-the-art\\nalgorithms have not yet reached satisfactory performance to be used instead of\\nmanual scoring by a sleep technician. Much research has been done to find good\\nfeature representations that extract the useful information to correctly\\nclassify each epoch into the correct sleep stage. While many useful features\\nhave been discovered, the amount of features have grown to an extent that a\\nfeature reduction step is necessary in order to avoid the curse of\\ndimensionality. One reason for the need of such a large feature set is that\\nmany features are good for discriminating only one of the sleep stages and are\\nless informative during other stages. This paper explores how a second feature\\nrepresentation over a large set of pre-defined features can be learned using an\\nauto-encoder with a selective attention for the current sleep stage in the\\ntraining batch. This selective attention allows the model to learn feature\\nrepresentations that focuses on the more relevant inputs without having to\\nperform any dimensionality reduction of the input data. The performance of the\\nproposed algorithm is evaluated on a large data set of polysomnography (PSG)\\nnight recordings of patients with sleep-disordered breathing. The performance\\nof the auto-encoder with selective attention is compared with a regular\\nauto-encoder and previous works using a deep belief network (DBN).\\n', '  Heterogeneous information networks (HINs) are ubiquitous in real-world\\napplications. Due to the heterogeneity in HINs, the typed edges may not fully\\nalign with each other. In order to capture the semantic subtlety, we propose\\nthe concept of aspects with each aspect being a unit representing one\\nunderlying semantic facet. Meanwhile, network embedding has emerged as a\\npowerful method for learning network representation, where the learned\\nembedding can be used as features in various downstream applications.\\nTherefore, we are motivated to propose a novel embedding learning\\nframework---AspEm---to preserve the semantic information in HINs based on\\nmultiple aspects. Instead of preserving information of the network in one\\nsemantic space, AspEm encapsulates information regarding each aspect\\nindividually. In order to select aspects for embedding purpose, we further\\ndevise a solution for AspEm based on dataset-wide statistics. To corroborate\\nthe efficacy of AspEm, we conducted experiments on two real-words datasets with\\ntwo types of applications---classification and link prediction. Experiment\\nresults demonstrate that AspEm can outperform baseline network embedding\\nlearning methods by considering multiple aspects, where the aspects can be\\nselected from the given HIN in an unsupervised manner.\\n', '  We consider the task of identifying attitudes towards a given set of entities\\nfrom text. Conventionally, this task is decomposed into two separate subtasks:\\ntarget detection that identifies whether each entity is mentioned in the text,\\neither explicitly or implicitly, and polarity classification that classifies\\nthe exact sentiment towards an identified entity (the target) into positive,\\nnegative, or neutral.\\nInstead, we show that attitude identification can be solved with an\\nend-to-end machine learning architecture, in which the two subtasks are\\ninterleaved by a deep memory network. In this way, signals produced in target\\ndetection provide clues for polarity classification, and reversely, the\\npredicted polarity provides feedback to the identification of targets.\\nMoreover, the treatments for the set of targets also influence each other --\\nthe learned representations may share the same semantics for some targets but\\nvary for others. The proposed deep memory network, the AttNet, outperforms\\nmethods that do not consider the interactions between the subtasks or those\\namong the targets, including conventional machine learning methods and the\\nstate-of-the-art deep learning models.\\n', \"  We present a hybrid neural network and rule-based system that generates pop\\nmusic. Music produced by pure rule-based systems often sounds mechanical. Music\\nproduced by machine learning sounds better, but still lacks hierarchical\\ntemporal structure. We restore temporal hierarchy by augmenting machine\\nlearning with a temporal production grammar, which generates the music's\\noverall structure and chord progressions. A compatible melody is then generated\\nby a conditional variational recurrent autoencoder. The autoencoder is trained\\nwith eight-measure segments from a corpus of 10,000 MIDI files, each of which\\nhas had its melody track and chord progressions identified heuristically. The\\nautoencoder maps melody into a multi-dimensional feature space, conditioned by\\nthe underlying chord progression. A melody is then generated by feeding a\\nrandom sample from that space to the autoencoder's decoder, along with the\\nchord progression generated by the grammar. The autoencoder can make musically\\nplausible variations on an existing melody, suitable for recurring motifs. It\\ncan also reharmonize a melody to a new chord progression, keeping the rhythm\\nand contour. The generated music compares favorably with that generated by\\nother academic and commercial software designed for the music-as-a-service\\nindustry.\\n\", \"  Learning with auxiliary tasks has been shown to improve the generalisation of\\na primary task. However, this comes at the cost of manually-labelling\\nadditional tasks which may, or may not, be useful for the primary task. We\\npropose a new method which automatically learns labels for an auxiliary task,\\nsuch that any supervised learning task can be improved without requiring access\\nto additional data. The approach is to train two neural networks: a\\nlabel-generation network to predict the auxiliary labels, and a multi-task\\nnetwork to train the primary task alongside the auxiliary task. The loss for\\nthe label-generation network incorporates the multi-task network's performance,\\nand so this interaction between the two networks can be seen as a form of meta\\nlearning. We show that our proposed method, Meta AuXiliary Learning (MAXL),\\noutperforms single-task learning on 7 image datasets by a significant margin,\\nwithout requiring additional auxiliary labels. We also show that MAXL\\noutperforms several other baselines for generating auxiliary labels, and is\\neven competitive when compared with human-defined auxiliary labels. The\\nself-supervised nature of our method leads to a promising new direction towards\\nautomated generalisation. The source code is available at\\n\\\\url{this https URL}.\\n\", '  Developers spend a significant amount of time searching for code: e.g., to\\nunderstand how to complete, correct, or adapt their own code for a new context.\\nUnfortunately, the state of the art in code search has not evolved much beyond\\ntext search over tokenized source. Code has much richer structure and semantics\\nthan normal text, and this property can be exploited to specialize the\\ncode-search process for better querying, searching, and ranking of code-search\\nresults.\\nWe present a new code-search engine named Source Forager. Given a query in\\nthe form of a C/C++ function, Source Forager searches a pre-populated code\\ndatabase for similar C/C++ functions. Source Forager preprocesses the database\\nto extract a variety of simple code features that capture different aspects of\\ncode. A search returns the $k$ functions in the database that are most similar\\nto the query, based on the various extracted code features.\\nWe tested the usefulness of Source Forager using a variety of code-search\\nqueries from two domains. Our experiments show that the ranked results returned\\nby Source Forager are accurate, and that query-relevant functions can be\\nreliably retrieved even when searching through a large code database that\\ncontains very few query-relevant functions.\\nWe believe that Source Forager is a first step towards much-needed tools that\\nprovide a better code-search experience.\\n', '  The task of multi-label learning is to predict a set of relevant labels for\\nthe unseen instance. Traditional multi-label learning algorithms treat each\\nclass label as a logical indicator of whether the corresponding label is\\nrelevant or irrelevant to the instance, i.e., +1 represents relevant to the\\ninstance and -1 represents irrelevant to the instance. Such label represented\\nby -1 or +1 is called logical label. Logical label cannot reflect different\\nlabel importance. However, for real-world multi-label learning problems, the\\nimportance of each possible label is generally different. For the real\\napplications, it is difficult to obtain the label importance information\\ndirectly. Thus we need a method to reconstruct the essential label importance\\nfrom the logical multilabel data. To solve this problem, we assume that each\\nmulti-label instance is described by a vector of latent real-valued labels,\\nwhich can reflect the importance of the corresponding labels. Such label is\\ncalled numerical label. The process of reconstructing the numerical labels from\\nthe logical multi-label data via utilizing the logical label information and\\nthe topological structure in the feature space is called Label Enhancement. In\\nthis paper, we propose a novel multi-label learning framework called LEMLL,\\ni.e., Label Enhanced Multi-Label Learning, which incorporates regression of the\\nnumerical labels and label enhancement into a unified framework. Extensive\\ncomparative studies validate that the performance of multi-label learning can\\nbe improved significantly with label enhancement and LEMLL can effectively\\nreconstruct latent label importance information from logical multi-label data.\\n', '  Supervised object detection and semantic segmentation require object or even\\npixel level annotations. When there exist image level labels only, it is\\nchallenging for weakly supervised algorithms to achieve accurate predictions.\\nThe accuracy achieved by top weakly supervised algorithms is still\\nsignificantly lower than their fully supervised counterparts. In this paper, we\\npropose a novel weakly supervised curriculum learning pipeline for multi-label\\nobject recognition, detection and semantic segmentation. In this pipeline, we\\nfirst obtain intermediate object localization and pixel labeling results for\\nthe training images, and then use such results to train task-specific deep\\nnetworks in a fully supervised manner. The entire process consists of four\\nstages, including object localization in the training images, filtering and\\nfusing object instances, pixel labeling for the training images, and\\ntask-specific network training. To obtain clean object instances in the\\ntraining images, we propose a novel algorithm for filtering, fusing and\\nclassifying object instances collected from multiple solution mechanisms. In\\nthis algorithm, we incorporate both metric learning and density-based\\nclustering to filter detected object instances. Experiments show that our\\nweakly supervised pipeline achieves state-of-the-art results in multi-label\\nimage classification as well as weakly supervised object detection and very\\ncompetitive results in weakly supervised semantic segmentation on MS-COCO,\\nPASCAL VOC 2007 and PASCAL VOC 2012.\\n', \"  Requirements elicitation requires extensive knowledge and deep understanding\\nof the problem domain where the final system will be situated. However, in many\\nsoftware development projects, analysts are required to elicit the requirements\\nfrom an unfamiliar domain, which often causes communication barriers between\\nanalysts and stakeholders. In this paper, we propose a requirements ELICitation\\nAid tool (ELICA) to help analysts better understand the target application\\ndomain by dynamic extraction and labeling of requirements-relevant knowledge.\\nTo extract the relevant terms, we leverage the flexibility and power of\\nWeighted Finite State Transducers (WFSTs) in dynamic modeling of natural\\nlanguage processing tasks. In addition to the information conveyed through\\ntext, ELICA captures and processes non-linguistic information about the\\nintention of speakers such as their confidence level, analytical tone, and\\nemotions. The extracted information is made available to the analysts as a set\\nof labeled snippets with highlighted relevant terms which can also be exported\\nas an artifact of the Requirements Engineering (RE) process. The application\\nand usefulness of ELICA are demonstrated through a case study. This study shows\\nhow pre-existing relevant information about the application domain and the\\ninformation captured during an elicitation meeting, such as the conversation\\nand stakeholders' intentions, can be captured and used to support analysts\\nachieving their tasks.\\n\", '  As high-throughput biological sequencing becomes faster and cheaper, the need\\nto extract useful information from sequencing becomes ever more paramount,\\noften limited by low-throughput experimental characterizations. For proteins,\\naccurate prediction of their functions directly from their primary amino-acid\\nsequences has been a long standing challenge. Here, machine learning using\\nartificial recurrent neural networks (RNN) was applied towards classification\\nof protein function directly from primary sequence without sequence alignment,\\nheuristic scoring or feature engineering. The RNN models containing\\nlong-short-term-memory (LSTM) units trained on public, annotated datasets from\\nUniProt achieved high performance for in-class prediction of four important\\nprotein functions tested, particularly compared to other machine learning\\nalgorithms using sequence-derived protein features. RNN models were used also\\nfor out-of-class predictions of phylogenetically distinct protein families with\\nsimilar functions, including proteins of the CRISPR-associated nuclease,\\nferritin-like iron storage and cytochrome P450 families. Applying the trained\\nRNN models on the partially unannotated UniRef100 database predicted not only\\ncandidates validated by existing annotations but also currently unannotated\\nsequences. Some RNN predictions for the ferritin-like iron sequestering\\nfunction were experimentally validated, even though their sequences differ\\nsignificantly from known, characterized proteins and from each other and cannot\\nbe easily predicted using popular bioinformatics methods. As sequencing and\\nexperimental characterization data increases rapidly, the machine-learning\\napproach based on RNN could be useful for discovery and prediction of\\nhomologues for a wide range of protein functions.\\n', '  Network embedding aims at projecting the network data into a low-dimensional\\nfeature space, where the nodes are represented as a unique feature vector and\\nnetwork structure can be effectively preserved. In recent years, more and more\\nonline application service sites can be represented as massive and complex\\nnetworks, which are extremely challenging for traditional machine learning\\nalgorithms to deal with. Effective embedding of the complex network data into\\nlow-dimension feature representation can both save data storage space and\\nenable traditional machine learning algorithms applicable to handle the network\\ndata. Network embedding performance will degrade greatly if the networks are of\\na sparse structure, like the emerging networks with few connections. In this\\npaper, we propose to learn the embedding representation for a target emerging\\nnetwork based on the broad learning setting, where the emerging network is\\naligned with other external mature networks at the same time. To solve the\\nproblem, a new embedding framework, namely \"Deep alIgned autoencoder based\\neMbEdding\" (DIME), is introduced in this paper. DIME handles the diverse link\\nand attribute in a unified analytic based on broad learning, and introduces the\\nmultiple aligned attributed heterogeneous social network concept to model the\\nnetwork structure. A set of meta paths are introduced in the paper, which\\ndefine various kinds of connections among users via the heterogeneous link and\\nattribute information. The closeness among users in the networks are defined as\\nthe meta proximity scores, which will be fed into DIME to learn the embedding\\nvectors of users in the emerging network. Extensive experiments have been done\\non real-world aligned social networks, which have demonstrated the\\neffectiveness of DIME in learning the emerging network embedding vectors.\\n', '  With the widespread use of information technologies, information networks are\\nbecoming increasingly popular to capture complex relationships across various\\ndisciplines, such as social networks, citation networks, telecommunication\\nnetworks, and biological networks. Analyzing these networks sheds light on\\ndifferent aspects of social life such as the structure of societies,\\ninformation diffusion, and communication patterns. In reality, however, the\\nlarge scale of information networks often makes network analytic tasks\\ncomputationally expensive or intractable. Network representation learning has\\nbeen recently proposed as a new learning paradigm to embed network vertices\\ninto a low-dimensional vector space, by preserving network topology structure,\\nvertex content, and other side information. This facilitates the original\\nnetwork to be easily handled in the new vector space for further analysis. In\\nthis survey, we perform a comprehensive review of the current literature on\\nnetwork representation learning in the data mining and machine learning field.\\nWe propose new taxonomies to categorize and summarize the state-of-the-art\\nnetwork representation learning techniques according to the underlying learning\\nmechanisms, the network information intended to preserve, as well as the\\nalgorithmic designs and methodologies. We summarize evaluation protocols used\\nfor validating network representation learning including published benchmark\\ndatasets, evaluation methods, and open source algorithms. We also perform\\nempirical studies to compare the performance of representative algorithms on\\ncommon datasets, and analyze their computational complexity. Finally, we\\nsuggest promising research directions to facilitate future study.\\n', '  Spiking neural networks (SNNs) enable power-efficient implementations due to\\ntheir sparse, spike-based coding scheme. This paper develops a bio-inspired SNN\\nthat uses unsupervised learning to extract discriminative features from speech\\nsignals, which can subsequently be used in a classifier. The architecture\\nconsists of a spiking convolutional/pooling layer followed by a fully connected\\nspiking layer for feature discovery. The convolutional layer of leaky,\\nintegrate-and-fire (LIF) neurons represents primary acoustic features. The\\nfully connected layer is equipped with a probabilistic spike-timing-dependent\\nplasticity learning rule. This layer represents the discriminative features\\nthrough probabilistic, LIF neurons. To assess the discriminative power of the\\nlearned features, they are used in a hidden Markov model (HMM) for spoken digit\\nrecognition. The experimental results show performance above 96% that compares\\nfavorably with popular statistical feature extraction methods. Our results\\nprovide a novel demonstration of unsupervised feature acquisition in an SNN.\\n', '  In this paper we propose an implement a general convolutional neural network\\n(CNN) building framework for designing real-time CNNs. We validate our models\\nby creating a real-time vision system which accomplishes the tasks of face\\ndetection, gender classification and emotion classification simultaneously in\\none blended step using our proposed CNN architecture. After presenting the\\ndetails of the training procedure setup we proceed to evaluate on standard\\nbenchmark sets. We report accuracies of 96% in the IMDB gender dataset and 66%\\nin the FER-2013 emotion dataset. Along with this we also introduced the very\\nrecent real-time enabled guided back-propagation visualization technique.\\nGuided back-propagation uncovers the dynamics of the weight changes and\\nevaluates the learned features. We argue that the careful implementation of\\nmodern CNN architectures, the use of the current regularization methods and the\\nvisualization of previously hidden features are necessary in order to reduce\\nthe gap between slow performances and real-time architectures. Our system has\\nbeen validated by its deployment on a Care-O-bot 3 robot used during\\nRoboCup@Home competitions. All our code, demos and pre-trained architectures\\nhave been released under an open-source license in our public repository.\\n', '  The use of drug combinations, termed polypharmacy, is common to treat\\npatients with complex diseases and co-existing conditions. However, a major\\nconsequence of polypharmacy is a much higher risk of adverse side effects for\\nthe patient. Polypharmacy side effects emerge because of drug-drug\\ninteractions, in which activity of one drug may change if taken with another\\ndrug. The knowledge of drug interactions is limited because these complex\\nrelationships are rare, and are usually not observed in relatively small\\nclinical testing. Discovering polypharmacy side effects thus remains an\\nimportant challenge with significant implications for patient mortality. Here,\\nwe present Decagon, an approach for modeling polypharmacy side effects. The\\napproach constructs a multimodal graph of protein-protein interactions,\\ndrug-protein target interactions, and the polypharmacy side effects, which are\\nrepresented as drug-drug interactions, where each side effect is an edge of a\\ndifferent type. Decagon is developed specifically to handle such multimodal\\ngraphs with a large number of edge types. Our approach develops a new graph\\nconvolutional neural network for multirelational link prediction in multimodal\\nnetworks. Decagon predicts the exact side effect, if any, through which a given\\ndrug combination manifests clinically. Decagon accurately predicts polypharmacy\\nside effects, outperforming baselines by up to 69%. We find that it\\nautomatically learns representations of side effects indicative of\\nco-occurrence of polypharmacy in patients. Furthermore, Decagon models\\nparticularly well side effects with a strong molecular basis, while on\\npredominantly non-molecular side effects, it achieves good performance because\\nof effective sharing of model parameters across edge types. Decagon creates\\nopportunities to use large pharmacogenomic and patient data to flag and\\nprioritize side effects for follow-up analysis.\\n', '  Cognitive neuroscience is enjoying rapid increase in extensive public\\nbrain-imaging datasets. It opens the door to large-scale statistical models.\\nFinding a unified perspective for all available data calls for scalable and\\nautomated solutions to an old challenge: how to aggregate heterogeneous\\ninformation on brain function into a universal cognitive system that relates\\nmental operations/cognitive processes/psychological tasks to brain networks? We\\ncast this challenge in a machine-learning approach to predict conditions from\\nstatistical brain maps across different studies. For this, we leverage\\nmulti-task learning and multi-scale dimension reduction to learn\\nlow-dimensional representations of brain images that carry cognitive\\ninformation and can be robustly associated with psychological stimuli. Our\\nmulti-dataset classification model achieves the best prediction performance on\\nseveral large reference datasets, compared to models without cognitive-aware\\nlow-dimension representations, it brings a substantial performance boost to the\\nanalysis of small datasets, and can be introspected to identify universal\\ntemplate cognitive concepts.\\n', '  Automatic body part recognition for CT slices can benefit various medical\\nimage applications. Recent deep learning methods demonstrate promising\\nperformance, with the requirement of large amounts of labeled images for\\ntraining. The intrinsic structural or superior-inferior slice ordering\\ninformation in CT volumes is not fully exploited. In this paper, we propose a\\nconvolutional neural network (CNN) based Unsupervised Body part Regression\\n(UBR) algorithm to address this problem. A novel unsupervised learning method\\nand two inter-sample CNN loss functions are presented. Distinct from previous\\nwork, UBR builds a coordinate system for the human body and outputs a\\ncontinuous score for each axial slice, representing the normalized position of\\nthe body part in the slice. The training process of UBR resembles a\\nself-organization process: slice scores are learned from inter-slice\\nrelationships. The training samples are unlabeled CT volumes that are abundant,\\nthus no extra annotation effort is needed. UBR is simple, fast, and accurate.\\nQuantitative and qualitative experiments validate its effectiveness. In\\naddition, we show two applications of UBR in network initialization and anomaly\\ndetection.\\n', \"  Word obfuscation or substitution means replacing one word with another word\\nin a sentence to conceal the textual content or communication. Word obfuscation\\nis used in adversarial communication by terrorist or criminals for conveying\\ntheir messages without getting red-flagged by security and intelligence\\nagencies intercepting or scanning messages (such as emails and telephone\\nconversations). ConceptNet is a freely available semantic network represented\\nas a directed graph consisting of nodes as concepts and edges as assertions of\\ncommon sense about these concepts. We present a solution approach exploiting\\nvast amount of semantic knowledge in ConceptNet for addressing the technically\\nchallenging problem of word substitution in adversarial communication. We frame\\nthe given problem as a textual reasoning and context inference task and utilize\\nConceptNet's natural-language-processing tool-kit for determining word\\nsubstitution. We use ConceptNet to compute the conceptual similarity between\\nany two given terms and define a Mean Average Conceptual Similarity (MACS)\\nmetric to identify out-of-context terms. The test-bed to evaluate our proposed\\napproach consists of Enron email dataset (having over 600000 emails generated\\nby 158 employees of Enron Corporation) and Brown corpus (totaling about a\\nmillion words drawn from a wide variety of sources). We implement word\\nsubstitution techniques used by previous researches to generate a test dataset.\\nWe conduct a series of experiments consisting of word substitution methods used\\nin the past to evaluate our approach. Experimental results reveal that the\\nproposed approach is effective.\\n\", '  We study a unique network dataset including periodic surveys and electronic\\nlogs of dyadic contacts via smartphones. The participants were a sample of\\nfreshmen entering university in the Fall 2011. Their opinions on a variety of\\npolitical and social issues and lists of activities on campus were regularly\\nrecorded at the beginning and end of each semester for the first three years of\\nstudy. We identify a behavioral network defined by call and text data, and a\\ncognitive network based on friendship nominations in ego-network surveys. Both\\nnetworks are limited to study participants. Since a wide range of attributes on\\neach node were collected in self-reports, we refer to these networks as\\nattribute-rich networks. We study whether student preferences for certain\\nattributes of friends can predict formation and dissolution of edges in both\\nnetworks. We introduce a method for computing student preferences for different\\nattributes which we use to predict link formation and dissolution. We then rank\\nthese attributes according to their importance for making predictions. We find\\nthat personal preferences, in particular political views, and preferences for\\ncommon activities help predict link formation and dissolution in both the\\nbehavioral and cognitive networks.\\n', '  As prior knowledge of objects or object features helps us make relations for\\nsimilar objects on attentional tasks, pre-trained deep convolutional neural\\nnetworks (CNNs) can be used to detect salient objects on images regardless of\\nthe object class is in the network knowledge or not. In this paper, we propose\\na top-down saliency model using CNN, a weakly supervised CNN model trained for\\n1000 object labelling task from RGB images. The model detects attentive regions\\nbased on their objectness scores predicted by selected features from CNNs. To\\nestimate the salient objects effectively, we combine both forward and backward\\nfeatures, while demonstrating that partially-guided backpropagation will\\nprovide sufficient information for selecting the features from forward run of\\nCNN model. Finally, these top-down cues are enhanced with a state-of-the-art\\nbottom-up model as complementing the overall saliency. As the proposed model is\\nan effective integration of forward and backward cues through objectness\\nwithout any supervision or regression to ground truth data, it gives promising\\nresults compared to state-of-the-art models in two different datasets.\\n', '  Skin cancer is one of the major types of cancers and its incidence has been\\nincreasing over the past decades. Skin lesions can arise from various\\ndermatologic disorders and can be classified to various types according to\\ntheir texture, structure, color and other morphological features. The accuracy\\nof diagnosis of skin lesions, specifically the discrimination of benign and\\nmalignant lesions, is paramount to ensure appropriate patient treatment.\\nMachine learning-based classification approaches are among popular automatic\\nmethods for skin lesion classification. While there are many existing methods,\\nconvolutional neural networks (CNN) have shown to be superior over other\\nclassical machine learning methods for object detection and classification\\ntasks. In this work, a fully automatic computerized method is proposed, which\\nemploys well established pre-trained convolutional neural networks and\\nensembles learning to classify skin lesions. We trained the networks using 2000\\nskin lesion images available from the ISIC 2017 challenge, which has three main\\ncategories and includes 374 melanoma, 254 seborrheic keratosis and 1372 benign\\nnevi images. The trained classifier was then tested on 150 unlabeled images.\\nThe results, evaluated by the challenge organizer and based on the area under\\nthe receiver operating characteristic curve (AUC), were 84.8% and 93.6% for\\nMelanoma and seborrheic keratosis binary classification problem, respectively.\\nThe proposed method achieved competitive results to experienced\\ndermatologist. Further improvement and optimization of the proposed method with\\na larger training dataset could lead to a more precise, reliable and robust\\nmethod for skin lesion classification.\\n', '  With the rapid growth of social media, massive misinformation is also\\nspreading widely on social media, such as microblog, and bring negative effects\\nto human life. Nowadays, automatic misinformation identification has drawn\\nattention from academic and industrial communities. For an event on social\\nmedia usually consists of multiple microblogs, current methods are mainly based\\non global statistical features. However, information on social media is full of\\nnoisy and outliers, which should be alleviated. Moreover, most of microblogs\\nabout an event have little contribution to the identification of\\nmisinformation, where useful information can be easily overwhelmed by useless\\ninformation. Thus, it is important to mine significant microblogs for a\\nreliable misinformation identification method. In this paper, we propose an\\nAttention-based approach for Identification of Misinformation (AIM). Based on\\nthe attention mechanism, AIM can select microblogs with largest attention\\nvalues for misinformation identification. The attention mechanism in AIM\\ncontains two parts: content attention and dynamic attention. Content attention\\nis calculated based textual features of each microblog. Dynamic attention is\\nrelated to the time interval between the posting time of a microblog and the\\nbeginning of the event. To evaluate AIM, we conduct a series of experiments on\\nthe Weibo dataset and the Twitter dataset, and the experimental results show\\nthat the proposed AIM model outperforms the state-of-the-art methods.\\n', '  We present a system for covert automated deception detection in real-life\\ncourtroom trial videos. We study the importance of different modalities like\\nvision, audio and text for this task. On the vision side, our system uses\\nclassifiers trained on low level video features which predict human\\nmicro-expressions. We show that predictions of high-level micro-expressions can\\nbe used as features for deception prediction. Surprisingly, IDT (Improved Dense\\nTrajectory) features which have been widely used for action recognition, are\\nalso very good at predicting deception in videos. We fuse the score of\\nclassifiers trained on IDT features and high-level micro-expressions to improve\\nperformance. MFCC (Mel-frequency Cepstral Coefficients) features from the audio\\ndomain also provide a significant boost in performance, while information from\\ntranscripts is not very beneficial for our system. Using various classifiers,\\nour automated system obtains an AUC of 0.877 (10-fold cross-validation) when\\nevaluated on subjects which were not part of the training set. Even though\\nstate-of-the-art methods use human annotations of micro-expressions for\\ndeception detection, our fully automated approach outperforms them by 5%. When\\ncombined with human annotations of micro-expressions, our AUC improves to\\n0.922. We also present results of a user-study to analyze how well do average\\nhumans perform on this task, what modalities they use for deception detection\\nand how they perform if only one modality is accessible. Our project page can\\nbe found at \\\\url{this https URL}.\\n', '  We propose to use neural networks for simultaneous detection and localization\\nof multiple sound sources in human-robot interaction. In contrast to\\nconventional signal processing techniques, neural network-based sound source\\nlocalization methods require fewer strong assumptions about the environment.\\nPrevious neural network-based methods have been focusing on localizing a single\\nsound source, which do not extend to multiple sources in terms of detection and\\nlocalization. In this paper, we thus propose a likelihood-based encoding of the\\nnetwork output, which naturally allows the detection of an arbitrary number of\\nsources. In addition, we investigate the use of sub-band cross-correlation\\ninformation as features for better localization in sound mixtures, as well as\\nthree different network architectures based on different motivations.\\nExperiments on real data recorded from a robot show that our proposed methods\\nsignificantly outperform the popular spatial spectrum-based approaches.\\n', '  Spatial understanding is a fundamental problem with wide-reaching real-world\\napplications. The representation of spatial knowledge is often modeled with\\nspatial templates, i.e., regions of acceptability of two objects under an\\nexplicit spatial relationship (e.g., \"on\", \"below\", etc.). In contrast with\\nprior work that restricts spatial templates to explicit spatial prepositions\\n(e.g., \"glass on table\"), here we extend this concept to implicit spatial\\nlanguage, i.e., those relationships (generally actions) for which the spatial\\narrangement of the objects is only implicitly implied (e.g., \"man riding\\nhorse\"). In contrast with explicit relationships, predicting spatial\\narrangements from implicit spatial language requires significant common sense\\nspatial understanding. Here, we introduce the task of predicting spatial\\ntemplates for two objects under a relationship, which can be seen as a spatial\\nquestion-answering task with a (2D) continuous output (\"where is the man w.r.t.\\na horse when the man is walking the horse?\"). We present two simple\\nneural-based models that leverage annotated images and structured text to learn\\nthis task. The good performance of these models reveals that spatial locations\\nare to a large extent predictable from implicit spatial language. Crucially,\\nthe models attain similar performance in a challenging generalized setting,\\nwhere the object-relation-object combinations (e.g.,\"man walking dog\") have\\nnever been seen before. Next, we go one step further by presenting the models\\nwith unseen objects (e.g., \"dog\"). In this scenario, we show that leveraging\\nword embeddings enables the models to output accurate spatial predictions,\\nproving that the models acquire solid common sense spatial knowledge allowing\\nfor such generalization.\\n', '  Following the rapidly growing digital image usage, automatic image\\ncategorization has become preeminent research area. It has broaden and adopted\\nmany algorithms from time to time, whereby multi-feature (generally,\\nhand-engineered features) based image characterization comes handy to improve\\naccuracy. Recently, in machine learning, pre-trained deep convolutional neural\\nnetworks (DCNNs or ConvNets) have been that the features extracted through such\\nDCNN can improve classification accuracy. Thence, in this paper, we further\\ninvestigate a feature embedding strategy to exploit cues from multiple DCNNs.\\nWe derive a generalized feature space by embedding three different DCNN\\nbottleneck features with weights respect to their Softmax cross-entropy loss.\\nTest outcomes on six different object classification data-sets and an action\\nclassification data-set show that regardless of variation in image statistics\\nand tasks the proposed multi-DCNN bottleneck feature fusion is well suited to\\nimage classification tasks and an effective complement of DCNN. The comparisons\\nto existing fusion-based image classification approaches prove that the\\nproposed method surmounts the state-of-the-art methods and produces competitive\\nresults with fully trained DCNNs as well.\\n', \"  'Style transfer' among images has recently emerged as a very active research\\ntopic, fuelled by the power of convolution neural networks (CNNs), and has\\nbecome fast a very popular technology in social media. This paper investigates\\nthe analogous problem in the audio domain: How to transfer the style of a\\nreference audio signal to a target audio content? We propose a flexible\\nframework for the task, which uses a sound texture model to extract statistics\\ncharacterizing the reference audio style, followed by an optimization-based\\naudio texture synthesis to modify the target content. In contrast to mainstream\\noptimization-based visual transfer method, the proposed process is initialized\\nby the target content instead of random noise and the optimized loss is only\\nabout texture, not structure. These differences proved key for audio style\\ntransfer in our experiments. In order to extract features of interest, we\\ninvestigate different architectures, whether pre-trained on other tasks, as\\ndone in image style transfer, or engineered based on the human auditory system.\\nExperimental results on different types of audio signal confirm the potential\\nof the proposed approach.\\n\", '  We introduce the concrete autoencoder, an end-to-end differentiable method\\nfor global feature selection, which efficiently identifies a subset of the most\\ninformative features and simultaneously learns a neural network to reconstruct\\nthe input data from the selected features. Our method is unsupervised, and is\\nbased on using a concrete selector layer as the encoder and using a standard\\nneural network as the decoder. During the training phase, the temperature of\\nthe concrete selector layer is gradually decreased, which encourages a\\nuser-specified number of discrete features to be learned. During test time, the\\nselected features can be used with the decoder network to reconstruct the\\nremaining input features. We evaluate concrete autoencoders on a variety of\\ndatasets, where they significantly outperform state-of-the-art methods for\\nfeature selection and data reconstruction. In particular, on a large-scale gene\\nexpression dataset, the concrete autoencoder selects a small subset of genes\\nwhose expression levels can be use to impute the expression levels of the\\nremaining genes. In doing so, it improves on the current widely-used\\nexpert-curated L1000 landmark genes, potentially reducing measurement costs by\\n20%. The concrete autoencoder can be implemented by adding just a few lines of\\ncode to a standard autoencoder.\\n', \"  We present a generative framework for generalized zero-shot learning where\\nthe training and test classes are not necessarily disjoint. Built upon a\\nvariational autoencoder based architecture, consisting of a probabilistic\\nencoder and a probabilistic conditional decoder, our model can generate novel\\nexemplars from seen/unseen classes, given their respective class attributes.\\nThese exemplars can subsequently be used to train any off-the-shelf\\nclassification model. One of the key aspects of our encoder-decoder\\narchitecture is a feedback-driven mechanism in which a discriminator (a\\nmultivariate regressor) learns to map the generated exemplars to the\\ncorresponding class attribute vectors, leading to an improved generator. Our\\nmodel's ability to generate and leverage examples from unseen classes to train\\nthe classification model naturally helps to mitigate the bias towards\\npredicting seen classes in generalized zero-shot learning settings. Through a\\ncomprehensive set of experiments, we show that our model outperforms several\\nstate-of-the-art methods, on several benchmark datasets, for both standard as\\nwell as generalized zero-shot learning.\\n\", \"  Recently, deep neural networks have demonstrated excellent performances in\\nrecognizing the age and gender on human face images. However, these models were\\napplied in a black-box manner with no information provided about which facial\\nfeatures are actually used for prediction and how these features depend on\\nimage preprocessing, model initialization and architecture choice. We present a\\nstudy investigating these different effects.\\nIn detail, our work compares four popular neural network architectures,\\nstudies the effect of pretraining, evaluates the robustness of the considered\\nalignment preprocessings via cross-method test set swapping and intuitively\\nvisualizes the model's prediction strategies in given preprocessing conditions\\nusing the recent Layer-wise Relevance Propagation (LRP) algorithm. Our\\nevaluations on the challenging Adience benchmark show that suitable parameter\\ninitialization leads to a holistic perception of the input, compensating\\nartefactual data representations. With a combination of simple preprocessing\\nsteps, we reach state of the art performance in gender recognition.\\n\", '  Most video summarization approaches have focused on extracting a summary from\\na single video; we propose an unsupervised framework for summarizing a\\ncollection of videos. We observe that each video in the collection may contain\\nsome information that other videos do not have, and thus exploring the\\nunderlying complementarity could be beneficial in creating a diverse\\ninformative summary. We develop a novel diversity-aware sparse optimization\\nmethod for multi-video summarization by exploring the complementarity within\\nthe videos. Our approach extracts a multi-video summary which is both\\ninteresting and representative in describing the whole video collection. To\\nefficiently solve our optimization problem, we develop an alternating\\nminimization algorithm that minimizes the overall objective function with\\nrespect to one video at a time while fixing the other videos. Moreover, we\\nintroduce a new benchmark dataset, Tour20, that contains 140 videos with\\nmultiple human created summaries, which were acquired in a controlled\\nexperiment. Finally, by extensive experiments on the new Tour20 dataset and\\nseveral other multi-view datasets, we show that the proposed approach clearly\\noutperforms the state-of-the-art methods on the two problems-topic-oriented\\nvideo summarization and multi-view video summarization in a camera network.\\n', '  The need for large annotated image datasets for training Convolutional Neural\\nNetworks (CNNs) has been a significant impediment for their adoption in\\ncomputer vision applications. We show that with transfer learning an effective\\nobject detector can be trained almost entirely on synthetically rendered\\ndatasets. We apply this strategy for detecting pack- aged food products\\nclustered in refrigerator scenes. Our CNN trained only with 4000 synthetic\\nimages achieves mean average precision (mAP) of 24 on a test set with 55\\ndistinct products as objects of interest and 17 distractor objects. A further\\nincrease of 12% in the mAP is obtained by adding only 400 real images to these\\n4000 synthetic images in the training set. A high degree of photorealism in the\\nsynthetic images was not essential in achieving this performance. We analyze\\nfactors like training data set size and 3D model dictionary size for their\\ninfluence on detection performance. Additionally, training strategies like\\nfine-tuning with selected layers and early stopping which affect transfer\\nlearning from synthetic scenes to real scenes are explored. Training CNNs with\\nsynthetic datasets is a novel application of high-performance computing and a\\npromising approach for object detection applications in domains where there is\\na dearth of large annotated image data.\\n', '  Crowdsourced video systems like YouTube and Twitch.tv have been a major\\ninternet phenomenon and are nowadays entertaining over a billion users. In\\naddition to video sharing and viewing, over the years they have developed new\\nfeatures to boost the community engagement and some managed to attract users to\\ndonate, to the community as well as to other users. User donation directly\\nreflects and influences user engagement in the community, and has a great\\nimpact on the success of such systems. Nevertheless, user donations in\\ncrowdsourced video systems remain trade secrets for most companies and to date\\nare still unexplored. In this work, we attempt to fill this gap, and we obtain\\nand provide a publicly available dataset on user donations in one crowdsourced\\nvideo system named BiliBili. Based on information on nearly 40 thousand\\ndonators, we examine the dynamics of user donations and their social\\nrelationships, we quantitively reveal the factors that potentially impact user\\ndonation, and we adopt machine-learned classifiers and network representation\\nlearning models to timely and accurately predict the destinations of the\\nmajority and the individual donations.\\n', '  A robot that can carry out a natural-language instruction has been a dream\\nsince before the Jetsons cartoon series imagined a life of leisure mediated by\\na fleet of attentive robot helpers. It is a dream that remains stubbornly\\ndistant. However, recent advances in vision and language methods have made\\nincredible progress in closely related areas. This is significant because a\\nrobot interpreting a natural-language navigation instruction on the basis of\\nwhat it sees is carrying out a vision and language process that is similar to\\nVisual Question Answering. Both tasks can be interpreted as visually grounded\\nsequence-to-sequence translation problems, and many of the same methods are\\napplicable. To enable and encourage the application of vision and language\\nmethods to the problem of interpreting visually-grounded navigation\\ninstructions, we present the Matterport3D Simulator -- a large-scale\\nreinforcement learning environment based on real imagery. Using this simulator,\\nwhich can in future support a range of embodied vision and language tasks, we\\nprovide the first benchmark dataset for visually-grounded natural language\\nnavigation in real buildings -- the Room-to-Room (R2R) dataset.\\n', '  The discriminative power of modern deep learning models for 3D human action\\nrecognition is growing ever so potent. In conjunction with the recent\\nresurgence of 3D human action representation with 3D skeletons, the quality and\\nthe pace of recent progress have been significant. However, the inner workings\\nof state-of-the-art learning based methods in 3D human action recognition still\\nremain mostly black-box. In this work, we propose to use a new class of models\\nknown as Temporal Convolutional Neural Networks (TCN) for 3D human action\\nrecognition. Compared to popular LSTM-based Recurrent Neural Network models,\\ngiven interpretable input such as 3D skeletons, TCN provides us a way to\\nexplicitly learn readily interpretable spatio-temporal representations for 3D\\nhuman action recognition. We provide our strategy in re-designing the TCN with\\ninterpretability in mind and how such characteristics of the model is leveraged\\nto construct a powerful 3D activity recognition method. Through this work, we\\nwish to take a step towards a spatio-temporal model that is easier to\\nunderstand, explain and interpret. The resulting model, Res-TCN, achieves\\nstate-of-the-art results on the largest 3D human action recognition dataset,\\nNTU-RGBD.\\n', '  Objective. The purpose of this work is to analyse the knowledge structure and\\ntrends in scientific research in the Online Information Reviews journal by\\nbibliometric analysis of key words and social network analysis of co-words.\\nMethods. Key words included in a set of 758 papers included in the Web of\\nScience database from 2000 to 2014 were analysed. We conducted a subject\\nanalysis considering the key words assigned to papers. A social network\\nanalysis was also conducted to identify the number of co-occurrences between\\nkey words (co-words). The Pajek software was used to create and graphically\\nvisualize the networks. Results. Internet is the most frequent key word (n=219)\\nand the most central in the network of co-words, strongly associated with\\nInformation retrieval, search engines, the World Wide Web, libraries and users\\nConclusions. Information science, as represented by Online Information Review\\nin the present study, is an evolving discipline that draws on literature from a\\nrelatively wide range of subjects. Although Online Information Review appears\\nto have well-defined and established research topics, the journal also changes\\nrapidly to embrace new lines of research.\\n', '  The effective representation of proteins is a crucial task that directly\\naffects the performance of many bioinformatics problems. Related proteins\\nusually bind to similar ligands. Chemical characteristics of ligands are known\\nto capture the functional and mechanistic properties of proteins suggesting\\nthat a ligand based approach can be utilized in protein representation. In this\\nstudy, we propose SMILESVec, a SMILES-based method to represent ligands and a\\nnovel method to compute similarity of proteins by describing them based on\\ntheir ligands. The proteins are defined utilizing the word-embeddings of the\\nSMILES strings of their ligands. The performance of the proposed protein\\ndescription method is evaluated in protein clustering task using TransClust and\\nMCL algorithms. Two other protein representation methods that utilize protein\\nsequence, BLAST and ProtVec, and two compound fingerprint based protein\\nrepresentation methods are compared. We showed that ligand-based protein\\nrepresentation, which uses only SMILES strings of the ligands that proteins\\nbind to, performs as well as protein-sequence based representation methods in\\nprotein clustering. The results suggest that ligand-based protein description\\ncan be an alternative to the traditional sequence or structure based\\nrepresentation of proteins and this novel approach can be applied to different\\nbioinformatics problems such as prediction of new protein-ligand interactions\\nand protein function annotation.\\n', '  Programmable packet processors and P4 as a programming language for such\\ndevices have gained significant interest, because their flexibility enables\\nrapid development of a diverse set of applications that work at line rate.\\nHowever, this flexibility, combined with the complexity of devices and\\nnetworks, increases the chance of introducing subtle bugs that are hard to\\ndiscover manually. Worse, this is a domain where bugs can have catastrophic\\nconsequences, yet formal analysis tools for P4 programs / networks are missing.\\nWe argue that formal analysis tools must be based on a formal semantics of\\nthe target language, rather than on its informal specification. To this end, we\\nprovide an executable formal semantics of the P4 language in the K framework.\\nBased on this semantics, K provides an interpreter and various analysis tools\\nincluding a symbolic model checker and a deductive program verifier for P4.\\nThis paper overviews our formal K semantics of P4, as well as several P4\\nlanguage design issues that we found during our formalization process. We also\\ndiscuss some applications resulting from the tools provided by K for P4\\nprogrammers and network administrators as well as language designers and\\ncompiler developers, such as detection of unportable code, state space\\nexploration of P4 programs and of networks, bug finding using symbolic\\nexecution, data plane verification, program verification, and translation\\nvalidation.\\n', '  Domain shift refers to the well known problem that a model trained in one\\nsource domain performs poorly when applied to a target domain with different\\nstatistics. {Domain Generalization} (DG) techniques attempt to alleviate this\\nissue by producing models which by design generalize well to novel testing\\ndomains. We propose a novel {meta-learning} method for domain generalization.\\nRather than designing a specific model that is robust to domain shift as in\\nmost previous DG work, we propose a model agnostic training procedure for DG.\\nOur algorithm simulates train/test domain shift during training by synthesizing\\nvirtual testing domains within each mini-batch. The meta-optimization objective\\nrequires that steps to improve training domain performance should also improve\\ntesting domain performance. This meta-learning procedure trains models with\\ngood generalization ability to novel domains. We evaluate our method and\\nachieve state of the art results on a recent cross-domain image classification\\nbenchmark, as well demonstrating its potential on two classic reinforcement\\nlearning tasks.\\n', '  We propose MAD-GAN, an intuitive generalization to the Generative Adversarial\\nNetworks (GANs) and its conditional variants to address the well known problem\\nof mode collapse. First, MAD-GAN is a multi-agent GAN architecture\\nincorporating multiple generators and one discriminator. Second, to enforce\\nthat different generators capture diverse high probability modes, the\\ndiscriminator of MAD-GAN is designed such that along with finding the real and\\nfake samples, it is also required to identify the generator that generated the\\ngiven fake sample. Intuitively, to succeed in this task, the discriminator must\\nlearn to push different generators towards different identifiable modes. We\\nperform extensive experiments on synthetic and real datasets and compare\\nMAD-GAN with different variants of GAN. We show high quality diverse sample\\ngenerations for challenging tasks such as image-to-image translation and face\\ngeneration. In addition, we also show that MAD-GAN is able to disentangle\\ndifferent modalities when trained using highly challenging diverse-class\\ndataset (e.g. dataset with images of forests, icebergs, and bedrooms). In the\\nend, we show its efficacy on the unsupervised feature representation task. In\\nAppendix, we introduce a similarity based competing objective (MAD-GAN-Sim)\\nwhich encourages different generators to generate diverse samples based on a\\nuser defined similarity metric. We show its performance on the image-to-image\\ntranslation, and also show its effectiveness on the unsupervised feature\\nrepresentation task.\\n', '  Forward-looking sonar can capture high resolution images of underwater\\nscenes, but their interpretation is complex. Generic object detection in such\\nimages has not been solved, specially in cases of small and unknown objects. In\\ncomparison, detection proposal algorithms have produced top performing object\\ndetectors in real-world color images. In this work we develop a Convolutional\\nNeural Network that can reliably score objectness of image windows in\\nforward-looking sonar images and by thresholding objectness, we generate\\ndetection proposals. In our dataset of marine garbage objects, we obtain 94%\\nrecall, generating around 60 proposals per image. The biggest strength of our\\nmethod is that it can generalize to previously unseen objects. We show this by\\ndetecting chain links, walls and a wrench without previous training in such\\nobjects. We strongly believe our method can be used for class-independent\\nobject detection, with many real-world applications such as chain following and\\nmine detection.\\n', '  We present a conceptually simple, flexible, and general framework for object\\ninstance segmentation. Our approach efficiently detects objects in an image\\nwhile simultaneously generating a high-quality segmentation mask for each\\ninstance. The method, called Mask R-CNN, extends Faster R-CNN by adding a\\nbranch for predicting an object mask in parallel with the existing branch for\\nbounding box recognition. Mask R-CNN is simple to train and adds only a small\\noverhead to Faster R-CNN, running at 5 fps. Moreover, Mask R-CNN is easy to\\ngeneralize to other tasks, e.g., allowing us to estimate human poses in the\\nsame framework. We show top results in all three tracks of the COCO suite of\\nchallenges, including instance segmentation, bounding-box object detection, and\\nperson keypoint detection. Without bells and whistles, Mask R-CNN outperforms\\nall existing, single-model entries on every task, including the COCO 2016\\nchallenge winners. We hope our simple and effective approach will serve as a\\nsolid baseline and help ease future research in instance-level recognition.\\nCode has been made available at: this https URL\\n', '  Embedded, continual learning for autonomous and adaptive behavior is a key\\napplication of neuromorphic hardware. However, neuromorphic implementations of\\nembedded learning at large scales that are both flexible and efficient have\\nbeen hindered by a lack of a suitable algorithmic framework. As a result, the\\nmost neuromorphic hardware is trained off-line on large clusters of dedicated\\nprocessors or GPUs and transferred post hoc to the device. We address this by\\nintroducing the neural and synaptic array transceiver (NSAT), a neuromorphic\\ncomputational framework facilitating flexible and efficient embedded learning\\nby matching algorithmic requirements and neural and synaptic dynamics. NSAT\\nsupports event-driven supervised, unsupervised and reinforcement learning\\nalgorithms including deep learning. We demonstrate the NSAT in a wide range of\\ntasks, including the simulation of Mihalas-Niebur neuron, dynamic neural\\nfields, event-driven random back-propagation for event-based deep learning,\\nevent-based contrastive divergence for unsupervised learning, and voltage-based\\nlearning rules for sequence learning. We anticipate that this contribution will\\nestablish the foundation for a new generation of devices enabling adaptive\\nmobile systems, wearable devices, and robots with data-driven autonomy.\\n', \"  Paraphrase generation is an important problem in NLP, especially in question\\nanswering, information retrieval, information extraction, conversation systems,\\nto name a few. In this paper, we address the problem of generating paraphrases\\nautomatically. Our proposed method is based on a combination of deep generative\\nmodels (VAE) with sequence-to-sequence models (LSTM) to generate paraphrases,\\ngiven an input sentence. Traditional VAEs when combined with recurrent neural\\nnetworks can generate free text but they are not suitable for paraphrase\\ngeneration for a given sentence. We address this problem by conditioning the\\nboth, encoder and decoder sides of VAE, on the original sentence, so that it\\ncan generate the given sentence's paraphrases. Unlike most existing models, our\\nmodel is simple, modular and can generate multiple paraphrases, for a given\\nsentence. Quantitative evaluation of the proposed method on a benchmark\\nparaphrase dataset demonstrates its efficacy, and its performance improvement\\nover the state-of-the-art methods by a significant margin, whereas qualitative\\nhuman evaluation indicate that the generated paraphrases are well-formed,\\ngrammatically correct, and are relevant to the input sentence. Furthermore, we\\nevaluate our method on a newly released question paraphrase dataset, and\\nestablish a new baseline for future research.\\n\", '  We propose an Encoder-Classifier framework to model the Mandarin tones using\\nrecurrent neural networks (RNN). In this framework, extracted frames of\\nfeatures for tone classification are fed in to the RNN and casted into a fixed\\ndimensional vector (tone embedding) and then classified into tone types using a\\nsoftmax layer along with other auxiliary inputs. We investigate various\\nconfigurations that help to improve the model, including pooling, feature\\nsplicing and utilization of syllable-level tone embeddings. Besides, tone\\nembeddings and durations of the contextual syllables are exploited to\\nfacilitate tone classification. Experimental results on Mandarin tone\\nclassification show the proposed network setups improve tone classification\\naccuracy. The results indicate that the RNN encoder-classifier based tone model\\nflexibly accommodates heterogeneous inputs (sequential and segmental) and hence\\nhas the advantages from both the sequential classification tone models and\\nsegmental classification tone models.\\n', '  The ability to generate natural language sequences from source code snippets\\nhas a variety of applications such as code summarization, documentation, and\\nretrieval. Sequence-to-sequence (seq2seq) models, adopted from neural machine\\ntranslation (NMT), have achieved state-of-the-art performance on these tasks by\\ntreating source code as a sequence of tokens. We present ${\\\\rm {\\\\scriptsize\\nCODE2SEQ}}$: an alternative approach that leverages the syntactic structure of\\nprogramming languages to better encode source code. Our model represents a code\\nsnippet as the set of compositional paths in its abstract syntax tree (AST) and\\nuses attention to select the relevant paths while decoding. We demonstrate the\\neffectiveness of our approach for two tasks, two programming languages, and\\nfour datasets of up to $16$M examples. Our model significantly outperforms\\nprevious models that were specifically designed for programming languages, as\\nwell as state-of-the-art NMT models. An interactive online demo of our model is\\navailable at this http URL. Our code, data and trained models are\\navailable at this http URL.\\n', \"  Understanding why a model makes a certain prediction can be as crucial as the\\nprediction's accuracy in many applications. However, the highest accuracy for\\nlarge modern datasets is often achieved by complex models that even experts\\nstruggle to interpret, such as ensemble or deep learning models, creating a\\ntension between accuracy and interpretability. In response, various methods\\nhave recently been proposed to help users interpret the predictions of complex\\nmodels, but it is often unclear how these methods are related and when one\\nmethod is preferable over another. To address this problem, we present a\\nunified framework for interpreting predictions, SHAP (SHapley Additive\\nexPlanations). SHAP assigns each feature an importance value for a particular\\nprediction. Its novel components include: (1) the identification of a new class\\nof additive feature importance measures, and (2) theoretical results showing\\nthere is a unique solution in this class with a set of desirable properties.\\nThe new class unifies six existing methods, notable because several recent\\nmethods in the class lack the proposed desirable properties. Based on insights\\nfrom this unification, we present new methods that show improved computational\\nperformance and/or better consistency with human intuition than previous\\napproaches.\\n\", '  Neural Machine Translation (NMT) models usually use large target vocabulary\\nsizes to capture most of the words in the target language. The vocabulary size\\nis a big factor when decoding new sentences as the final softmax layer\\nnormalizes over all possible target words. To address this problem, it is\\nwidely common to restrict the target vocabulary with candidate lists based on\\nthe source sentence. Usually, the candidate lists are a combination of external\\nword-to-word aligner, phrase table entries or most frequent words. In this\\nwork, we propose a simple and yet novel approach to learn candidate lists\\ndirectly from the attention layer during NMT training. The candidate lists are\\nhighly optimized for the current NMT model and do not need any external\\ncomputation of the candidate pool. We show significant decoding speedup\\ncompared with using the entire vocabulary, without losing any translation\\nquality for two language pairs.\\n', '  Segmenting foreground object from a video is a challenging task because of\\nthe large deformations of the objects, occlusions, and background clutter. In\\nthis paper, we propose a frame-by-frame but computationally efficient approach\\nfor video object segmentation by clustering visually similar generic object\\nsegments throughout the video. Our algorithm segments various object instances\\nappearing in the video and then perform clustering in order to group visually\\nsimilar segments into one cluster. Since the object that needs to be segmented\\nappears in most part of the video, we can retrieve the foreground segments from\\nthe cluster having maximum number of segments, thus filtering out noisy\\nsegments that do not represent any object. We then apply a track and fill\\napproach in order to localize the objects in the frames where the object\\nsegmentation framework fails to segment any object. Our algorithm performs\\ncomparably to the recent automatic methods for video object segmentation when\\nbenchmarked on DAVIS dataset while being computationally much faster.\\n', '  Being an unsupervised machine learning and data mining technique,\\nbiclustering and its multimodal extensions are becoming popular tools for\\nanalysing object-attribute data in different domains. Apart from conventional\\nclustering techniques, biclustering is searching for homogeneous groups of\\nobjects while keeping their common description, e.g., in binary setting, their\\nshared attributes. In bioinformatics, biclustering is used to find genes, which\\nare active in a subset of situations, thus being candidates for biomarkers.\\nHowever, the authors of those biclustering techniques that are popular in gene\\nexpression analysis, may overlook the existing methods. For instance, BiMax\\nalgorithm is aimed at finding biclusters, which are well-known for decades as\\nformal concepts. Moreover, even if bioinformatics classify the biclustering\\nmethods according to reasonable domain-driven criteria, their classification\\ntaxonomies may be different from survey to survey and not full as well. So, in\\nthis paper we propose to use concept lattices as a tool for taxonomy building\\n(in the biclustering domain) and attribute exploration as means for\\ncross-domain taxonomy completion.\\n', '  While social media offer great communication opportunities, they also\\nincrease the vulnerability of young people to threatening situations online.\\nRecent studies report that cyberbullying constitutes a growing problem among\\nyoungsters. Successful prevention depends on the adequate detection of\\npotentially harmful messages and the information overload on the Web requires\\nintelligent systems to identify potential risks automatically. The focus of\\nthis paper is on automatic cyberbullying detection in social media text by\\nmodelling posts written by bullies, victims, and bystanders of online bullying.\\nWe describe the collection and fine-grained annotation of a training corpus for\\nEnglish and Dutch and perform a series of binary classification experiments to\\ndetermine the feasibility of automatic cyberbullying detection. We make use of\\nlinear support vector machines exploiting a rich feature set and investigate\\nwhich information sources contribute the most for this particular task.\\nExperiments on a holdout test set reveal promising results for the detection of\\ncyberbullying-related posts. After optimisation of the hyperparameters, the\\nclassifier yields an F1-score of 64% and 61% for English and Dutch\\nrespectively, and considerably outperforms baseline systems based on keywords\\nand word unigrams.\\n', '  Word evolution refers to the changing meanings and associations of words\\nthroughout time, as a byproduct of human language evolution. By studying word\\nevolution, we can infer social trends and language constructs over different\\nperiods of human history. However, traditional techniques such as word\\nrepresentation learning do not adequately capture the evolving language\\nstructure and vocabulary. In this paper, we develop a dynamic statistical model\\nto learn time-aware word vector representation. We propose a model that\\nsimultaneously learns time-aware embeddings and solves the resulting \"alignment\\nproblem\". This model is trained on a crawled NYTimes dataset. Additionally, we\\ndevelop multiple intuitive evaluation strategies of temporal word embeddings.\\nOur qualitative and quantitative tests indicate that our method not only\\nreliably captures this evolution over time, but also consistently outperforms\\nstate-of-the-art temporal embedding approaches on both semantic accuracy and\\nalignment quality.\\n', \"  In general, neural networks are not currently capable of learning tasks in a\\nsequential fashion. When a novel, unrelated task is learnt by a neural network,\\nit substantially forgets how to solve previously learnt tasks. One of the\\noriginal solutions to this problem is pseudo-rehearsal, which involves learning\\nthe new task while rehearsing generated items representative of the previous\\ntask/s. This is very effective for simple tasks. However, pseudo-rehearsal has\\nnot yet been successfully applied to very complex tasks because in these tasks\\nit is difficult to generate representative items. We accomplish\\npseudo-rehearsal by using a Generative Adversarial Network to generate items so\\nthat our deep network can learn to sequentially classify the CIFAR-10, SVHN and\\nMNIST datasets. After training on all tasks, our network loses only 1.67%\\nabsolute accuracy on CIFAR-10 and gains 0.24% absolute accuracy on SVHN. Our\\nmodel's performance is a substantial improvement compared to the current state\\nof the art solution.\\n\", '  Biological plastic neural networks are systems of extraordinary computational\\ncapabilities shaped by evolution, development, and lifetime learning. The\\ninterplay of these elements leads to the emergence of adaptive behavior and\\nintelligence. Inspired by such intricate natural phenomena, Evolved Plastic\\nArtificial Neural Networks (EPANNs) use simulated evolution in-silico to breed\\nplastic neural networks with a large variety of dynamics, architectures, and\\nplasticity rules: these artificial systems are composed of inputs, outputs, and\\nplastic components that change in response to experiences in an environment.\\nThese systems may autonomously discover novel adaptive algorithms, and lead to\\nhypotheses on the emergence of biological adaptation. EPANNs have seen\\nconsiderable progress over the last two decades. Current scientific and\\ntechnological advances in artificial neural networks are now setting the\\nconditions for radically new approaches and results. In particular, the\\nlimitations of hand-designed networks could be overcome by more flexible and\\ninnovative solutions. This paper brings together a variety of inspiring ideas\\nthat define the field of EPANNs. The main methods and results are reviewed.\\nFinally, new opportunities and developments are presented.\\n', '  Recently, graph neural networks have attracted great attention and achieved\\nprominent performance in various research fields. Most of those algorithms have\\nassumed pairwise relationships of objects of interest. However, in many real\\napplications, the relationships between objects are in higher-order, beyond a\\npairwise formulation. To efficiently learn deep embeddings on the high-order\\ngraph-structured data, we introduce two end-to-end trainable operators to the\\nfamily of graph neural networks, i.e., hypergraph convolution and hypergraph\\nattention. Whilst hypergraph convolution defines the basic formulation of\\nperforming convolution on a hypergraph, hypergraph attention further enhances\\nthe capacity of representation learning by leveraging an attention module. With\\nthe two operators, a graph neural network is readily extended to a more\\nflexible model and applied to diverse applications where non-pairwise\\nrelationships are observed. Extensive experimental results with semi-supervised\\nnode classification demonstrate the effectiveness of hypergraph convolution and\\nhypergraph attention.\\n', '  In online social networks people often express attitudes towards others,\\nwhich forms massive sentiment links among users. Predicting the sign of\\nsentiment links is a fundamental task in many areas such as personal\\nadvertising and public opinion analysis. Previous works mainly focus on textual\\nsentiment classification, however, text information can only disclose the \"tip\\nof the iceberg\" about users\\' true opinions, of which the most are unobserved\\nbut implied by other sources of information such as social relation and users\\'\\nprofile. To address this problem, in this paper we investigate how to predict\\npossibly existing sentiment links in the presence of heterogeneous information.\\nFirst, due to the lack of explicit sentiment links in mainstream social\\nnetworks, we establish a labeled heterogeneous sentiment dataset which consists\\nof users\\' sentiment relation, social relation and profile knowledge by\\nentity-level sentiment extraction method. Then we propose a novel and flexible\\nend-to-end Signed Heterogeneous Information Network Embedding (SHINE) framework\\nto extract users\\' latent representations from heterogeneous networks and\\npredict the sign of unobserved sentiment links. SHINE utilizes multiple deep\\nautoencoders to map each user into a low-dimension feature space while\\npreserving the network structure. We demonstrate the superiority of SHINE over\\nstate-of-the-art baselines on link prediction and node recommendation in two\\nreal-world datasets. The experimental results also prove the efficacy of SHINE\\nin cold start scenario.\\n', '  Machine learning has become pervasive in multiple domains, impacting a wide\\nvariety of applications, such as knowledge discovery and data mining, natural\\nlanguage processing, information retrieval, computer vision, social and health\\ninformatics, ubiquitous computing, etc. Two essential problems of machine\\nlearning are how to generate features and how to acquire labels for machines to\\nlearn. Particularly, labeling large amount of data for each domain-specific\\nproblem can be very time consuming and costly. It has become a key obstacle in\\nmaking learning protocols realistic in applications. In this paper, we will\\ndiscuss how to use the existing general-purpose world knowledge to enhance\\nmachine learning processes, by enriching the features or reducing the labeling\\nwork. We start from the comparison of world knowledge with domain-specific\\nknowledge, and then introduce three key problems in using world knowledge in\\nlearning processes, i.e., explicit and implicit feature representation,\\ninference for knowledge linking and disambiguation, and learning with direct or\\nindirect supervision. Finally we discuss the future directions of this research\\ntopic.\\n', '  Endowing a dialogue system with particular personality traits is essential to\\ndeliver more human-like conversations. However, due to the challenge of\\nembodying personality via language expression and the lack of large-scale\\npersona-labeled dialogue data, this research problem is still far from\\nwell-studied. In this paper, we investigate the problem of incorporating\\nexplicit personality traits in dialogue generation to deliver personalized\\ndialogues.\\nTo this end, firstly, we construct PersonalDialog, a large-scale multi-turn\\ndialogue dataset containing various traits from a large number of speakers. The\\ndataset consists of 20.83M sessions and 56.25M utterances from 8.47M speakers.\\nEach utterance is associated with a speaker who is marked with traits like Age,\\nGender, Location, Interest Tags, etc. Several anonymization schemes are\\ndesigned to protect the privacy of each speaker. This large-scale dataset will\\nfacilitate not only the study of personalized dialogue generation, but also\\nother researches on sociolinguistics or social science.\\nSecondly, to study how personality traits can be captured and addressed in\\ndialogue generation, we propose persona-aware dialogue generation models within\\nthe sequence to sequence learning framework. Explicit personality traits\\n(structured by key-value pairs) are embedded using a trait fusion module.\\nDuring the decoding process, two techniques, namely persona-aware attention and\\npersona-aware bias, are devised to capture and address trait-related\\ninformation. Experiments demonstrate that our model is able to address proper\\ntraits in different contexts. Case studies also show interesting results for\\nthis challenging research problem.\\n', '  Designing a logo for a new brand is a lengthy and tedious back-and-forth\\nprocess between a designer and a client. In this paper we explore to what\\nextent machine learning can solve the creative task of the designer. For this,\\nwe build a dataset -- LLD -- of 600k+ logos crawled from the world wide web.\\nTraining Generative Adversarial Networks (GANs) for logo synthesis on such\\nmulti-modal data is not straightforward and results in mode collapse for some\\nstate-of-the-art methods. We propose the use of synthetic labels obtained\\nthrough clustering to disentangle and stabilize GAN training. We are able to\\ngenerate a high diversity of plausible logos and we demonstrate latent space\\nexploration techniques to ease the logo design task in an interactive manner.\\nMoreover, we validate the proposed clustered GAN training on CIFAR 10,\\nachieving state-of-the-art Inception scores when using synthetic labels\\nobtained via clustering the features of an ImageNet classifier. GANs can cope\\nwith multi-modal data by means of synthetic labels achieved through clustering,\\nand our results show the creative potential of such techniques for logo\\nsynthesis and manipulation. Our dataset and models will be made publicly\\navailable at this https URL.\\n', '  Human visual object recognition is typically rapid and seemingly effortless,\\nas well as largely independent of viewpoint and object orientation. Until very\\nrecently, animate visual systems were the only ones capable of this remarkable\\ncomputational feat. This has changed with the rise of a class of computer\\nvision algorithms called deep neural networks (DNNs) that achieve human-level\\nclassification performance on object recognition tasks. Furthermore, a growing\\nnumber of studies report similarities in the way DNNs and the human visual\\nsystem process objects, suggesting that current DNNs may be good models of\\nhuman visual object recognition. Yet there clearly exist important\\narchitectural and processing differences between state-of-the-art DNNs and the\\nprimate visual system. The potential behavioural consequences of these\\ndifferences are not well understood. We aim to address this issue by comparing\\nhuman and DNN generalisation abilities towards image degradations. We find the\\nhuman visual system to be more robust to image manipulations like contrast\\nreduction, additive noise or novel eidolon-distortions. In addition, we find\\nprogressively diverging classification error-patterns between humans and DNNs\\nwhen the signal gets weaker, indicating that there may still be marked\\ndifferences in the way humans and current DNNs perform visual object\\nrecognition. We envision that our findings as well as our carefully measured\\nand freely available behavioural datasets provide a new useful benchmark for\\nthe computer vision community to improve the robustness of DNNs and a\\nmotivation for neuroscientists to search for mechanisms in the brain that could\\nfacilitate this robustness.\\n', '  It is well-established by cognitive neuroscience that human perception of\\nobjects constitutes a complex process, where object appearance information is\\ncombined with evidence about the so-called object \"affordances\", namely the\\ntypes of actions that humans typically perform when interacting with them. This\\nfact has recently motivated the \"sensorimotor\" approach to the challenging task\\nof automatic object recognition, where both information sources are fused to\\nimprove robustness. In this work, the aforementioned paradigm is adopted,\\nsurpassing current limitations of sensorimotor object recognition research.\\nSpecifically, the deep learning paradigm is introduced to the problem for the\\nfirst time, developing a number of novel neuro-biologically and\\nneuro-physiologically inspired architectures that utilize state-of-the-art\\nneural networks for fusing the available information sources in multiple ways.\\nThe proposed methods are evaluated using a large RGB-D corpus, which is\\nspecifically collected for the task of sensorimotor object recognition and is\\nmade publicly available. Experimental results demonstrate the utility of\\naffordance information to object recognition, achieving an up to 29% relative\\nerror reduction by its inclusion.\\n', \"  We address personalization issues of image captioning, which have not been\\ndiscussed yet in previous research. For a query image, we aim to generate a\\ndescriptive sentence, accounting for prior knowledge such as the user's active\\nvocabularies in previous documents. As applications of personalized image\\ncaptioning, we tackle two post automation tasks: hashtag prediction and post\\ngeneration, on our newly collected Instagram dataset, consisting of 1.1M posts\\nfrom 6.3K users. We propose a novel captioning model named Context Sequence\\nMemory Network (CSMN). Its unique updates over previous memory network models\\ninclude (i) exploiting memory as a repository for multiple types of context\\ninformation, (ii) appending previously generated words into memory to capture\\nlong-term information without suffering from the vanishing gradient problem,\\nand (iii) adopting CNN memory structure to jointly represent nearby ordered\\nmemory slots for better context understanding. With quantitative evaluation and\\nuser studies via Amazon Mechanical Turk, we show the effectiveness of the three\\nnovel features of CSMN and its performance enhancement for personalized image\\ncaptioning over state-of-the-art captioning models.\\n\", '  Profound vitamin B12 deficiency is a known cause of disease, but the role of\\nlow or intermediate levels of B12 in the development of neuropathy and other\\nneuropsychiatric symptoms as well as the relationship of eating meat and B12\\nlevels is unclear. Here we use food-related internet search patterns from a\\nsample of 8.5 million US-based people as a proxy to B12 intake and correlate\\nthese searches with internet searches related to possible effects of B12\\ndeficiency. Food-related search patterns are highly correlated with known\\nconsumption and food-related searches (Spearman 0.69). Awareness of B12\\ndeficiency was associated with a higher consumption of B12-rich foods and with\\nqueries for B12 supplements. Searches for terms related to neurological\\ndisorders were correlated with searches for B12-poor foods, in contrast with\\ncontrol terms. Popular medicines, those having fewer indications, and those\\nwhich are predominantly used to treat pain are more strongly correlated with\\nthe ability to predict neuropathic pain queries using the B12 contents of food.\\nOur findings provide evidence for the utility of using Internet search patterns\\nto investigate health questions in large populations and suggest that low B12\\nintake may be associated with a broader spectrum of neurological disorders than\\ncurrently appreciated.\\n', '  In deep reinforcement learning (RL) tasks, an efficient exploration mechanism\\nshould be able to encourage an agent to take actions that lead to less frequent\\nstates which may yield higher accumulative future return. However, both knowing\\nabout the future and evaluating the frequentness of states are non-trivial\\ntasks, especially for deep RL domains, where a state is represented by\\nhigh-dimensional image frames. In this paper, we propose a novel informed\\nexploration framework for deep RL, where we build the capability for an RL\\nagent to predict over the future transitions and evaluate the frequentness for\\nthe predicted future frames in a meaningful manner. To this end, we train a\\ndeep prediction model to predict future frames given a state-action pair, and a\\nconvolutional autoencoder model to hash over the seen frames. In addition, to\\nutilize the counts derived from the seen frames to evaluate the frequentness\\nfor the predicted frames, we tackle the challenge of matching the predicted\\nfuture frames and their corresponding seen frames at the latent feature level.\\nIn this way, we derive a reliable metric for evaluating the novelty of the\\nfuture direction pointed by each action, and hence inform the agent to explore\\nthe least frequent one.\\n', \"  The impact of developmental and aging processes on brain connectivity and the\\nconnectome has been widely studied. Network theoretical measures and certain\\ntopological principles are computed from the entire brain, however there is a\\nneed to separate and understand the underlying subnetworks which contribute\\ntowards these observed holistic connectomic alterations. One organizational\\nprinciple is the rich-club - a core subnetwork of brain regions that are\\nstrongly connected, forming a high-cost, high-capacity backbone that is\\ncritical for effective communication in the network. Investigations primarily\\nfocus on its alterations with disease and age. Here, we present a systematic\\nanalysis of not only the rich-club, but also other subnetworks derived from\\nthis backbone - namely feeder and seeder subnetworks. Our analysis is applied\\nto structural connectomes in a normal cohort from a large, publicly available\\nlifespan study. We demonstrate changes in rich-club membership with age\\nalongside a shift in importance from 'peripheral' seeder to feeder subnetworks.\\nOur results show a refinement within the rich-club structure (increase in\\ntransitivity and betweenness centrality), as well as increased efficiency in\\nthe feeder subnetwork and decreased measures of network integration and\\nsegregation in the seeder subnetwork. These results demonstrate the different\\ndevelopmental patterns when analyzing the connectome stratified according to\\nits rich-club and the potential of utilizing this subnetwork analysis to reveal\\nthe evolution of brain architectural alterations across the life-span.\\n\", '  Although Generative Adversarial Networks (GANs) have shown remarkable success\\nin various tasks, they still face challenges in generating high quality images.\\nIn this paper, we propose Stacked Generative Adversarial Networks (StackGAN)\\naiming at generating high-resolution photo-realistic images. First, we propose\\na two-stage generative adversarial network architecture, StackGAN-v1, for\\ntext-to-image synthesis. The Stage-I GAN sketches the primitive shape and\\ncolors of the object based on given text description, yielding low-resolution\\nimages. The Stage-II GAN takes Stage-I results and text descriptions as inputs,\\nand generates high-resolution images with photo-realistic details. Second, an\\nadvanced multi-stage generative adversarial network architecture, StackGAN-v2,\\nis proposed for both conditional and unconditional generative tasks. Our\\nStackGAN-v2 consists of multiple generators and discriminators in a tree-like\\nstructure; images at multiple scales corresponding to the same scene are\\ngenerated from different branches of the tree. StackGAN-v2 shows more stable\\ntraining behavior than StackGAN-v1 by jointly approximating multiple\\ndistributions. Extensive experiments demonstrate that the proposed stacked\\ngenerative adversarial networks significantly outperform other state-of-the-art\\nmethods in generating photo-realistic images.\\n', '  Deep neural networks require a large amount of labeled training data during\\nsupervised learning. However, collecting and labeling so much data might be\\ninfeasible in many cases. In this paper, we introduce a source-target selective\\njoint fine-tuning scheme for improving the performance of deep learning tasks\\nwith insufficient training data. In this scheme, a target learning task with\\ninsufficient training data is carried out simultaneously with another source\\nlearning task with abundant training data. However, the source learning task\\ndoes not use all existing training data. Our core idea is to identify and use a\\nsubset of training images from the original source learning task whose\\nlow-level characteristics are similar to those from the target learning task,\\nand jointly fine-tune shared convolutional layers for both tasks. Specifically,\\nwe compute descriptors from linear or nonlinear filter bank responses on\\ntraining images from both tasks, and use such descriptors to search for a\\ndesired subset of training samples for the source learning task.\\nExperiments demonstrate that our selective joint fine-tuning scheme achieves\\nstate-of-the-art performance on multiple visual classification tasks with\\ninsufficient training data for deep learning. Such tasks include Caltech 256,\\nMIT Indoor 67, Oxford Flowers 102 and Stanford Dogs 120. In comparison to\\nfine-tuning without a source domain, the proposed method can improve the\\nclassification accuracy by 2% - 10% using a single model.\\n', '  Due to recent advances in technology, the recording and analysis of video\\ndata has become an increasingly common component of athlete training\\nprogrammes. Today it is incredibly easy and affordable to set up a fixed camera\\nand record athletes in a wide range of sports, such as diving, gymnastics,\\ngolf, tennis, etc. However, the manual analysis of the obtained footage is a\\ntime-consuming task which involves isolating actions of interest and\\ncategorizing them using domain-specific knowledge. In order to automate this\\nkind of task, three challenging sub-problems are often encountered: 1)\\ntemporally cropping events/actions of interest from continuous video; 2)\\ntracking the object of interest; and 3) classifying the events/actions of\\ninterest.\\nMost previous work has focused on solving just one of the above sub-problems\\nin isolation. In contrast, this paper provides a complete solution to the\\noverall action monitoring task in the context of a challenging real-world\\nexemplar. Specifically, we address the problem of diving classification. This\\nis a challenging problem since the person (diver) of interest typically\\noccupies fewer than 1% of the pixels in each frame. The model is required to\\nlearn the temporal boundaries of a dive, even though other divers and\\nbystanders may be in view. Finally, the model must be sensitive to subtle\\nchanges in body pose over a large number of frames to determine the\\nclassification code. We provide effective solutions to each of the sub-problems\\nwhich combine to provide a highly functional solution to the task as a whole.\\nThe techniques proposed can be easily generalized to video footage recorded\\nfrom other sports.\\n', \"  Feed-forward convolutional neural networks (CNNs) are currently\\nstate-of-the-art for object classification tasks such as ImageNet. Further,\\nthey are quantitatively accurate models of temporally-averaged responses of\\nneurons in the primate brain's visual system. However, biological visual\\nsystems have two ubiquitous architectural features not shared with typical\\nCNNs: local recurrence within cortical areas, and long-range feedback from\\ndownstream areas to upstream areas. Here we explored the role of recurrence in\\nimproving classification performance. We found that standard forms of\\nrecurrence (vanilla RNNs and LSTMs) do not perform well within deep CNNs on the\\nImageNet task. In contrast, novel cells that incorporated two structural\\nfeatures, bypassing and gating, were able to boost task accuracy substantially.\\nWe extended these design principles in an automated search over thousands of\\nmodel architectures, which identified novel local recurrent cells and\\nlong-range feedback connections useful for object recognition. Moreover, these\\ntask-optimized ConvRNNs matched the dynamics of neural activity in the primate\\nvisual system better than feedforward networks, suggesting a role for the\\nbrain's recurrent connections in performing difficult visual behaviors.\\n\", '  Remote sensing image scene classification plays an important role in a wide\\nrange of applications and hence has been receiving remarkable attention. During\\nthe past years, significant efforts have been made to develop various datasets\\nor present a variety of approaches for scene classification from remote sensing\\nimages. However, a systematic review of the literature concerning datasets and\\nmethods for scene classification is still lacking. In addition, almost all\\nexisting datasets have a number of limitations, including the small scale of\\nscene classes and the image numbers, the lack of image variations and\\ndiversity, and the saturation of accuracy. These limitations severely limit the\\ndevelopment of new approaches especially deep learning-based methods. This\\npaper first provides a comprehensive review of the recent progress. Then, we\\npropose a large-scale dataset, termed \"NWPU-RESISC45\", which is a publicly\\navailable benchmark for REmote Sensing Image Scene Classification (RESISC),\\ncreated by Northwestern Polytechnical University (NWPU). This dataset contains\\n31,500 images, covering 45 scene classes with 700 images in each class. The\\nproposed NWPU-RESISC45 (i) is large-scale on the scene classes and the total\\nimage number, (ii) holds big variations in translation, spatial resolution,\\nviewpoint, object pose, illumination, background, and occlusion, and (iii) has\\nhigh within-class diversity and between-class similarity. The creation of this\\ndataset will enable the community to develop and evaluate various data-driven\\nalgorithms. Finally, several representative methods are evaluated using the\\nproposed dataset and the results are reported as a useful baseline for future\\nresearch.\\n', '  This paper proposes a new algorithm for controlling classification results by\\ngenerating a small additive perturbation without changing the classifier\\nnetwork. Our work is inspired by existing works generating adversarial\\nperturbation that worsens classification performance. In contrast to the\\nexisting methods, our work aims to generate perturbations that can enhance\\noverall classification performance. To solve this performance enhancement\\nproblem, we newly propose a perturbation generation network (PGN) influenced by\\nthe adversarial learning strategy. In our problem, the information in a large\\nexternal dataset is summarized by a small additive perturbation, which helps to\\nimprove the performance of the classifier trained with the target dataset. In\\naddition to this performance enhancement problem, we show that the proposed PGN\\ncan be adopted to solve the classical adversarial problem without utilizing the\\ninformation on the target classifier. The mentioned characteristics of our\\nmethod are verified through extensive experiments on publicly available visual\\ndatasets.\\n', '  Computer aided diagnostic (CAD) system is crucial for modern med-ical\\nimaging. But almost all CAD systems operate on reconstructed images, which were\\noptimized for radiologists. Computer vision can capture features that is subtle\\nto human observers, so it is desirable to design a CAD system op-erating on the\\nraw data. In this paper, we proposed a deep-neural-network-based detection\\nsystem for lung nodule detection in computed tomography (CT). A\\nprimal-dual-type deep reconstruction network was applied first to convert the\\nraw data to the image space, followed by a 3-dimensional convolutional neural\\nnetwork (3D-CNN) for the nodule detection. For efficient network training, the\\ndeep reconstruction network and the CNN detector was trained sequentially\\nfirst, then followed by one epoch of end-to-end fine tuning. The method was\\nevaluated on the Lung Image Database Consortium image collection (LIDC-IDRI)\\nwith simulated forward projections. With 144 multi-slice fanbeam pro-jections,\\nthe proposed end-to-end detector could achieve comparable sensitivity with the\\nreference detector, which was trained and applied on the fully-sampled image\\ndata. It also demonstrated superior detection performance compared to detectors\\ntrained on the reconstructed images. The proposed method is general and could\\nbe expanded to most detection tasks in medical imaging.\\n', '  From medical charts to national census, healthcare has traditionally operated\\nunder a paper-based paradigm. However, the past decade has marked a long and\\narduous transformation bringing healthcare into the digital age. Ranging from\\nelectronic health records, to digitized imaging and laboratory reports, to\\npublic health datasets, today, healthcare now generates an incredible amount of\\ndigital information. Such a wealth of data presents an exciting opportunity for\\nintegrated machine learning solutions to address problems across multiple\\nfacets of healthcare practice and administration. Unfortunately, the ability to\\nderive accurate and informative insights requires more than the ability to\\nexecute machine learning models. Rather, a deeper understanding of the data on\\nwhich the models are run is imperative for their success. While a significant\\neffort has been undertaken to develop models able to process the volume of data\\nobtained during the analysis of millions of digitalized patient records, it is\\nimportant to remember that volume represents only one aspect of the data. In\\nfact, drawing on data from an increasingly diverse set of sources, healthcare\\ndata presents an incredibly complex set of attributes that must be accounted\\nfor throughout the machine learning pipeline. This chapter focuses on\\nhighlighting such challenges, and is broken down into three distinct\\ncomponents, each representing a phase of the pipeline. We begin with attributes\\nof the data accounted for during preprocessing, then move to considerations\\nduring model building, and end with challenges to the interpretation of model\\noutput. For each component, we present a discussion around data as it relates\\nto the healthcare domain and offer insight into the challenges each may impose\\non the efficiency of machine learning techniques.\\n', '  Recent advances in Representation Learning and Adversarial Training seem to\\nsucceed in removing unwanted features from the learned representation. We show\\nthat demographic information of authors is encoded in -- and can be recovered\\nfrom -- the intermediate representations learned by text-based neural\\nclassifiers. The implication is that decisions of classifiers trained on\\ntextual data are not agnostic to -- and likely condition on -- demographic\\nattributes. When attempting to remove such demographic information using\\nadversarial training, we find that while the adversarial component achieves\\nchance-level development-set accuracy during training, a post-hoc classifier,\\ntrained on the encoded sentences from the first part, still manages to reach\\nsubstantially higher classification accuracies on the same data. This behavior\\nis consistent across several tasks, demographic properties and datasets. We\\nexplore several techniques to improve the effectiveness of the adversarial\\ncomponent. Our main conclusion is a cautionary one: do not rely on the\\nadversarial training to achieve invariant representation to sensitive features.\\n', '  Learning network representations has a variety of applications, such as\\nnetwork classification. Most existing work in this area focuses on static\\nundirected networks and do not account for presence of directed edges or\\ntemporarily changes. Furthermore, most work focuses on node representations\\nthat do poorly on tasks like network classification. In this paper, we propose\\na novel, flexible and scalable network embedding methodology, \\\\emph{gl2vec},\\nfor network classification in both static and temporal directed networks.\\n\\\\emph{gl2vec} constructs vectors for feature representation using static or\\ntemporal network graphlet distributions and a null model for comparing them\\nagainst random graphs. We argue that \\\\emph{gl2vec} can be used to classify and\\ncompare networks of varying sizes and time period with high accuracy. We\\ndemonstrate the efficacy and usability of \\\\emph{gl2vec} over existing\\nstate-of-the-art methods on network classification tasks such as network type\\nclassification and subgraph identification in several real-world static and\\ntemporal directed networks. Experimental results further show that\\n\\\\emph{gl2vec}, concatenated with a wide range of state-of-the-art methods,\\nimproves classification accuracy by up to $10\\\\%$ in real-world applications\\nsuch as detecting departments for subgraphs in an email network or identifying\\nmobile users given their app switching behaviors represented as static or\\ntemporal directed networks.\\n', '  Often, more time is spent on finding a model that works well, rather than\\ntuning the model and working directly with the dataset. Our research began as\\nan attempt to improve upon a simple Recurrent Neural Network for answering\\n\"simple\" first-order questions (QA-RNN), developed by Ferhan Ture and Oliver\\nJojic, from Comcast Labs, using the SimpleQuestions dataset. Their baseline\\nmodel, a bidirectional, 2-layer LSTM RNN and a GRU RNN, have accuracies of 0.94\\nand 0.90, for entity detection and relation prediction, respectively. We fine\\ntuned these models by doing substantial hyper-parameter tuning, getting\\nresulting accuracies of 0.70 and 0.80, for entity detection and relation\\nprediction, respectively. An accuracy of 0.984 was obtained on entity detection\\nusing a 1-layer LSTM, where preprocessing was done by removing all words not\\npart of a noun chunk from the question. 100% of the dataset was available for\\nrelation prediction, but only 20% of the dataset, was available for entity\\ndetection, which we believe to be much of the reason for our initial\\ndifficulties in replicating their result, despite the fact we were able to\\nimprove on their entity detection results.\\n']\n"]}],"source":["paras=[]\n","i=0\n","db = db.reset_index(drop = True)\n","for i in range(10000):\n","  if db.Prob1[i]>0.99 and db.Topic1[i]==lda_model[bow_vector][0][0]:\n","    paras.append(db.text[i])\n","\n","print(paras)"]},{"cell_type":"code","execution_count":45,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"elapsed":127,"status":"ok","timestamp":1659535699498,"user":{"displayName":"Kabhilan S","userId":"04978609858624650983"},"user_tz":-330},"id":"YxerE2cS2Hbb","outputId":"b240cfeb-9b72-401e-9d01-aff8621f6ee3"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["'0.021*\"network\" + 0.017*\"model\" + 0.014*\"learn\" + 0.010*\"data\" + 0.009*\"method\" + 0.008*\"propos\" + 0.008*\"train\" + 0.007*\"neural\" + 0.007*\"base\" + 0.007*\"imag\"'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":45}],"source":["lda_model.print_topics()[0][1]"]},{"cell_type":"code","execution_count":46,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":127,"status":"ok","timestamp":1659535699499,"user":{"displayName":"Kabhilan S","userId":"04978609858624650983"},"user_tz":-330},"id":"jTpbUerCrTU8","outputId":"eaff79e4-d68e-4e26-bd4a-f24f8da57c4b"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["292"]},"metadata":{},"execution_count":46}],"source":["len(paras)"]},{"cell_type":"code","execution_count":47,"metadata":{"executionInfo":{"elapsed":20,"status":"ok","timestamp":1659535699499,"user":{"displayName":"Kabhilan S","userId":"04978609858624650983"},"user_tz":-330},"id":"49pg3HIPnbSD"},"outputs":[],"source":["def para():\n","  paras=[]\n","  for i in range(db.size):\n","    if db.Prob1[i]>0.5 and db.Topic1[i]==lda_model[bow_vector][0][0]:\n","      paras.append(db.text[i])\n","  return paras"]},{"cell_type":"code","execution_count":48,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":19,"status":"ok","timestamp":1659535699499,"user":{"displayName":"Kabhilan S","userId":"04978609858624650983"},"user_tz":-330},"id":"F4BDNu1LoxZr","outputId":"1f133b94-113d-454d-cc4d-f03cda992e72"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["272636"]},"metadata":{},"execution_count":48}],"source":["db.size"]},{"cell_type":"code","execution_count":49,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"elapsed":18,"status":"ok","timestamp":1659535699500,"user":{"displayName":"Kabhilan S","userId":"04978609858624650983"},"user_tz":-330},"id":"IZKJdLCxpPil","outputId":"ee53ed8f-7f9f-454f-adad-c1877ca2833b"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["'0.007*\"model\" + 0.007*\"field\" + 0.006*\"observ\" + 0.006*\"result\" + 0.005*\"time\" + 0.005*\"energi\" + 0.005*\"studi\" + 0.005*\"phase\" + 0.005*\"state\" + 0.005*\"effect\"'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":49}],"source":["lda_model.print_topic(1,10)"]},{"cell_type":"code","execution_count":50,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"elapsed":18,"status":"ok","timestamp":1659535699500,"user":{"displayName":"Kabhilan S","userId":"04978609858624650983"},"user_tz":-330},"id":"F0QFRT6G6MXb","outputId":"a45403fb-fac4-4be8-ca47-951f125ee55c"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["'0.010*\"result\" + 0.010*\"space\" + 0.008*\"problem\" + 0.007*\"group\" + 0.007*\"model\" + 0.007*\"general\" + 0.007*\"paper\" + 0.007*\"function\"'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":50}],"source":["lda_model.print_topic(4,8)"]},{"cell_type":"code","execution_count":51,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":17,"status":"ok","timestamp":1659535699500,"user":{"displayName":"Kabhilan S","userId":"04978609858624650983"},"user_tz":-330},"id":"oKw6cnaj9Cyp","outputId":"b694e54e-d1a4-46ff-f2c4-e5feecd91e86"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["[(0, 0.012063339),\n"," (1, 0.012307062),\n"," (2, 0.9513883),\n"," (3, 0.012008032),\n"," (4, 0.0122333085)]"]},"metadata":{},"execution_count":51}],"source":["lda_model[bow_vector]"]},{"cell_type":"code","execution_count":52,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":15,"status":"ok","timestamp":1659535699500,"user":{"displayName":"Kabhilan S","userId":"04978609858624650983"},"user_tz":-330},"id":"QDyqar-ILL_E","outputId":"40a70288-10c4-4ad0-e098-f1f4c4c02eee"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["2"]},"metadata":{},"execution_count":52}],"source":["from operator import itemgetter\n","\n","max(lda_model[bow_vector],key=itemgetter(1))[0]"]},{"cell_type":"code","execution_count":53,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":14,"status":"ok","timestamp":1659535699501,"user":{"displayName":"Kabhilan S","userId":"04978609858624650983"},"user_tz":-330},"id":"5yAXiiQYAz-C","outputId":"d49de100-308b-4f4a-97ed-fb98c5bcbd3f"},"outputs":[{"output_type":"stream","name":"stdout","text":["4\n"]}],"source":["a=[]\n","dom1=0\n","for index, score in lda_model[bow_vector]:\n","      b=(\"Topic no. {}\\t Score: {}\\t Topic: {}\".format(index, score, lda_model.print_topic(index, 10)))\n","      a.append(b)\n","      if index!=0 :\n","        if lda_model[bow_vector][index][1]>lda_model[bow_vector][index-1][1]:\n","          dom1=lda_model[bow_vector][index][0]\n","print(dom1)"]},{"cell_type":"code","execution_count":54,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":12,"status":"ok","timestamp":1659535699501,"user":{"displayName":"Kabhilan S","userId":"04978609858624650983"},"user_tz":-330},"id":"JuzyMQgmBkNZ","outputId":"4fd92d67-9e95-465b-d0f6-ee8f3ae47d9a"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["[(0, 0.012063353),\n"," (1, 0.012307065),\n"," (2, 0.95138216),\n"," (3, 0.0120080365),\n"," (4, 0.012239357)]"]},"metadata":{},"execution_count":54}],"source":["lda_model[bow_vector]"]},{"cell_type":"code","execution_count":55,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":53},"executionInfo":{"elapsed":10,"status":"ok","timestamp":1659535699501,"user":{"displayName":"Kabhilan S","userId":"04978609858624650983"},"user_tz":-330},"id":"a2ZNy9RpJgzw","outputId":"fbb8beb9-9299-4459-c731-34177f418450"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["'The recent discovery that the exponent of matrix multiplication is determined by the rank of the symmetrized matrix multiplication tensor has invigorated interest in better understanding symmetrized matrix multiplication'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":55}],"source":["unseen_document"]},{"cell_type":"code","execution_count":80,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1710,"status":"ok","timestamp":1659538226987,"user":{"displayName":"Kabhilan S","userId":"04978609858624650983"},"user_tz":-330},"id":"0KlfauNBRJcG","outputId":"66de3c10-db3f-489b-fad1-222714805c38"},"outputs":[{"output_type":"stream","name":"stdout","text":["/drive/ngrok-ssh\n","--2022-08-03 14:50:22--  https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-windows-amd64.zip\n","Resolving bin.equinox.io (bin.equinox.io)... 52.202.168.65, 54.161.241.46, 18.205.222.128, ...\n","Connecting to bin.equinox.io (bin.equinox.io)|52.202.168.65|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 14001275 (13M) [application/octet-stream]\n","Saving to: ngrok-stable-windows-amd64.zip\n","\n","ngrok-stable-window 100%[===================>]  13.35M  54.2MB/s    in 0.2s    \n","\n","2022-08-03 14:50:23 (54.2 MB/s) - ngrok-stable-windows-amd64.zip saved [14001275/14001275]\n","\n","Archive:  ngrok-stable-windows-amd64.zip\n","cp: cannot stat '/drive/ngrok-ssh/ngrok': No such file or directory\n","chmod: cannot access '/ngrok': No such file or directory\n","/bin/bash: /ngrok: No such file or directory\n"]}],"source":["!mkdir -p /drive/ngrok-ssh\n","%cd /drive/ngrok-ssh\n","!wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-windows-amd64.zip -O ngrok-stable-windows-amd64.zip\n","!unzip -u ngrok-stable-windows-amd64.zip\n","!cp /drive/ngrok-ssh/ngrok /ngrok\n","!chmod +x /ngrok\n","!/ngrok authtoken 1sxzBR5dgIiI9PuhgTLFYyZlBxe_6n17cXwWC8LmW5pFGVwU8"]},{"cell_type":"code","source":["cp --help"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ccQprG05IPHk","executionInfo":{"status":"ok","timestamp":1659537290984,"user_tz":-330,"elapsed":7,"user":{"displayName":"Kabhilan S","userId":"04978609858624650983"}},"outputId":"9b1e2270-ea13-46e3-80f4-9ed618f87522"},"execution_count":68,"outputs":[{"output_type":"stream","name":"stdout","text":["Usage: cp [OPTION]... [-T] SOURCE DEST\n","  or:  cp [OPTION]... SOURCE... DIRECTORY\n","  or:  cp [OPTION]... -t DIRECTORY SOURCE...\n","Copy SOURCE to DEST, or multiple SOURCE(s) to DIRECTORY.\n","\n","Mandatory arguments to long options are mandatory for short options too.\n","  -a, --archive                same as -dR --preserve=all\n","      --attributes-only        don't copy the file data, just the attributes\n","      --backup[=CONTROL]       make a backup of each existing destination file\n","  -b                           like --backup but does not accept an argument\n","      --copy-contents          copy contents of special files when recursive\n","  -d                           same as --no-dereference --preserve=links\n","  -f, --force                  if an existing destination file cannot be\n","                                 opened, remove it and try again (this option\n","                                 is ignored when the -n option is also used)\n","  -i, --interactive            prompt before overwrite (overrides a previous -n\n","                                  option)\n","  -H                           follow command-line symbolic links in SOURCE\n","  -l, --link                   hard link files instead of copying\n","  -L, --dereference            always follow symbolic links in SOURCE\n","  -n, --no-clobber             do not overwrite an existing file (overrides\n","                                 a previous -i option)\n","  -P, --no-dereference         never follow symbolic links in SOURCE\n","  -p                           same as --preserve=mode,ownership,timestamps\n","      --preserve[=ATTR_LIST]   preserve the specified attributes (default:\n","                                 mode,ownership,timestamps), if possible\n","                                 additional attributes: context, links, xattr,\n","                                 all\n","      --no-preserve=ATTR_LIST  don't preserve the specified attributes\n","      --parents                use full source file name under DIRECTORY\n","  -R, -r, --recursive          copy directories recursively\n","      --reflink[=WHEN]         control clone/CoW copies. See below\n","      --remove-destination     remove each existing destination file before\n","                                 attempting to open it (contrast with --force)\n","      --sparse=WHEN            control creation of sparse files. See below\n","      --strip-trailing-slashes  remove any trailing slashes from each SOURCE\n","                                 argument\n","  -s, --symbolic-link          make symbolic links instead of copying\n","  -S, --suffix=SUFFIX          override the usual backup suffix\n","  -t, --target-directory=DIRECTORY  copy all SOURCE arguments into DIRECTORY\n","  -T, --no-target-directory    treat DEST as a normal file\n","  -u, --update                 copy only when the SOURCE file is newer\n","                                 than the destination file or when the\n","                                 destination file is missing\n","  -v, --verbose                explain what is being done\n","  -x, --one-file-system        stay on this file system\n","  -Z                           set SELinux security context of destination\n","                                 file to default type\n","      --context[=CTX]          like -Z, or if CTX is specified then set the\n","                                 SELinux or SMACK security context to CTX\n","      --help     display this help and exit\n","      --version  output version information and exit\n","\n","By default, sparse SOURCE files are detected by a crude heuristic and the\n","corresponding DEST file is made sparse as well.  That is the behavior\n","selected by --sparse=auto.  Specify --sparse=always to create a sparse DEST\n","file whenever the SOURCE file contains a long enough sequence of zero bytes.\n","Use --sparse=never to inhibit creation of sparse files.\n","\n","When --reflink[=always] is specified, perform a lightweight copy, where the\n","data blocks are copied only when modified.  If this is not possible the copy\n","fails, or if --reflink=auto is specified, fall back to a standard copy.\n","\n","The backup suffix is '~', unless set with --suffix or SIMPLE_BACKUP_SUFFIX.\n","The version control method may be selected via the --backup option or through\n","the VERSION_CONTROL environment variable.  Here are the values:\n","\n","  none, off       never make backups (even if --backup is given)\n","  numbered, t     make numbered backups\n","  existing, nil   numbered if numbered backups exist, simple otherwise\n","  simple, never   always make simple backups\n","\n","As a special case, cp makes a backup of SOURCE when the force and backup\n","options are given and SOURCE and DEST are the same name for an existing,\n","regular file.\n","\n","GNU coreutils online help: <http://www.gnu.org/software/coreutils/>\n","Full documentation at: <http://www.gnu.org/software/coreutils/cp>\n","or available locally via: info '(coreutils) cp invocation'\n"]}]},{"cell_type":"code","execution_count":79,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BHA1JIHWurr-","executionInfo":{"status":"ok","timestamp":1659538196791,"user_tz":-330,"elapsed":221599,"user":{"displayName":"Kabhilan S","userId":"04978609858624650983"}},"outputId":"bf7f4e46-5a05-4e8e-d8ff-578311259296"},"outputs":[{"output_type":"stream","name":"stdout","text":[" * Serving Flask app \"__main__\" (lazy loading)\n"," * Environment: production\n","\u001b[31m   WARNING: This is a development server. Do not use it in a production deployment.\u001b[0m\n","\u001b[2m   Use a production WSGI server instead.\u001b[0m\n"," * Debug mode: off\n"]},{"output_type":"stream","name":"stderr","text":[" * Running on http://127.0.0.1:5000/ (Press CTRL+C to quit)\n"]},{"output_type":"stream","name":"stdout","text":[" * Running on http://462a-104-196-30-243.ngrok.io\n"," * Traffic stats available on http://127.0.0.1:4040\n"]},{"output_type":"stream","name":"stderr","text":["127.0.0.1 - - [03/Aug/2022 14:46:17] \"\u001b[37mGET / HTTP/1.1\u001b[0m\" 200 -\n","127.0.0.1 - - [03/Aug/2022 14:46:18] \"\u001b[33mGET /favicon.ico HTTP/1.1\u001b[0m\" 404 -\n","127.0.0.1 - - [03/Aug/2022 14:49:21] \"\u001b[37mGET / HTTP/1.1\u001b[0m\" 200 -\n","127.0.0.1 - - [03/Aug/2022 14:49:22] \"\u001b[33mGET /favicon.ico HTTP/1.1\u001b[0m\" 404 -\n","127.0.0.1 - - [03/Aug/2022 14:49:33] \"\u001b[37mGET / HTTP/1.1\u001b[0m\" 200 -\n","127.0.0.1 - - [03/Aug/2022 14:49:34] \"\u001b[33mGET /favicon.ico HTTP/1.1\u001b[0m\" 404 -\n","127.0.0.1 - - [03/Aug/2022 14:49:34] \"\u001b[37mGET / HTTP/1.1\u001b[0m\" 200 -\n","127.0.0.1 - - [03/Aug/2022 14:49:35] \"\u001b[33mGET /favicon.ico HTTP/1.1\u001b[0m\" 404 -\n","127.0.0.1 - - [03/Aug/2022 14:49:35] \"\u001b[37mGET / HTTP/1.1\u001b[0m\" 200 -\n","127.0.0.1 - - [03/Aug/2022 14:49:35] \"\u001b[37mGET / HTTP/1.1\u001b[0m\" 200 -\n","127.0.0.1 - - [03/Aug/2022 14:49:36] \"\u001b[37mGET / HTTP/1.1\u001b[0m\" 200 -\n","127.0.0.1 - - [03/Aug/2022 14:49:36] \"\u001b[37mGET / HTTP/1.1\u001b[0m\" 200 -\n","127.0.0.1 - - [03/Aug/2022 14:49:37] \"\u001b[37mGET / HTTP/1.1\u001b[0m\" 200 -\n","127.0.0.1 - - [03/Aug/2022 14:49:39] \"\u001b[37mGET / HTTP/1.1\u001b[0m\" 200 -\n","127.0.0.1 - - [03/Aug/2022 14:49:41] \"\u001b[33mGET /favicon.ico HTTP/1.1\u001b[0m\" 404 -\n"]}],"source":["from flask import Flask, render_template, request\n","from flask_ngrok import run_with_ngrok\n","app = Flask(__name__, template_folder=\"/content/gdrive/My Drive/templates\")\n","from operator import itemgetter\n","run_with_ngrok(app)   \n","  \n","@app.route(\"/\")\n","def index():\n","    a=[1,2,3,4,5]\n","    return render_template('index.html')\n","@app.route('/classify',methods=['GET','POST'])\n","def comparer():\n","  if request.method == 'POST':\n","    rawtext = request.form['rawtext']\n","    unseen_document=rawtext\n","    bow_vector = dictionary.doc2bow(preprocess(unseen_document))\n","    a=[]\n","    a.append(\" \")\n","    dom1=0\n","    for index, score in lda_model[bow_vector]:\n","      b=(\"Topic no. {}\\t Score: {}\\t Topic: {}\".format(index, score, lda_model.print_topic(index, 10)))\n","      a.append(b)\n","  \n","  dom1=max(lda_model[bow_vector],key=itemgetter(1))[0]\n","  return render_template('index.html',T1=a[1],T2=a[2],T3=a[3],T4=a[4],T5=a[5],dom=dom1,paras=para())\n","def para():\n","  paras=[]\n","  i=0\n","  for i in range(10000):\n","    if db.Prob1[i]>0.99 and db.Topic1[i]==lda_model[bow_vector][dom1][0]:\n","      paras.append(db.text[i])\n","  return paras\n","app.run()"]},{"cell_type":"code","execution_count":57,"metadata":{"id":"wcC0Glz4VQ80","executionInfo":{"status":"ok","timestamp":1659535906716,"user_tz":-330,"elapsed":5,"user":{"displayName":"Kabhilan S","userId":"04978609858624650983"}}},"outputs":[],"source":[""]}],"metadata":{"colab":{"collapsed_sections":[],"name":"Indexing_Contents_of_Documents_Using_LDA.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.8"}},"nbformat":4,"nbformat_minor":0}
